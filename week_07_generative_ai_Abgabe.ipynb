{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d731a498",
      "metadata": {
        "id": "d731a498"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a668a88",
      "metadata": {
        "id": "4a668a88"
      },
      "source": [
        "**This week's exercise has 4 tasks, for a total of 10 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building a modern PyTorch segmentation model\n",
        "- Training a modern model on a real-world segmentation task and achieving passable results\n",
        "\n",
        "**Note**: This is the last exercise concerning pure computer vision. Starting next week, we will begin with Natural Language Processing, i.e. text data. Therefore, don't worry too much if this exercise feels hard or if you can't complete all of it ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e34867",
      "metadata": {
        "id": "96e34867"
      },
      "source": [
        "#### Chapter 3.5 - Segmentation\n",
        "\n",
        "In previous tasks, we solved classification problems - we provide some input(s), typically an image, and get out a few numbers, which are the predicted pseudo-probabilities that our input belongs to some class, such as \"tumor\" or \"no tumor\". For this exercise, we will explore a new task that is extremely common in medical AI research and in clinical practice. This task is called segmentation. In segmentation, the goal is to go from an input image to one or several segmentations (also called *segmentation maps*) of that image. For the example of LiTS, this means that our input remains the same - a 256x256 image with 1 channel. However, our model outputs and targets are now different - they also have the shape 256x256 pixels, times the number of output classes, in our case 3 (background, liver, liver+tumor). Each 256x256 output is basically a map of which pixels in the original image belong to a certain class with what (pseudo-)probability. The training objective, in its simplest form, is also the same; Cross-Entropy Loss, but per pixel, instead of per-image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696c73ae",
      "metadata": {
        "id": "696c73ae"
      },
      "source": [
        "To solve today's tasks, we will need to build ourselves a few new things that look almost the same as things we have already built.\n",
        "\n",
        "**Task 1 (2 points)**: We will need a new Dataset class. It is the same as usual, except this time, when we return image and target in the getitem method, our target is now also a multi-dimensional tensor of size.\n",
        "\n",
        "We will return two kinds of targets - class-index targets and one-hot encoded targets. Class-index targets you already know. Every pixel is assigned a class, which can be 0 for background, 1 for liver, and 2 for lesions. The corresponding tensor has the size $H * W$. One-hot encoded targets instead have size $C * H * W$ - each channel is one class (the 0th channel is background, etc.), and the values for each pixel in a channel are 1 if that pixel belongs to that class and 0 if not. We will need both later on - class-index targets because that is the input for the normal CrossEntropyLoss, and one-hot targets because we will use them in this format for our DiceLoss.\n",
        "\n",
        "Since the \"background\" class has no segmentations, you will have to improvise them from the existing segmentations for this task.\n",
        "\n",
        "Your dataset class should return both targets at the end of the \\_\\_getitem\\_\\_ method like this: `return image, c_targets, oh_targets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N1bQ57_y3-YV",
      "metadata": {
        "id": "N1bQ57_y3-YV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download our data again:\n",
        "#!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "#!rm -rf ./sample_data/\n",
        "!rm -rf ./Clean_LiTS\n",
        "!unzip -qq ./drive/MyDrive/Clean_LiTS.zip -d .\n",
        "#!rm ./Clean_LiTS.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as ttf\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LiTS_Segmentation_Dataset(Dataset):\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        self.img_dir = f\"./Clean_LiTS/{mode}\"\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        # With this\n",
        "        image = image.to(torch.float32)\n",
        "\n",
        "        # Typical CT window for abdominal soft tissue\n",
        "        window_center = 40\n",
        "        window_width = 300\n",
        "\n",
        "        image = (image - window_center) / window_width\n",
        "        image = torch.clamp(image, -1, 1)\n",
        "\n",
        "        row=self.data.iloc[idx]\n",
        "        # 2. Load the Segmentation Masks\n",
        "        # The CSV has columns pointing to the separate mask files\n",
        "        liver_mask_name = row['liver_segmentation']\n",
        "        lesion_mask_name = row['lesion_segmentation']\n",
        "\n",
        "        liver_path = os.path.join(self.img_dir, liver_mask_name)\n",
        "        lesion_path = os.path.join(self.img_dir, lesion_mask_name)\n",
        "\n",
        "        liver_mask = Image.open(liver_path).convert(\"L\")\n",
        "        lesion_mask = Image.open(lesion_path).convert(\"L\")\n",
        "\n",
        "        # Convert masks to tensors [1, H, W]\n",
        "        liver_tensor = ttf.to_tensor(liver_mask)\n",
        "        lesion_tensor = ttf.to_tensor(lesion_mask)\n",
        "\n",
        "        # 3. Create Class-Index Target (c_targets)\n",
        "        # Start with a background of zeros [H, W]\n",
        "        c_targets = torch.zeros(image.shape[1:], dtype=torch.long)\n",
        "\n",
        "        # Mark liver pixels as 1\n",
        "        # We check where pixel value > 0 (since loaded masks might be 0-255 or 0-1)\n",
        "        c_targets[liver_tensor.squeeze(0) > 0] = 1\n",
        "\n",
        "        # Mark lesion pixels as 2 (This overwrites liver, which is correct)\n",
        "        c_targets[lesion_tensor.squeeze(0) > 0] = 2\n",
        "\n",
        "        # 4. Create One-Hot Target (oh_targets)\n",
        "        # F.one_hot creates [H, W, C], we need [C, H, W] for PyTorch\n",
        "        num_classes = 3\n",
        "        oh_targets = F.one_hot(c_targets, num_classes=num_classes) # [H, W, 3]\n",
        "        oh_targets = oh_targets.permute(2, 0, 1).float()           # [3, H, W]\n",
        "\n",
        "        return image, c_targets, oh_targets\n",
        "\n",
        "# --- Setup DataLoaders ---\n",
        "train_dataset = LiTS_Segmentation_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Segmentation_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Segmentation_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 1,\n",
        "    prefetch_factor = 2,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 1,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 1,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "BgrwMRFdC3dQ"
      },
      "id": "BgrwMRFdC3dQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2d416cc5",
      "metadata": {
        "id": "2d416cc5"
      },
      "source": [
        "**Task 2 (2 points)**: Plot a few images that contain livers and tumors, as well as their corresponding segmentation maps. Do they look correct? Is there anything special to note?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize the triplet (Image, Class Target, One-Hot Channels)\n",
        "def visualize_sample(dataset):\n",
        "    # Find an index that has a tumor so the plot is interesting\n",
        "    tumor_indices = dataset.data.index[dataset.data['lesion_visible'] == True].tolist()\n",
        "    if not tumor_indices:\n",
        "        print(\"No tumors found in this split!\")\n",
        "        idx = 0\n",
        "    else:\n",
        "        idx = tumor_indices[0] # Take the first one with a tumor\n",
        "\n",
        "    image, c_target, oh_target = dataset[idx]\n",
        "\n",
        "    # Prepare for plotting\n",
        "    # Image: [1, H, W] -> [H, W]\n",
        "    img_show = image.squeeze(0)\n",
        "\n",
        "    # Class Target: [H, W] (Values 0,1,2)\n",
        "\n",
        "    # One-Hot: [3, H, W] -> We'll plot each channel separately\n",
        "\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "    # 1. Original CT Scan\n",
        "    axes[0].imshow(img_show, cmap=\"gray\")\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # 2. Combined Class Target\n",
        "    # We use a colormap where 0=Black, 1=Greenish, 2=Yellowish\n",
        "    axes[1].imshow(c_target, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "    axes[1].set_title(\"Class-Index Target\\n(0=Bg, 1=Liv, 2=Tum)\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # 3. One-Hot: Background Channel\n",
        "    axes[2].imshow(oh_target[0], cmap=\"gray\")\n",
        "    axes[2].set_title(\"One-Hot: Background (0)\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    # 4. One-Hot: Liver Channel\n",
        "    axes[3].imshow(oh_target[1], cmap=\"gray\")\n",
        "    axes[3].set_title(\"One-Hot: Liver (1)\")\n",
        "    axes[3].axis(\"off\")\n",
        "\n",
        "    # 5. One-Hot: Tumor Channel\n",
        "    axes[4].imshow(oh_target[2], cmap=\"gray\")\n",
        "    axes[4].set_title(\"One-Hot: Tumor (2)\")\n",
        "    axes[4].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Sanity Check\n",
        "    print(f\"Image Shape: {image.shape}\")\n",
        "    print(f\"Class Target Shape: {c_target.shape} | Unique Values: {torch.unique(c_target)}\")\n",
        "    print(f\"One-Hot Target Shape: {oh_target.shape} | Sum of channels (should be 1 everywhere): {oh_target.sum(dim=0).mean().item()}\")\n",
        "\n",
        "print(\"--- Visualizing Training Sample ---\")\n",
        "visualize_sample(train_dataset)"
      ],
      "metadata": {
        "id": "N5xevugvp52A"
      },
      "id": "N5xevugvp52A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def visualize_three_samples(dataset):\n",
        "\n",
        "    # --- Choose indices that contain tumors (if available) ---\n",
        "    if \"lesion_visible\" in dataset.data.columns:\n",
        "        tumor_indices = dataset.data.index[dataset.data['lesion_visible'] == True].tolist()\n",
        "    else:\n",
        "        tumor_indices = []\n",
        "\n",
        "    # If no tumor slices known, fallback to 3 random indices\n",
        "    if len(tumor_indices) >= 3:\n",
        "        selected_indices = random.sample(tumor_indices, 3)\n",
        "    else:\n",
        "        selected_indices = random.sample(range(len(dataset)), 3)\n",
        "\n",
        "    print(f\"Selected indices: {selected_indices}\")\n",
        "\n",
        "    # --- Plot each selected sample ---\n",
        "    for idx in selected_indices:\n",
        "        image, c_target, oh_target = dataset[idx]\n",
        "\n",
        "        img_show = image.squeeze(0)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "        # 1. Original Image\n",
        "        axes[0].imshow(img_show, cmap=\"gray\")\n",
        "        axes[0].set_title(\"Original Image\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        # 2. Class-Index (0,1,2)\n",
        "        axes[1].imshow(c_target, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "        axes[1].set_title(\"Class-Index Target\\n(0=Bg, 1=Liv, 2=Tum)\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        # 3. Background Channel\n",
        "        axes[2].imshow(oh_target[0], cmap=\"gray\")\n",
        "        axes[2].set_title(\"One-Hot: Background (0)\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        # 4. Liver Channel\n",
        "        axes[3].imshow(oh_target[1], cmap=\"gray\")\n",
        "        axes[3].set_title(\"One-Hot: Liver (1)\")\n",
        "        axes[3].axis(\"off\")\n",
        "\n",
        "        # 5. Tumor Channel\n",
        "        axes[4].imshow(oh_target[2], cmap=\"gray\")\n",
        "        axes[4].set_title(\"One-Hot: Tumor (2)\")\n",
        "        axes[4].axis(\"off\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Sanity check\n",
        "        print(f\"Sample idx = {idx}\")\n",
        "        print(f\"Image Shape: {image.shape}\")\n",
        "        print(f\"Class Target Shape: {c_target.shape} | Unique Values: {torch.unique(c_target)}\")\n",
        "        print(f\"One-Hot Target Shape: {oh_target.shape} | Sum-of-channels mean: {oh_target.sum(dim=0).mean().item()}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# --- Run ---\n",
        "print(\"--- Visualizing 3 Random Samples ---\")\n",
        "visualize_three_samples(train_dataset)\n"
      ],
      "metadata": {
        "id": "PSbgIjZBtZYa"
      },
      "id": "PSbgIjZBtZYa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Background is artificially made"
      ],
      "metadata": {
        "id": "ZK_1pjO3siNY"
      },
      "id": "ZK_1pjO3siNY"
    },
    {
      "cell_type": "markdown",
      "id": "0d744a45",
      "metadata": {
        "id": "0d744a45"
      },
      "source": [
        "**Task 3 (2 points)**: Next, we need a different loss function. At the bottom, we provide a training/testing loop that already contains cross-entropy loss and a functional segmentation model, plus evaluation. We have learned in the lecture that DICE score, and by extension a DICE-based loss, can be useful for imbalanced classes. We have also discovered that LiTS 2017 contains a class imbalance - slices with tumors are much more rare than slices with livers. Hence, we will make our own DICE loss.\n",
        "\n",
        "The formula for the DICE loss is computed as follows: $1 - \\frac{2 * (|X \\land Y|)+\\epsilon}{|X|+|Y|+\\epsilon}$, where $X$ is the prediction and $Y$ the target.\n",
        "\n",
        "The DICE Loss class you create should fulfill the following criteria:\n",
        "- It subclasses torch.nn.module.\n",
        "- It is a class that implements an \\_\\_init\\_\\_ function.\n",
        "- The loss also implements a \\_\\_forward\\_\\_ function that accepts as inputs a prediction tensor and a target tensor, both of shape B x 3 x 256 x 256 - 3 channels because we will segment background, liver, and liver+tumor again. The output is the computed loss.\n",
        "- You may add class weighting to offset the class imbalance.\n",
        "\n",
        "Your total loss should be `total_loss = ce_loss + dice_loss`, and your backward pass should be `total_loss.backward()`.\n",
        "Run the training for a few epochs, once with and once without DICE loss included as part of the overall loss. In your experiment, which version worked better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01ba8f4",
      "metadata": {
        "id": "c01ba8f4"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as nnf\n",
        "\n",
        "def compute_dice_score(prediction: torch.Tensor, target: torch.Tensor):\n",
        "\n",
        "    prediction = prediction.to(dtype = torch.bool)\n",
        "    target = target.to(dtype = torch.bool)\n",
        "\n",
        "    intersection = torch.sum(prediction * target)   # TP\n",
        "    p_cardinality = torch.sum(prediction)           # TP+FP\n",
        "    t_cardinality = torch.sum(target)               # TP+FN\n",
        "    cardinality = p_cardinality + t_cardinality\n",
        "    eps = 1e-8\n",
        "\n",
        "    if cardinality != 0:\n",
        "        dice = (2 * intersection + eps) / (cardinality + eps) # 2*TP / (2*TP+FP+FN + eps)\n",
        "    else:\n",
        "        dice = None\n",
        "\n",
        "    return dice\n",
        "\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes Dice loss for a single class channel.\n",
        "    prediction: B x H x W (values between 0 and 1)\n",
        "    target:     B x H x W (0/1 one-hot)\n",
        "    \"\"\"\n",
        "    def __init__(self, weight: float=1.0):\n",
        "        super().__init__()\n",
        "        self.weight =weight\n",
        "\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "\n",
        "        eps=1e-8\n",
        "\n",
        "        # Flatten to (B, -1)\n",
        "        pred_flat = prediction.reshape(-1)\n",
        "        target_flat = target.reshape(-1)\n",
        "\n",
        "        intersection = torch.sum(pred_flat*target_flat)\n",
        "        cardinality = torch.sum(pred_flat) + torch.sum(target_flat)\n",
        "\n",
        "        dice = (2.0 * intersection + eps) / (cardinality + eps)\n",
        "        return self.weight*(1.0 - dice)\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-class Dice loss.\n",
        "    predictions: B x C x H x W (softmax probabilities)\n",
        "    targets:     B x C x H x W (one-hot)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=3, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.binary_loss = BinaryDiceLoss()\n",
        "\n",
        "        if class_weights is None:\n",
        "            # Background gets weight 1, liver 1, tumor HIGHER because rare\n",
        "            self.class_weights = torch.tensor([1.0, 1.0, 4.0])\n",
        "        else:\n",
        "            self.class_weights = torch.tensor(class_weights)\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        predictions: softmax output, shape (B, C, H, W)\n",
        "        targets: one-hot, shape (B, C, H, W)\n",
        "        \"\"\"\n",
        "        dice_total = 0.0\n",
        "\n",
        "        for c in range(self.num_classes):\n",
        "            dice_c = self.binary_loss(predictions[:, c], targets[:, c])\n",
        "            dice_total += self.class_weights[c] * dice_c\n",
        "\n",
        "        # Normalize by sum of weights\n",
        "        return dice_total / self.class_weights.sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optimizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleSegModel(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, num_classes, 1)  # logits fÃ¼r jede Klasse\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x  # B x C x H x W\n"
      ],
      "metadata": {
        "id": "2j8K5kYW0AjQ"
      },
      "id": "2j8K5kYW0AjQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 3\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Modell\n",
        "model = SimpleSegModel(in_channels=3, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Loss-Funktionen\n",
        "ce_loss_fn = nn.CrossEntropyLoss()\n",
        "dice_loss_fn = DiceLoss(num_classes=num_classes)\n",
        "\n",
        "# Dummy DataLoader (ersetze durch echte Daten)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Beispiel: 20 Bilder, 3x64x64\n",
        "images = torch.randn(20, 3, 64, 64)\n",
        "c_targets = torch.randint(0, num_classes, (20, 64, 64))        # Klassenindices\n",
        "oh_targets = F.one_hot(c_targets, num_classes=num_classes).permute(0,3,1,2).float()  # One-hot\n",
        "\n",
        "dataset = TensorDataset(images, c_targets, oh_targets)\n",
        "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "s4X7yi4lHqO1"
      },
      "id": "s4X7yi4lHqO1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(use_dice=True):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, c_targets, oh_targets in train_dataloader:\n",
        "            images = images.to(device)\n",
        "            c_targets = c_targets.to(device)\n",
        "            oh_targets = oh_targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(images)           # B x C x H x W\n",
        "            probs = F.softmax(predictions, dim=1)\n",
        "\n",
        "            ce_loss = ce_loss_fn(predictions, c_targets)\n",
        "            if use_dice:\n",
        "                dice_loss = dice_loss_fn(probs, oh_targets)\n",
        "                total_loss = ce_loss + dice_loss\n",
        "            else:\n",
        "                total_loss = ce_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training mit Cross-Entropy + Dice-Loss:\")\n",
        "train_model(use_dice=True)\n",
        "\n",
        "print(\"\\nTraining nur mit Cross-Entropy-Loss:\")\n",
        "train_model(use_dice=False)\n"
      ],
      "metadata": {
        "id": "ihV6yC0nH0Xx"
      },
      "id": "ihV6yC0nH0Xx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t3ZqDuPII5rY"
      },
      "id": "t3ZqDuPII5rY"
    },
    {
      "cell_type": "markdown",
      "id": "66a3f09f",
      "metadata": {
        "id": "66a3f09f"
      },
      "source": [
        "**Task 4 (4 points)**: Finally, we want to make our own model that can handle segmentations. For this course, we will build ourselves a U-Net. The original paper can be found here: https://arxiv.org/pdf/1505.04597.\n",
        "\n",
        "The input dimensions for the network will be the usual B x 1 x 256 x 256. The output dimensions should be B x 3 x 256 x 256. We have three output channels because we will still predict classes 0 (background), 1 (liver) and 2 (liver tumor) - this time, however, we predict the classes on a per-pixel basis.\n",
        "\n",
        "Since our input images have vastly smaller dimensions compared to those used in the original UNet-Paper, we will opt for a different scale of UNet. The general design remains the same as in the paper, except:\n",
        "\n",
        "- We will only downsample 3 times by a factor of 2, using MaxPool (for a minimum resolution 32x32).\n",
        "- Our 3x3 Convolutions will have Padding. Consequently, there will be no cropping during skip connections\n",
        "- We will only have 3 skip connections.\n",
        "- We will go for fewer maximum channels (as we have only 3 downsampling steps, we will have 64, 128, 256, and 512 channels).\n",
        "- Our final output will be 3 channels wide, not 2 (we predict background, liver, and liver tumors).\n",
        "\n",
        "Note that training a segmentation models takes a little while - we do not award points for results here, because it would mean that you would have to wait a long time to see whether your changes helped performance. All we want to see is that your model learns anything useful at all. As a rough guideline, you will probably start seeing ok liver segmentations after 1 epoch, and good liver and ok lesion segmentations after 2 or 3 epochs.\n",
        "\n",
        "If everything works correctly, you can copy the previous training loop and should get some good results. Don't forget to look at some of your predictions! Are they reasonable? Empty? Weird? Can you discover some kind of systemic issues with your predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c43e445",
      "metadata": {
        "id": "6c43e445"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels: int = 1, num_classes: int = 3, base_channels: int = 32, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        def double_conv(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.enc1 = double_conv(in_channels, base_channels)\n",
        "        self.enc2 = double_conv(base_channels, base_channels * 2)\n",
        "        self.enc3 = double_conv(base_channels * 2, base_channels * 4)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            double_conv(base_channels * 4, base_channels * 8),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.up3 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)\n",
        "        self.dec3 = double_conv(base_channels * 8, base_channels * 4)\n",
        "        self.up2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
        "        self.dec2 = double_conv(base_channels * 4, base_channels * 2)\n",
        "        self.up1 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
        "        self.dec1 = double_conv(base_channels * 2, base_channels)\n",
        "        self.classifier = nn.Conv2d(base_channels, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Encoder\n",
        "        c1 = self.enc1(x)\n",
        "        c2 = self.enc2(self.pool(c1))\n",
        "        c3 = self.enc3(self.pool(c2))\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool(c3))\n",
        "        # Decoder with skip connections\n",
        "        u3 = self.up3(b)\n",
        "        u3 = torch.cat([u3, c3], dim=1)\n",
        "        d3 = self.dec3(u3)\n",
        "        u2 = self.up2(d3)\n",
        "        u2 = torch.cat([u2, c2], dim=1)\n",
        "        d2 = self.dec2(u2)\n",
        "        u1 = self.up1(d2)\n",
        "        u1 = torch.cat([u1, c1], dim=1)\n",
        "        d1 = self.dec1(u1)\n",
        "        return self.classifier(d1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 3\n",
        "num_epochs = 12\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 1e-4\n",
        "dice_weight = 1.0\n",
        "ce_weight = 0.8\n",
        "gradient_clip = 1.0\n",
        "train_batch_size = 8\n",
        "model = UNet(in_channels=1, num_classes=num_classes, base_channels=32, dropout=0.1)\n",
        "model = model.to(device)\n",
        "dice_loss = DiceLoss(num_classes = 3).to(device) # Your dice loss class goes here\n",
        "ce_loss = nn.CrossEntropyLoss(\n",
        "    weight = torch.tensor([1.0, 2.5, 8.0]).to(device = device),\n",
        "    reduction = \"mean\",\n",
        "    #ignore_index = 0\n",
        "    )\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
        "loader_workers = max(1, min(4, os.cpu_count() // 2 if os.cpu_count() else 0))\n",
        "loader_kwargs = dict(num_workers = loader_workers, pin_memory = torch.cuda.is_available())\n",
        "if loader_kwargs[\"num_workers\"] > 0:\n",
        "    loader_kwargs[\"prefetch_factor\"] = 2\n",
        "    loader_kwargs[\"persistent_workers\"] = True\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        "    **loader_kwargs,\n",
        " )\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = False,\n",
        "    **loader_kwargs,\n",
        " )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = False,\n",
        "    **loader_kwargs,\n",
        " )"
      ],
      "metadata": {
        "id": "Mj55tXoZ1QbC"
      },
      "id": "Mj55tXoZ1QbC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If your model and loss work, this should at least execute successfully.\n",
        "# If you only wish to test your model, just comment out the dice_loss component everywhere.\n",
        "from tqdm.auto import tqdm\n",
        "best_val_loss = float(\"inf\")\n",
        "history = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    num_train_samples = 0\n",
        "    for data, c_targets, oh_targets in tqdm(train_dataloader, leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "        predictions = model(data)\n",
        "        probs = torch.softmax(predictions, dim=1)\n",
        "        loss_1 = dice_loss(probs, oh_targets)\n",
        "        loss_2 = ce_loss(predictions, c_targets)\n",
        "        total_loss = loss_1 * dice_weight + loss_2 * ce_weight\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)\n",
        "        optimizer.step()\n",
        "        train_loss += total_loss.item() * data.size(0)\n",
        "        num_train_samples += data.size(0)\n",
        "    train_loss = train_loss / max(1, num_train_samples)\n",
        "    # Validate once after every epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss_total = 0.0\n",
        "        val_samples = 0\n",
        "        background_sum = liver_sum = lesion_sum = 0.0\n",
        "        background_count = liver_count = lesion_count = 0\n",
        "        for val_step, (data, c_targets, oh_targets) in enumerate(tqdm(val_dataloader, leave=False)):\n",
        "            data, c_targets, oh_targets = data.to(device), c_targets.to(device), oh_targets.to(device)\n",
        "            predictions = model(data)\n",
        "            probs = torch.softmax(predictions, dim=1)\n",
        "            # loss\n",
        "            loss_1 = dice_loss(probs, oh_targets)\n",
        "            loss_2 = ce_loss(predictions, c_targets)\n",
        "            total_loss = loss_1 * dice_weight + loss_2 * ce_weight\n",
        "            batch_size_now = data.size(0)\n",
        "            val_loss_total += total_loss.item() * batch_size_now\n",
        "            val_samples += batch_size_now\n",
        "            p_arg = nnf.one_hot(torch.argmax(probs, dim = 1), num_classes = 3).moveaxis(-1, 1)\n",
        "            background_seg = oh_targets[:, 0, :, :]\n",
        "            liver_seg = oh_targets[:, 1, :, :]\n",
        "            lesion_seg = oh_targets[:, 2, :, :]\n",
        "            background_dice = compute_dice_score(p_arg[:,0,:,:], background_seg)\n",
        "            if background_dice is not None:\n",
        "                background_sum += background_dice.item() * batch_size_now\n",
        "                background_count += batch_size_now\n",
        "            if liver_seg.sum() != 0.0:\n",
        "                liver_dice = compute_dice_score(p_arg[:,1,:,:], liver_seg)\n",
        "                if liver_dice is not None:\n",
        "                    liver_sum += liver_dice.item() * batch_size_now\n",
        "                    liver_count += batch_size_now\n",
        "            if lesion_seg.sum() != 0.0:\n",
        "                lesion_dice = compute_dice_score(p_arg[:,2,:,:], lesion_seg)\n",
        "                if lesion_dice is not None:\n",
        "                    lesion_sum += lesion_dice.item() * batch_size_now\n",
        "                    lesion_count += batch_size_now\n",
        "        avg_background_dice = (background_sum / background_count) if background_count else 0.0\n",
        "        avg_liver_dice = (liver_sum / liver_count) if liver_count else 0.0\n",
        "        avg_lesion_dice = (lesion_sum / lesion_count) if lesion_count else 0.0\n",
        "        val_loss = val_loss_total / max(1, val_samples)\n",
        "        scheduler.step(val_loss)\n",
        "        history.append(dict(epoch=epoch+1, train_loss=train_loss, val_loss=val_loss, background_dice=avg_background_dice, liver_dice=avg_liver_dice, lesion_dice= avg_lesion_dice))\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch: {epoch+1},    LR: {current_lr:.2e},   Train Loss: {train_loss:.4f},   Validation Loss: {val_loss:.4f}, Background Dice:{avg_background_dice: .4f}, Liver Dice score: {avg_liver_dice: .4f}, Lesion Dice Score:{avg_lesion_dice: .4f}\")\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        # After we are done validating, let's not forget to go back to storing gradients.\n",
        "        model.train()"
      ],
      "metadata": {
        "id": "4h2N5_bu1fWr"
      },
      "id": "4h2N5_bu1fWr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try looking at some images and predicted segmentations to see how badly or how well you've done\n",
        "import random\n",
        "import numpy as np\n",
        "def visualize_predictions(dataset, model, device, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualize predictions from the model on a few test samples.\n",
        "    Ensures at least one sample with liver and one with tumor.\n",
        "    Shows original image, ground truth segmentation, and predicted segmentation.\n",
        "    \"\"\"\n",
        "    # Define colors for classes: Background=Black, Liver=Green, Tumor=Red\n",
        "    colors = np.array([\n",
        "        [0, 0, 0],      # Background: Black\n",
        "        [0, 255, 0],    # Liver: Green\n",
        "        [255, 0, 0]     # Tumor: Red\n",
        "    ]) / 255.0  # Normalize to [0,1]\n",
        "\n",
        "    # Select indices: at least one with liver, one with tumor, rest random\n",
        "    liver_indices = dataset.data.index[dataset.data['liver_visible'] == True].tolist()\n",
        "    tumor_indices = dataset.data.index[dataset.data['lesion_visible'] == True].tolist()\n",
        "\n",
        "    selected_indices = []\n",
        "\n",
        "    # Add one with liver if available\n",
        "    if liver_indices:\n",
        "        selected_indices.append(random.choice(liver_indices))\n",
        "\n",
        "    # Add one with tumor if available and different from liver one\n",
        "    if tumor_indices:\n",
        "        tumor_choice = random.choice(tumor_indices)\n",
        "        if tumor_choice not in selected_indices:\n",
        "            selected_indices.append(tumor_choice)\n",
        "        elif len(tumor_indices) > 1:\n",
        "            # Try another\n",
        "            tumor_indices.remove(tumor_choice)\n",
        "            selected_indices.append(random.choice(tumor_indices))\n",
        "\n",
        "    # Fill the rest with random samples\n",
        "    remaining = num_samples - len(selected_indices)\n",
        "    all_indices = list(range(len(dataset)))\n",
        "    random_indices = random.sample([i for i in all_indices if i not in selected_indices], remaining)\n",
        "    selected_indices.extend(random_indices)\n",
        "\n",
        "    # Ensure we have exactly num_samples\n",
        "    selected_indices = selected_indices[:num_samples]\n",
        "    print(f\"Selected indices: {selected_indices}\")\n",
        "\n",
        "    for idx in selected_indices:\n",
        "        image, c_targets, oh_targets = dataset[idx]\n",
        "\n",
        "        # Prepare image for display\n",
        "        img_show = image.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Ground truth segmentation (class indices)\n",
        "        gt_seg = c_targets.cpu().numpy()\n",
        "        gt_rgb = colors[gt_seg]  # Shape: (H, W, 3)\n",
        "\n",
        "        # Make prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            input_tensor = image.unsqueeze(0).to(device)  # Add batch dimension\n",
        "            predictions = model(input_tensor)\n",
        "            probs = torch.softmax(predictions, dim=1)\n",
        "            pred_seg = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()  # Remove batch dimension\n",
        "            pred_rgb = colors[pred_seg]  # Shape: (H, W, 3)\n",
        "\n",
        "        # Plot\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "        # Original Image\n",
        "        axes[0].imshow(img_show, cmap=\"gray\")\n",
        "        axes[0].set_title(\"Original CT Image\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        # Ground Truth Segmentation\n",
        "        axes[1].imshow(gt_rgb)\n",
        "        axes[1].set_title(\"Ground Truth Segmentation\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        # Predicted Segmentation\n",
        "        axes[2].imshow(pred_rgb)\n",
        "        axes[2].set_title(\"Predicted Segmentation\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        # Add legend\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [\n",
        "            Patch(facecolor='black', label='Background (0)'),\n",
        "            Patch(facecolor='green', label='Liver (1)'),\n",
        "            Patch(facecolor='red', label='Tumor (2)')\n",
        "        ]\n",
        "        fig.legend(handles=legend_elements, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print some stats\n",
        "        print(f\"Sample {idx}:\")\n",
        "        print(f\"  Ground Truth Classes: {set(gt_seg.flatten())}\")\n",
        "        print(f\"  Predicted Classes: {set(pred_seg.flatten())}\")\n",
        "        print(\"-\" * 50)\n",
        "# Visualize 5 random test samples, ensuring at least one with liver and one with tumor\n",
        "print(\"Visualizing model predictions on test data (at least one sample per class):\")\n",
        "visualize_predictions(test_dataset, model, device, num_samples=5)"
      ],
      "metadata": {
        "id": "98DG-wP-1pfC"
      },
      "id": "98DG-wP-1pfC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d731a498",
      "metadata": {
        "id": "d731a498"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a668a88",
      "metadata": {
        "id": "4a668a88"
      },
      "source": [
        "**This week's exercise has 4 tasks, for a total of 10 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building a modern PyTorch segmentation model\n",
        "- Training a modern model on a real-world segmentation task and achieving passable results\n",
        "\n",
        "**Note**: This is the last exercise concerning pure computer vision. Starting next week, we will begin with Natural Language Processing, i.e. text data. Therefore, don't worry too much if this exercise feels hard or if you can't complete all of it ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e34867",
      "metadata": {
        "id": "96e34867"
      },
      "source": [
        "#### Chapter 3.5 - Segmentation\n",
        "\n",
        "In previous tasks, we solved classification problems - we provide some input(s), typically an image, and get out a few numbers, which are the predicted pseudo-probabilities that our input belongs to some class, such as \"tumor\" or \"no tumor\". For this exercise, we will explore a new task that is extremely common in medical AI research and in clinical practice. This task is called segmentation. In segmentation, the goal is to go from an input image to one or several segmentations (also called *segmentation maps*) of that image. For the example of LiTS, this means that our input remains the same - a 256x256 image with 1 channel. However, our model outputs and targets are now different - they also have the shape 256x256 pixels, times the number of output classes, in our case 3 (background, liver, liver+tumor). Each 256x256 output is basically a map of which pixels in the original image belong to a certain class with what (pseudo-)probability. The training objective, in its simplest form, is also the same; Cross-Entropy Loss, but per pixel, instead of per-image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696c73ae",
      "metadata": {
        "id": "696c73ae"
      },
      "source": [
        "To solve today's tasks, we will need to build ourselves a few new things that look almost the same as things we have already built.\n",
        "\n",
        "**Task 1 (2 points)**: We will need a new Dataset class. It is the same as usual, except this time, when we return image and target in the getitem method, our target is now also a multi-dimensional tensor of size.\n",
        "\n",
        "We will return two kinds of targets - class-index targets and one-hot encoded targets. Class-index targets you already know. Every pixel is assigned a class, which can be 0 for background, 1 for liver, and 2 for lesions. The corresponding tensor has the size $H * W$. One-hot encoded targets instead have size $C * H * W$ - each channel is one class (the 0th channel is background, etc.), and the values for each pixel in a channel are 1 if that pixel belongs to that class and 0 if not. We will need both later on - class-index targets because that is the input for the normal CrossEntropyLoss, and one-hot targets because we will use them in this format for our DiceLoss.\n",
        "\n",
        "Since the \"background\" class has no segmentations, you will have to improvise them from the existing segmentations for this task.\n",
        "\n",
        "Your dataset class should return both targets at the end of the \\_\\_getitem\\_\\_ method like this: `return image, c_targets, oh_targets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N1bQ57_y3-YV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1bQ57_y3-YV",
        "outputId": "4e679588-22b6-4e6a-bb21-9a32a26527d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download our data again:\n",
        "#!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "#!rm -rf ./sample_data/\n",
        "!rm -rf ./Clean_LiTS\n",
        "!unzip -qq ./drive/MyDrive/Clean_LiTS.zip -d .\n",
        "#!rm ./Clean_LiTS.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as ttf\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LiTS_Segmentation_Dataset(Dataset):\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        self.img_dir = f\"./Clean_LiTS/{mode}\"\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        # With this\n",
        "        image = image.to(torch.float32)\n",
        "\n",
        "        # Typical CT window for abdominal soft tissue\n",
        "        window_center = 40\n",
        "        window_width = 300\n",
        "\n",
        "        image = (image - window_center) / window_width\n",
        "        image = torch.clamp(image, -1, 1)\n",
        "\n",
        "        row=self.data.iloc[idx]\n",
        "        # 2. Load the Segmentation Masks\n",
        "        # The CSV has columns pointing to the separate mask files\n",
        "        liver_mask_name = row['liver_segmentation']\n",
        "        lesion_mask_name = row['lesion_segmentation']\n",
        "\n",
        "        liver_path = os.path.join(self.img_dir, liver_mask_name)\n",
        "        lesion_path = os.path.join(self.img_dir, lesion_mask_name)\n",
        "\n",
        "        liver_mask = Image.open(liver_path).convert(\"L\")\n",
        "        lesion_mask = Image.open(lesion_path).convert(\"L\")\n",
        "\n",
        "        # Convert masks to tensors [1, H, W]\n",
        "        liver_tensor = ttf.to_tensor(liver_mask)\n",
        "        lesion_tensor = ttf.to_tensor(lesion_mask)\n",
        "\n",
        "        # 3. Create Class-Index Target (c_targets)\n",
        "        # Start with a background of zeros [H, W]\n",
        "        c_targets = torch.zeros(image.shape[1:], dtype=torch.long)\n",
        "\n",
        "        # Mark liver pixels as 1\n",
        "        # We check where pixel value > 0 (since loaded masks might be 0-255 or 0-1)\n",
        "        c_targets[liver_tensor.squeeze(0) > 0] = 1\n",
        "\n",
        "        # Mark lesion pixels as 2 (This overwrites liver, which is correct)\n",
        "        c_targets[lesion_tensor.squeeze(0) > 0] = 2\n",
        "\n",
        "        # 4. Create One-Hot Target (oh_targets)\n",
        "        # F.one_hot creates [H, W, C], we need [C, H, W] for PyTorch\n",
        "        num_classes = 3\n",
        "        oh_targets = F.one_hot(c_targets, num_classes=num_classes) # [H, W, 3]\n",
        "        oh_targets = oh_targets.permute(2, 0, 1).float()           # [3, H, W]\n",
        "\n",
        "        return image, c_targets, oh_targets\n",
        "\n",
        "# --- Setup DataLoaders ---\n",
        "train_dataset = LiTS_Segmentation_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Segmentation_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Segmentation_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 1,\n",
        "    prefetch_factor = 2,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 1,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 1,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "BgrwMRFdC3dQ"
      },
      "id": "BgrwMRFdC3dQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2d416cc5",
      "metadata": {
        "id": "2d416cc5"
      },
      "source": [
        "**Task 2 (2 points)**: Plot a few images that contain livers and tumors, as well as their corresponding segmentation maps. Do they look correct? Is there anything special to note?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize the triplet (Image, Class Target, One-Hot Channels)\n",
        "def visualize_sample(dataset):\n",
        "    # Find an index that has a tumor so the plot is interesting\n",
        "    tumor_indices = dataset.data.index[dataset.data['lesion_visible'] == True].tolist()\n",
        "    if not tumor_indices:\n",
        "        print(\"No tumors found in this split!\")\n",
        "        idx = 0\n",
        "    else:\n",
        "        idx = tumor_indices[0] # Take the first one with a tumor\n",
        "\n",
        "    image, c_target, oh_target = dataset[idx]\n",
        "\n",
        "    # Prepare for plotting\n",
        "    # Image: [1, H, W] -> [H, W]\n",
        "    img_show = image.squeeze(0)\n",
        "\n",
        "    # Class Target: [H, W] (Values 0,1,2)\n",
        "\n",
        "    # One-Hot: [3, H, W] -> We'll plot each channel separately\n",
        "\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "    # 1. Original CT Scan\n",
        "    axes[0].imshow(img_show, cmap=\"gray\")\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # 2. Combined Class Target\n",
        "    # We use a colormap where 0=Black, 1=Greenish, 2=Yellowish\n",
        "    axes[1].imshow(c_target, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "    axes[1].set_title(\"Class-Index Target\\n(0=Bg, 1=Liv, 2=Tum)\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # 3. One-Hot: Background Channel\n",
        "    axes[2].imshow(oh_target[0], cmap=\"gray\")\n",
        "    axes[2].set_title(\"One-Hot: Background (0)\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    # 4. One-Hot: Liver Channel\n",
        "    axes[3].imshow(oh_target[1], cmap=\"gray\")\n",
        "    axes[3].set_title(\"One-Hot: Liver (1)\")\n",
        "    axes[3].axis(\"off\")\n",
        "\n",
        "    # 5. One-Hot: Tumor Channel\n",
        "    axes[4].imshow(oh_target[2], cmap=\"gray\")\n",
        "    axes[4].set_title(\"One-Hot: Tumor (2)\")\n",
        "    axes[4].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Sanity Check\n",
        "    print(f\"Image Shape: {image.shape}\")\n",
        "    print(f\"Class Target Shape: {c_target.shape} | Unique Values: {torch.unique(c_target)}\")\n",
        "    print(f\"One-Hot Target Shape: {oh_target.shape} | Sum of channels (should be 1 everywhere): {oh_target.sum(dim=0).mean().item()}\")\n",
        "\n",
        "print(\"--- Visualizing Training Sample ---\")\n",
        "visualize_sample(train_dataset)"
      ],
      "metadata": {
        "id": "N5xevugvp52A"
      },
      "id": "N5xevugvp52A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def visualize_three_samples(dataset):\n",
        "\n",
        "    # --- Choose indices that contain tumors (if available) ---\n",
        "    if \"lesion_visible\" in dataset.data.columns:\n",
        "        tumor_indices = dataset.data.index[dataset.data['lesion_visible'] == True].tolist()\n",
        "    else:\n",
        "        tumor_indices = []\n",
        "\n",
        "    # If no tumor slices known, fallback to 3 random indices\n",
        "    if len(tumor_indices) >= 3:\n",
        "        selected_indices = random.sample(tumor_indices, 3)\n",
        "    else:\n",
        "        selected_indices = random.sample(range(len(dataset)), 3)\n",
        "\n",
        "    print(f\"Selected indices: {selected_indices}\")\n",
        "\n",
        "    # --- Plot each selected sample ---\n",
        "    for idx in selected_indices:\n",
        "        image, c_target, oh_target = dataset[idx]\n",
        "\n",
        "        img_show = image.squeeze(0)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "        # 1. Original Image\n",
        "        axes[0].imshow(img_show, cmap=\"gray\")\n",
        "        axes[0].set_title(\"Original Image\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        # 2. Class-Index (0,1,2)\n",
        "        axes[1].imshow(c_target, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "        axes[1].set_title(\"Class-Index Target\\n(0=Bg, 1=Liv, 2=Tum)\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        # 3. Background Channel\n",
        "        axes[2].imshow(oh_target[0], cmap=\"gray\")\n",
        "        axes[2].set_title(\"One-Hot: Background (0)\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        # 4. Liver Channel\n",
        "        axes[3].imshow(oh_target[1], cmap=\"gray\")\n",
        "        axes[3].set_title(\"One-Hot: Liver (1)\")\n",
        "        axes[3].axis(\"off\")\n",
        "\n",
        "        # 5. Tumor Channel\n",
        "        axes[4].imshow(oh_target[2], cmap=\"gray\")\n",
        "        axes[4].set_title(\"One-Hot: Tumor (2)\")\n",
        "        axes[4].axis(\"off\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Sanity check\n",
        "        print(f\"Sample idx = {idx}\")\n",
        "        print(f\"Image Shape: {image.shape}\")\n",
        "        print(f\"Class Target Shape: {c_target.shape} | Unique Values: {torch.unique(c_target)}\")\n",
        "        print(f\"One-Hot Target Shape: {oh_target.shape} | Sum-of-channels mean: {oh_target.sum(dim=0).mean().item()}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# --- Run ---\n",
        "print(\"--- Visualizing 3 Random Samples ---\")\n",
        "visualize_three_samples(train_dataset)\n"
      ],
      "metadata": {
        "id": "PSbgIjZBtZYa"
      },
      "id": "PSbgIjZBtZYa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Background is artificially made"
      ],
      "metadata": {
        "id": "ZK_1pjO3siNY"
      },
      "id": "ZK_1pjO3siNY"
    },
    {
      "cell_type": "markdown",
      "id": "0d744a45",
      "metadata": {
        "id": "0d744a45"
      },
      "source": [
        "**Task 3 (2 points)**: Next, we need a different loss function. At the bottom, we provide a training/testing loop that already contains cross-entropy loss and a functional segmentation model, plus evaluation. We have learned in the lecture that DICE score, and by extension a DICE-based loss, can be useful for imbalanced classes. We have also discovered that LiTS 2017 contains a class imbalance - slices with tumors are much more rare than slices with livers. Hence, we will make our own DICE loss.\n",
        "\n",
        "The formula for the DICE loss is computed as follows: $1 - \\frac{2 * (|X \\land Y|)+\\epsilon}{|X|+|Y|+\\epsilon}$, where $X$ is the prediction and $Y$ the target.\n",
        "\n",
        "The DICE Loss class you create should fulfill the following criteria:\n",
        "- It subclasses torch.nn.module.\n",
        "- It is a class that implements an \\_\\_init\\_\\_ function.\n",
        "- The loss also implements a \\_\\_forward\\_\\_ function that accepts as inputs a prediction tensor and a target tensor, both of shape B x 3 x 256 x 256 - 3 channels because we will segment background, liver, and liver+tumor again. The output is the computed loss.\n",
        "- You may add class weighting to offset the class imbalance.\n",
        "\n",
        "Your total loss should be `total_loss = ce_loss + dice_loss`, and your backward pass should be `total_loss.backward()`.\n",
        "Run the training for a few epochs, once with and once without DICE loss included as part of the overall loss. In your experiment, which version worked better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01ba8f4",
      "metadata": {
        "id": "c01ba8f4"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as nnf\n",
        "\n",
        "def compute_dice_score(prediction: torch.Tensor, target: torch.Tensor):\n",
        "\n",
        "    prediction = prediction.to(dtype = torch.bool)\n",
        "    target = target.to(dtype = torch.bool)\n",
        "\n",
        "    intersection = torch.sum(prediction * target)   # TP\n",
        "    p_cardinality = torch.sum(prediction)           # TP+FP\n",
        "    t_cardinality = torch.sum(target)               # TP+FN\n",
        "    cardinality = p_cardinality + t_cardinality\n",
        "    eps = 1e-8\n",
        "\n",
        "    if cardinality != 0:\n",
        "        dice = (2 * intersection + eps) / (cardinality + eps) # 2*TP / (2*TP+FP+FN + eps)\n",
        "    else:\n",
        "        dice = None\n",
        "\n",
        "    return dice\n",
        "\n",
        "class BinaryDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes Dice loss for a single class channel.\n",
        "    prediction: B x H x W (values between 0 and 1)\n",
        "    target:     B x H x W (0/1 one-hot)\n",
        "    \"\"\"\n",
        "    def __init__(self, weight: float=1.0):\n",
        "        super().__init__()\n",
        "        self.weight =weight\n",
        "\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "\n",
        "        eps=1e-8\n",
        "\n",
        "        # Flatten to (B, -1)\n",
        "        pred_flat = prediction.reshape(-1)\n",
        "        target_flat = target.reshape(-1)\n",
        "\n",
        "        intersection = torch.sum(pred_flat*target_flat)\n",
        "        cardinality = torch.sum(pred_flat) + torch.sum(target_flat)\n",
        "\n",
        "        dice = (2.0 * intersection + eps) / (cardinality + eps)\n",
        "        return self.weight*(1.0 - dice)\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-class Dice loss.\n",
        "    predictions: B x C x H x W (softmax probabilities)\n",
        "    targets:     B x C x H x W (one-hot)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=3, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.binary_loss = BinaryDiceLoss()\n",
        "\n",
        "        if class_weights is None:\n",
        "            # Background gets weight 1, liver 1, tumor HIGHER because rare\n",
        "            self.class_weights = torch.tensor([1.0, 1.0, 4.0])\n",
        "        else:\n",
        "            self.class_weights = torch.tensor(class_weights)\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        predictions: softmax output, shape (B, C, H, W)\n",
        "        targets: one-hot, shape (B, C, H, W)\n",
        "        \"\"\"\n",
        "        dice_total = 0.0\n",
        "\n",
        "        for c in range(self.num_classes):\n",
        "            dice_c = self.binary_loss(predictions[:, c], targets[:, c])\n",
        "            dice_total += self.class_weights[c] * dice_c\n",
        "\n",
        "        # Normalize by sum of weights\n",
        "        return dice_total / self.class_weights.sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optimizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleSegModel(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, num_classes, 1)  # logits für jede Klasse\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x  # B x C x H x W\n"
      ],
      "metadata": {
        "id": "2j8K5kYW0AjQ"
      },
      "id": "2j8K5kYW0AjQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 3\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Modell\n",
        "model = SimpleSegModel(in_channels=3, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Loss-Funktionen\n",
        "ce_loss_fn = nn.CrossEntropyLoss()\n",
        "dice_loss_fn = DiceLoss(num_classes=num_classes)\n",
        "\n",
        "# Dummy DataLoader (ersetze durch echte Daten)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Beispiel: 20 Bilder, 3x64x64\n",
        "images = torch.randn(20, 3, 64, 64)\n",
        "c_targets = torch.randint(0, num_classes, (20, 64, 64))        # Klassenindices\n",
        "oh_targets = F.one_hot(c_targets, num_classes=num_classes).permute(0,3,1,2).float()  # One-hot\n",
        "\n",
        "dataset = TensorDataset(images, c_targets, oh_targets)\n",
        "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "s4X7yi4lHqO1"
      },
      "id": "s4X7yi4lHqO1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(use_dice=True):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, c_targets, oh_targets in train_dataloader:\n",
        "            images = images.to(device)\n",
        "            c_targets = c_targets.to(device)\n",
        "            oh_targets = oh_targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(images)           # B x C x H x W\n",
        "            probs = F.softmax(predictions, dim=1)\n",
        "\n",
        "            ce_loss = ce_loss_fn(predictions, c_targets)\n",
        "            if use_dice:\n",
        "                dice_loss = dice_loss_fn(probs, oh_targets)\n",
        "                total_loss = ce_loss + dice_loss\n",
        "            else:\n",
        "                total_loss = ce_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training mit Cross-Entropy + Dice-Loss:\")\n",
        "train_model(use_dice=True)\n",
        "\n",
        "print(\"\\nTraining nur mit Cross-Entropy-Loss:\")\n",
        "train_model(use_dice=False)\n"
      ],
      "metadata": {
        "id": "ihV6yC0nH0Xx"
      },
      "id": "ihV6yC0nH0Xx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t3ZqDuPII5rY"
      },
      "id": "t3ZqDuPII5rY"
    },
    {
      "cell_type": "markdown",
      "id": "66a3f09f",
      "metadata": {
        "id": "66a3f09f"
      },
      "source": [
        "**Task 4 (4 points)**: Finally, we want to make our own model that can handle segmentations. For this course, we will build ourselves a U-Net. The original paper can be found here: https://arxiv.org/pdf/1505.04597.\n",
        "\n",
        "The input dimensions for the network will be the usual B x 1 x 256 x 256. The output dimensions should be B x 3 x 256 x 256. We have three output channels because we will still predict classes 0 (background), 1 (liver) and 2 (liver tumor) - this time, however, we predict the classes on a per-pixel basis.\n",
        "\n",
        "Since our input images have vastly smaller dimensions compared to those used in the original UNet-Paper, we will opt for a different scale of UNet. The general design remains the same as in the paper, except:\n",
        "\n",
        "- We will only downsample 3 times by a factor of 2, using MaxPool (for a minimum resolution 32x32).\n",
        "- Our 3x3 Convolutions will have Padding. Consequently, there will be no cropping during skip connections\n",
        "- We will only have 3 skip connections.\n",
        "- We will go for fewer maximum channels (as we have only 3 downsampling steps, we will have 64, 128, 256, and 512 channels).\n",
        "- Our final output will be 3 channels wide, not 2 (we predict background, liver, and liver tumors).\n",
        "\n",
        "Note that training a segmentation models takes a little while - we do not award points for results here, because it would mean that you would have to wait a long time to see whether your changes helped performance. All we want to see is that your model learns anything useful at all. As a rough guideline, you will probably start seeing ok liver segmentations after 1 epoch, and good liver and ok lesion segmentations after 2 or 3 epochs.\n",
        "\n",
        "If everything works correctly, you can copy the previous training loop and should get some good results. Don't forget to look at some of your predictions! Are they reasonable? Empty? Weird? Can you discover some kind of systemic issues with your predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c43e445",
      "metadata": {
        "id": "6c43e445"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(Conv3x3 -> ReLU -> Conv3x3 -> ReLU) with padding so spatial dims preserved\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels: int = 1, out_channels: int = 3, base_filters: int = 64):\n",
        "        super().__init__()\n",
        "        f = base_filters\n",
        "        # Encoder\n",
        "        self.enc1 = DoubleConv(in_channels, f)       # -> 64\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(f, f*2)              # -> 128\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = DoubleConv(f*2, f*4)            # -> 256\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(f*4, f*8)      # -> 512\n",
        "\n",
        "        # Decoder (Transposed conv upsample)\n",
        "        self.up3 = nn.ConvTranspose2d(f*8, f*4, kernel_size=2, stride=2)  # 512->256\n",
        "        self.dec3 = DoubleConv(f*8, f*4)  # concat skip from enc3\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(f*4, f*2, kernel_size=2, stride=2)  # 256->128\n",
        "        self.dec2 = DoubleConv(f*4, f*2)  # concat skip from enc2\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2)    # 128->64\n",
        "        self.dec1 = DoubleConv(f*2, f)   # concat skip from enc1\n",
        "\n",
        "        # Final 1x1 conv to get desired number of output channels\n",
        "        self.final_conv = nn.Conv2d(f, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)   # B x 64 x 256 x 256\n",
        "        p1 = self.pool1(e1) # B x 64 x 128 x 128\n",
        "\n",
        "        e2 = self.enc2(p1)  # B x 128 x 128 x 128\n",
        "        p2 = self.pool2(e2) # B x 128 x 64 x 64\n",
        "\n",
        "        e3 = self.enc3(p2)  # B x 256 x 64 x 64\n",
        "        p3 = self.pool3(e3) # B x 256 x 32 x 32\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(p3) # B x 512 x 32 x 32\n",
        "\n",
        "        # Decoder + skip connections (no cropping necessary because padding=1 preserved sizes)\n",
        "        u3 = self.up3(b)  # B x 256 x 64 x 64\n",
        "        # concat along channel dim\n",
        "        d3 = torch.cat([u3, e3], dim=1)  # B x (256+256) x 64 x 64\n",
        "        d3 = self.dec3(d3)               # B x 256 x 64 x 64\n",
        "\n",
        "        u2 = self.up2(d3)  # B x 128 x 128 x 128\n",
        "        d2 = torch.cat([u2, e2], dim=1)  # B x (128+128) x 128 x 128\n",
        "        d2 = self.dec2(d2)               # B x 128 x 128 x 128\n",
        "\n",
        "        u1 = self.up1(d2)  # B x 64 x 256 x 256\n",
        "        d1 = torch.cat([u1, e1], dim=1)  # B x (64+64) x 256 x 256\n",
        "        d1 = self.dec1(d1)               # B x 64 x 256 x 256\n",
        "\n",
        "        out = self.final_conv(d1)        # B x out_channels x 256 x 256\n",
        "        # Return raw logits (no softmax); losses will handle logits appropriately\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab0024b",
      "metadata": {
        "id": "0ab0024b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Device\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modell instanziieren und auf Gerät verschieben\n",
        "model = UNet(in_channels=1, out_channels=3, base_filters=64)\n",
        "model = model.to(device)\n",
        "\n",
        "# --- einfache Multi-class Dice Loss (numerisch stabil) ---\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, num_classes=3, smooth=1e-6):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, logits, targets_one_hot):\n",
        "        \"\"\"\n",
        "        logits: B x C x H x W (raw output)\n",
        "        targets_one_hot: B x C x H x W (one-hot targets, floats 0/1)\n",
        "        \"\"\"\n",
        "        # Convert logits to probabilities with softmax along channel dim\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        # Flatten spatial dims\n",
        "        B, C, H, W = probs.shape\n",
        "        probs_flat = probs.view(B, C, -1)            # B x C x (H*W)\n",
        "        targets_flat = targets_one_hot.view(B, C, -1).float()\n",
        "\n",
        "        # Dice per class per batch element\n",
        "        intersection = (probs_flat * targets_flat).sum(-1)   # B x C\n",
        "        denom = probs_flat.sum(-1) + targets_flat.sum(-1)   # B x C\n",
        "\n",
        "        dice = (2.0 * intersection + self.smooth) / (denom + self.smooth)  # B x C\n",
        "        # average across classes, then across batch\n",
        "        loss = 1.0 - dice.mean()\n",
        "        return loss\n",
        "\n",
        "# Instantiate dice loss\n",
        "dice_loss = DiceLoss(num_classes=3)\n",
        "\n",
        "# CrossEntropyLoss for multi-class segmentation (expects logits BxC xH xW and targets BxH xW)\n",
        "ce_loss = nn.CrossEntropyLoss(\n",
        "    weight=torch.tensor([1.0, 5.0, 20.0], device=device),\n",
        "    reduction=\"mean\",\n",
        "    # ignore_index could be used if you mask background in targets\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cX3ZORkH-y2q",
      "metadata": {
        "id": "cX3ZORkH-y2q"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as nnf\n",
        "\n",
        "num_epochs = 5\n",
        "dice_weight = 1.0\n",
        "ce_weight = 1.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for step, (data, c_targets, oh_targets) in enumerate(\n",
        "        tqdm(train_dataloader, desc=f\"Train Epoch {epoch+1}/{num_epochs}\")\n",
        "    ):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ---- FIX: falls data 3 Kanäle hat → auf 1 Kanal reduzieren ----\n",
        "        if data.shape[1] != 1:\n",
        "            data = data[:, :1, :, :]     # <-- wichtigste Änderung\n",
        "\n",
        "        data = data.to(device)\n",
        "        c_targets = c_targets.to(device)\n",
        "        oh_targets = oh_targets.to(device)\n",
        "\n",
        "        predictions = model(data)\n",
        "\n",
        "        loss_1 = dice_loss(predictions, oh_targets)\n",
        "        loss_2 = ce_loss(predictions, c_targets)\n",
        "        total_loss = loss_1 * dice_weight + loss_2 * ce_weight\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}] Step [{step+1}/{len(train_dataloader)}] \"\n",
        "                f\"Loss: {total_loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += total_loss.item() * data.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_dataloader.dataset)\n",
        "    print(f\"Epoch {epoch+1} training loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    # =======================\n",
        "    #       VALIDATION\n",
        "    # =======================\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        losses = []\n",
        "        background_dices = []\n",
        "        background_counts = []\n",
        "        liver_dices = []\n",
        "        liver_counts = []\n",
        "        lesion_dices = []\n",
        "        lesion_counts = []\n",
        "\n",
        "        for val_step, (data, c_targets, oh_targets) in enumerate(\n",
        "            tqdm(val_dataloader, desc=\"Validation\")\n",
        "        ):\n",
        "\n",
        "            # ---- FIX: falls data 3 Kanäle hat → auf 1 Kanal reduzieren ----\n",
        "            if data.shape[1] != 1:\n",
        "                data = data[:, :1, :, :]\n",
        "\n",
        "            data = data.to(device)\n",
        "            c_targets = c_targets.to(device)\n",
        "            oh_targets = oh_targets.to(device)\n",
        "\n",
        "            predictions = model(data)\n",
        "\n",
        "            p_arg = nnf.one_hot(torch.argmax(predictions, dim=1), num_classes=3).moveaxis(-1, 1)\n",
        "\n",
        "            loss_1 = dice_loss(predictions, oh_targets)\n",
        "            loss_2 = ce_loss(predictions, c_targets)\n",
        "            total_loss = loss_1 * dice_weight + loss_2 * ce_weight\n",
        "            losses.append(total_loss.item())\n",
        "\n",
        "            bs = data.size(0)\n",
        "            background_seg = oh_targets[:, 0, :, :].cpu()\n",
        "            liver_seg = oh_targets[:, 1, :, :].cpu()\n",
        "            lesion_seg = oh_targets[:, 2, :, :].cpu()\n",
        "\n",
        "            background_dice = compute_dice_score(p_arg[:,0,:,:].cpu(), background_seg)\n",
        "            background_counts.append(bs)\n",
        "            background_dices.append(background_dice)\n",
        "\n",
        "            if liver_seg.sum() != 0.0:\n",
        "                liver_dice = compute_dice_score(p_arg[:,1,:,:].cpu(), liver_seg)\n",
        "                liver_counts.append(bs)\n",
        "                liver_dices.append(liver_dice)\n",
        "\n",
        "            if lesion_seg.sum() != 0.0:\n",
        "                lesion_dice = compute_dice_score(p_arg[:,2,:,:].cpu(), lesion_seg)\n",
        "                lesion_counts.append(bs)\n",
        "                lesion_dices.append(lesion_dice)\n",
        "\n",
        "        avg_background_dice = sum([d*s for d,s in zip(background_dices, background_counts)]) / sum(background_counts)\n",
        "        avg_liver_dice = (sum([d*s for d,s in zip(liver_dices, liver_counts)]) / sum(liver_counts)) if liver_counts else 0.0\n",
        "        avg_lesion_dice = (sum([d*s for d,s in zip(lesion_dices, lesion_counts)]) / sum(lesion_counts)) if lesion_counts else 0.0\n",
        "\n",
        "        avg_loss = sum(losses) / len(losses)\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1}, Validation Loss: {avg_loss:.4f}, \"\n",
        "            f\"Liver Dice: {avg_liver_dice:.4f}, Lesion Dice: {avg_lesion_dice:.4f}\"\n",
        "        )\n",
        "\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fVDuNErcj7T",
      "metadata": {
        "id": "7fVDuNErcj7T"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    losses = []\n",
        "    background_dices = []\n",
        "    background_counts = []\n",
        "    liver_dices = []\n",
        "    liver_counts = []\n",
        "    lesion_dices = []\n",
        "    lesion_counts = []\n",
        "\n",
        "    for test_step, (data, c_targets, oh_targets) in enumerate(tqdm(test_dataloader, desc=\"Testing\")):\n",
        "        data = data.to(device)\n",
        "        c_targets = c_targets.to(device)\n",
        "        oh_targets = oh_targets.to(device)\n",
        "\n",
        "        predictions = model(data)\n",
        "        p_arg = nnf.one_hot(torch.argmax(predictions, dim=1), num_classes=3).moveaxis(-1, 1)\n",
        "\n",
        "        loss_1 = dice_loss(predictions, oh_targets)\n",
        "        loss_2 = ce_loss(predictions, c_targets)\n",
        "        total_loss = loss_1 * dice_weight + loss_2 * ce_weight\n",
        "        losses.append(total_loss.item())\n",
        "\n",
        "        bs = data.size(0)\n",
        "        background_seg = oh_targets[:, 0, :, :].cpu()\n",
        "        liver_seg = oh_targets[:, 1, :, :].cpu()\n",
        "        lesion_seg = oh_targets[:, 2, :, :].cpu()\n",
        "\n",
        "        background_dice = compute_dice_score(p_arg[:,0,:,:].cpu(), background_seg)\n",
        "        background_counts.append(bs)\n",
        "        background_dices.append(background_dice)\n",
        "\n",
        "        if liver_seg.sum() != 0.0:\n",
        "            liver_dice = compute_dice_score(p_arg[:,1,:,:].cpu(), liver_seg)\n",
        "            liver_counts.append(bs)\n",
        "            liver_dices.append(liver_dice)\n",
        "\n",
        "        if lesion_seg.sum() != 0.0:\n",
        "            lesion_dice = compute_dice_score(p_arg[:,2,:,:].cpu(), lesion_seg)\n",
        "            lesion_counts.append(bs)\n",
        "            lesion_dices.append(lesion_dice)\n",
        "\n",
        "    avg_background_dice = sum([dice * size for dice, size in zip(background_dices, background_counts)]) / sum(background_counts)\n",
        "    avg_liver_dice = sum([dice * size for dice, size in zip(liver_dices, liver_counts)]) / sum(liver_counts) if liver_counts else 0.0\n",
        "    avg_lesion_dice = sum([dice * size for dice, size in zip(lesion_dices, lesion_counts)]) / sum(lesion_counts) if lesion_counts else 0.0\n",
        "    avg_loss = sum(losses) / len(losses)\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Liver Dice: {avg_liver_dice:.4f}, Lesion Dice: {avg_lesion_dice:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oDnF9k7ZHhkE",
      "metadata": {
        "id": "oDnF9k7ZHhkE"
      },
      "outputs": [],
      "source": [
        "# Try looking at some images and predicted segmentations to see how badly or how well you've done\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_predictions(data, oh_targets, predictions, idx=0):\n",
        "    \"\"\"\n",
        "    data: B x 1 x H x W  (tensor)\n",
        "    oh_targets: B x C x H x W (one-hot)\n",
        "    predictions: logits B x C x H x W\n",
        "    idx: index in batch to display\n",
        "    \"\"\"\n",
        "    img = data[idx,0].cpu().numpy()\n",
        "    gt = np.argmax(oh_targets[idx].cpu().numpy(), axis=0)      # H x W : {0,1,2}\n",
        "    pred = np.argmax(predictions[idx].cpu().numpy(), axis=0)   # H x W\n",
        "\n",
        "    fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
        "    axes[0].imshow(img, cmap='gray')\n",
        "    axes[0].set_title(\"Input\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(gt, cmap='gray')\n",
        "    axes[1].set_title(\"Ground Truth (0:bg,1:liver,2:lesion)\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(pred, cmap='gray')\n",
        "    axes[2].set_title(\"Prediction\")\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Beispiel: nimm ein Batch aus dem val_dataloader\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    data, c_targets, oh_targets = next(iter(val_dataloader))\n",
        "    data = data.to(device)\n",
        "    preds = model(data)\n",
        "    show_predictions(data.cpu(), oh_targets.cpu(), preds.cpu(), idx=0)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
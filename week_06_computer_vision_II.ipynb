{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5a61491",
      "metadata": {
        "id": "a5a61491"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77f8126",
      "metadata": {
        "id": "c77f8126"
      },
      "source": [
        "**This week's exercise has 3 tasks, for a total of 10.5 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building core components of modern PyTorch models\n",
        "- Assembling modern PyTorch models from components\n",
        "- Training a modern model on a real-world task and achieving passable results\n",
        "\n",
        "**Note**: Since you have already proven that you are capable of creating the core components of a typical training loop yourself, we will provide some utility code for this section. This is done so that you can focus on the important parts of this lesson, and to help us debug your code in case you need help."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4f7614",
      "metadata": {
        "id": "8e4f7614"
      },
      "source": [
        "#### Chapter 3.1 - Data Augmentation\n",
        "\n",
        "**What is data augmentation?**\n",
        "Have you ever lost your glasses and then squinted, or tried to look through a rainy window? Or looked at a false color image, maybe a forest where the trees are blue and the sky green? You can usually make an educated guess what you are looking at, even though the image you see is different than usual. This is, essentialy, what data augmentation is. It's the same data, still recognizable, but slightly altered in some way.\n",
        "\n",
        "**What is that useful for?**\n",
        "Let me begin with an anecdote that you've probably heard in the lectures. Say you have pictures of cats and dogs, and want your model to tell the two apart. How many people you know go to the park with their dogs? I imagine many. Hence, many images of dogs are dogs lying on the grass. The same is generally untrue for cats, at least I have never heard of anyone walking their cat to the park. At any rate, here is what happens when I train a neural network on these images: The model takes a shortcut. It sees a lot of green and the correct answer for these pictures is always \"Dog\". It learns \"Green = Dog\". This is what we call overfitting. We have overfitted to the existence of green in the background as a quintessential part of what makes a dog. Sometimes, we get away with this, if our data always has this correlation.\n",
        "\n",
        "Now I get some new data. A bunch of people have taken pictures of their cats, sunbathing on the terrace. The garden is in the background. Lots of green. The model, in its infinite wisdom, will at first guess that these images are of dogs. Clearly, our model's ability to tell apart cats and dogs has not generalized to this new dataset.\n",
        "\n",
        "So how can we prevent the model from taking shortcuts and encourage learning information that generalizes? We force these generalizations in training. If I gave you an image of a dog, but the grass was brown, and the dog green, you could still identify it as a dog, instead of a cat, right? And so should the model, if we can manage it. So let's also make it train using pictures of cats and dogs where the colours are different or removed. Suddenly, the shortcut solution is no longer useful, and the model must rely on shape, texture, or contextual information like a leash. The practice of color change described is a practical and useful data augmentation that is used in state-of-the-art image recognition.\n",
        "\n",
        "In addition to color changes, there is a myriad of other techniques, such as cropping, image rotation or flipping, edge filters, solarization, random noise, and many, many more. Basically, anything that you believe may eventually show up in testing data and that you want the model to generalize to, can be made into a corresponding data augmentation.\n",
        "\n",
        "**How do we use data augmentations in practice?**\n",
        "There are two ways of adding data augmentation during training. Either, you can implement it inside of your dataset, so that it only returns augmented image tensors, or right before feeding your image tensors into your model. Both options are acceptable and come with advantages and disadvantages, although the more common way is to separate dataset and augmentations. We also showcase the native PyTorch way of augmenting data below.\n",
        "\n",
        "If you are particularly eager, or want to try your hand at making image augmentation functions yourself, it can be fun and is definitely good practice. However, PyTorch comes with a large selection of image augmentations right out of the box, and in the following chapter, we will look at how to make use of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e352a55a",
      "metadata": {
        "id": "e352a55a",
        "outputId": "a1471c54-22f5-42f6-c401-d601f2b61034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.transforms.functional as ttf\n",
        "\n",
        "# Torchvision contains two ways of utilizing transforms:\n",
        "# Functional and Composed.\n",
        "\n",
        "# Functional does what it advertises - it is a function which\n",
        "# you can use on your tensors. Here is an example which performs\n",
        "# a center crop:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transformed_images = ttf.center_crop(img = dummy_images, output_size = (128, 128))\n",
        "print(transformed_images.size())\n",
        "\n",
        "# Functional transforms have the inherent advantage of giving the\n",
        "# user very fine-grained control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "55a993c1",
      "metadata": {
        "id": "55a993c1",
        "outputId": "2207919d-dfd4-4a62-8556-55b4b45316a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# The alternative is the so-called Composed form, which uses\n",
        "# classes to achieve the same result. We make a Composed Transform\n",
        "# like so:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transforms = tt.Compose([\n",
        "    tt.RandomCrop(size = (128, 128)),\n",
        "    tt.RandomHorizontalFlip()\n",
        "])\n",
        "transformed_images = transforms(dummy_images)\n",
        "print(transformed_images.size())\n",
        "\n",
        "# As you can see, Compose offers us the option of sequentially\n",
        "# executing multiple transformations in a single line of code.\n",
        "# We also get the option of using randomized augmentations,\n",
        "# where the randomization is already done for us.\n",
        "\n",
        "# In practice, either style of writing transformations is fine.\n",
        "# In fact, they are equivalent, as Compose calls the functional\n",
        "# versions of the transforms under the hood. In the case of\n",
        "# randomized augmentations, the class handles all the randomizing\n",
        "# and then calls the functional transform with the random inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b4a34c",
      "metadata": {
        "id": "b8b4a34c"
      },
      "source": [
        "**Task 1 (1+1 points)**: A complete list of Torchvision's available transforms can be found here: https://docs.pytorch.org/vision/0.9/transforms.html. Consider the task we are working on right now - working with CT images from the LiTS 2017 dataset. Which data augmentations strike you as a good idea to add to our training **(1 point)**? Which do you think are a bad idea or cannot work at all **(1 point)**? Are there any which are missing in Torchvision? If you don't know what they do, try them out and judge for yourselves. Can you think of other image types with other physics behind them? Are the rules for them going to be different?\n",
        "\n",
        "There are no definitely correct or incorrect answers here. The goal for this task is for you to be able to argue your case convincingly (to us) and think closely about your dataset. You can test your assumptions when completing the other tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOT HELPFUL:\n",
        "classtorchvision.transforms.CenterCrop(size)  -Could potentially crop of the tumor and then our truths are suddenly wrong ---> not good\n",
        "\n",
        "torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)[source]\n",
        "Randomly change the brightness, contrast, saturation and hue of an image. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions. If img is PIL Image, mode “1”, “L”, “I”, “F” and modes with transparency (alpha channel) are not supported.\n",
        "--> if really random and not consistent in the image it could make areas appear like a tumor or make a tumor disappear (by making the surrounding area too bright)\n",
        "\n",
        "torchvision.transforms.FiveCrop(size)[source]\n",
        "Crop the given image into four corners and the central crop. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n",
        "-->a crop can cut the tumor partially or completely out of a given crop, which can harm detection performance, not helpfull\n",
        "\n",
        "torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')\n",
        "\n",
        "\n",
        "Unnecessary\n",
        "transform.resize/scale"
      ],
      "metadata": {
        "id": "bPZRkVpRjy_e"
      },
      "id": "bPZRkVpRjy_e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "HELPFUL:\n",
        "torchvision.transforms.Grayscale(num_output_channels=1)\n",
        "Convert image to grayscale. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions\n",
        "-> good, less calculating capacity, maybe better contrast\n",
        "\n",
        "\n",
        "classtorchvision.transforms.Pad(padding, fill=0, padding_mode='constant')[source]\n",
        "Pad the given image on all sides with the given “pad” value. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means at most 2 leading dimensions for mode reflect and symmetric, at most 3 leading dimensions for mode edge, and an arbitrary number of leading dimensions for mode constant\n",
        "-> helpful to provide context near edge, good for tumors on the edge of the scan\n",
        "\n",
        "classtorchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=<InterpolationMode.NEAREST: 'nearest'>, fill=0, fillcolor=None, resample=None)[source]\n",
        "Random affine transformation of the image keeping center invariant. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions. (ROTATION; Sliding; etc)\n",
        "-> helps to not learn just the spot of the tumor\n",
        "\n",
        "classtorchvision.transforms.RandomApply(transforms, p=0.5)\n",
        "You can define what transformations should be done, but they are only done randomly with certain percentages\n",
        "-> very good, because it doesn`T transform all data\n",
        "\n",
        "All random functions of those things are also good e.g\n",
        "random.grayscale\n",
        "random.horizontal.flip /vertical.flip\n",
        "random.perspective\n",
        "random.rotation\n",
        "\n",
        "\n",
        "torchvision.transforms.GaussianBlur(kernel_size, sigma=(0.1, 2.0))\n",
        "-> including bad quality\n"
      ],
      "metadata": {
        "id": "bj5SyEoblnXM"
      },
      "id": "bj5SyEoblnXM"
    },
    {
      "cell_type": "markdown",
      "id": "325ff768",
      "metadata": {
        "id": "325ff768"
      },
      "source": [
        "#### Chapter 3.2 - Regularization Techniques\n",
        "\n",
        "While there are multiple definitions or guidelines for what regularization is supposed to do (see lectures), in terms of practical concerns, all regularization techniques have the same aim, expressed through different means: Improving some aspect of the performance of your deep learning models. We differentiate them, broadly, from data augmentations, because regularization techniques generally concern themselves with the learning process, e.g. loss function modifications, learning rate optimizations, temporary model modifications, etc., and *not* the underlying data in our model training.\n",
        "\n",
        "There are a number of different strategies, far too many to list all of them here, but a few particularly successful ones have made it into common use - so much so that they are more prevalent than regularization-less, \"vanilla\" deep learning. These fall into different groups, briefly discussed below.\n",
        "\n",
        "#### Additional Loss Components\n",
        "\n",
        "The loss function for any given modern optimization task is typically continuous and not always smooth everywhere. As a consequence, there are many different parameter configurations in a model that result in the same train-time loss. Not all of these express the same behavior during training or testing, however. When we modify our loss to penalize certain training behaviors, we allow the training process to select for models and parameters that give models that generalize better, converge to a solution faster, etc.,  despite often expressing the same training loss. Let's look at some examples that should be familiar from the lecture:\n",
        "\n",
        "**L1 Loss** - Often also called LASSO, L1 Loss is a penalty term added to the normal loss during training, which is defined as:\n",
        "$L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|$. Growing linearly with parameter magnitude, we penalize the model. This, in turn, forces the model to use fewer and smaller weights - relying on more weights than it needs, and thus probably overfitting, is disincentivized. Similarly, we just forced our model to stick to weights near zero, which we already know is generally a preferable area for model activations to stick to.\n",
        "\n",
        "**L2 Loss** - L2 Loss is the more popular cousin of the L1 Loss, which does approximately the same thing, except the penalty is equal to the sum over all squared parameter magnitudes: $L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|^2$ The reasoning behind it is similar, but it has seen far more practical adoption.\n",
        "\n",
        "**Weight Decay** - An operation that effectively performs the same duty, weight decay reduces the magnitude of weights after each backward pass, for example by subtracting a small constant or multiplying with a factor. In essence, this eliminates parameters which are rarely \"used\" and were thus likely involved in overfitting on a small amount of data anyway. Parameters that are regularly updated (and therefore probably useful), will always remain near their optimal value despite weight decay. Interestingly, Weight Decay is mathematically equivalent to L2 Loss in terms of net parameter updates.\n",
        "\n",
        "#### Training Strategies\n",
        "**Early Stopping**: For early stopping, we monitor the performance of the model on a validation set and stop training when the performance stops improving. This ensures that the model does not continue to train on the training data and potentially overfit, while also saving computational resources.\n",
        "\n",
        "**Dropout**: Dropout is a regularization technique where, during training, a random subset of neurons in a layer is \"dropped out\" (set to zero) for each forward pass. This prevents the network from relying too heavily on specific neurons and encourages it to learn more robust and generalized features. During inference, all neurons are used, but their outputs are scaled to account for the dropout during training.\n",
        "\n",
        "**Learning Rate Scheduling (LR Scheduling)**: Learning rate scheduling involves dynamically adjusting the learning rate during training. A high learning rate at the start helps the model converge quickly, while a lower learning rate later allows for fine-tuning. Common strategies include step decay, exponential decay, and cosine annealing. Proper learning rate scheduling can lead to faster convergence and better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cd62d1",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "f6cd62d1"
      },
      "source": [
        "**Task 1.5 (3 x 0.5 points)**: Let us implement and compare the effects of different regularization techniques on a simple neural network. To do so, follow these steps:\n",
        "\n",
        "1. Create a small neural network (e.g., 2-3 layers) and train it on the LiTS dataset without any regularization. Record the training and validation accuracy/loss. (P.S.: You can use the model from last week's exercise as a starting point.)\n",
        "2. Add L2 regularization to the model and observe how it affects the training and validation performance (Check the Adam optimizer documentation to find out how to add this regularization). Compare the results with the unregularized model.\n",
        "3. Add dropout to the model and repeat the training process (check the PyTorch documentation to find out how to add this regularization - you do not need to implement it yourself). Compare the results with the previous models.\n",
        "\n",
        "For each step, make sure to copy the relevant code snippets into a new cell, instead of modifying the existing code. This way, we can keep track of the different versions of the model and their performances.\n",
        "\n",
        "You might want to look up the documentation for implementing these techniques in PyTorch.\n",
        "\n",
        "For each regularization technique, explain how it impacts the model's performance and generalization. Which combination of techniques works best for this dataset? Why do you think that is the case?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as ttf\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL"
      ],
      "metadata": {
        "id": "No91MSYuileo"
      },
      "id": "No91MSYuileo",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download our data again:\n",
        "#!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "#!rm -rf ./sample_data/\n",
        "!rm -rf ./Clean_LiTS\n",
        "!unzip -qq ./drive/MyDrive/Clean_LiTS.zip -d .\n",
        "#!rm ./Clean_LiTS.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ysWOilT4ioyz",
        "outputId": "215c87a6-e6d5-41b2-b5bc-64f2a7272a96"
      },
      "id": "ysWOilT4ioyz",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f91bab93",
      "metadata": {
        "id": "f91bab93"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class LiTS_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    For our sample solution, we go for the easier variant.\n",
        "\n",
        "    In this specific dataset, we don't load the images until we need them - for a\n",
        "    short training, or limited resources, this is good behavior. If you have the\n",
        "    necessary RAM to pre-load all of your data, you don't have to load the data\n",
        "    multiple times, and save compute costs in the long run. The downside is that\n",
        "    when you are trying to debug, you wait for ages every time, and if you simply\n",
        "    do not have the compute resources, you can't even do it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        image = image.to(dtype = torch.float32)\n",
        "        image -= torch.min(image)\n",
        "        image /= torch.max(image)\n",
        "\n",
        "        liver_visible = self.data.loc[idx, \"liver_visible\"]\n",
        "        lesion_visible = self.data.loc[idx, \"lesion_visible\"]\n",
        "        # Note that targets must have the data type torch.long - a 64-bit integer,\n",
        "        # unlike the image tensor, which is usually a 32-bit float, the default\n",
        "        # dtype for tensors when none is given\n",
        "        if lesion_visible and liver_visible:\n",
        "            target = torch.tensor(2, dtype = torch.long)\n",
        "        elif not lesion_visible and liver_visible:\n",
        "            target = torch.tensor(1, dtype = torch.long)\n",
        "        elif not lesion_visible and not liver_visible:\n",
        "            target = torch.tensor(0, dtype = torch.long)\n",
        "        else:\n",
        "            print(\n",
        "                idx,\n",
        "                lesion_visible,\n",
        "                liver_visible,\n",
        "                self.data.loc[idx, \"liver_visible\"],\n",
        "                self.data.loc[idx, \"lesion_visible\"],\n",
        "                self.data.loc[idx, \"filename\"]\n",
        "                )\n",
        "            raise ValueError(\"Invalid target\")\n",
        "\n",
        "        return image, target\n",
        "\n",
        "train_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bdaccb5c",
      "metadata": {
        "id": "bdaccb5c"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#class Really_Good_Model_forLIST(nn.Module):\n",
        "#    \"\"\"\n",
        "#    Ein kleines CNN, das am Ende 3 Klassen ausgibt.\n",
        "#    \"\"\"\n",
        "#\n",
        "#    def __init__(self, input_shape):\n",
        "#        super().__init__()\n",
        "\n",
        "\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 12, kernel_size=3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(12, 12, kernel_size=3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))  # auf 1x1 reduzieren\n",
        "        self.fc = nn.Linear(12, out_classes)  # 12 Kanäle -> out_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efe25fa",
      "metadata": {
        "id": "5efe25fa"
      },
      "outputs": [],
      "source": [
        "# Now for the training loop\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e04157a",
      "metadata": {
        "id": "9e04157a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a9ccc2-6488-40e6-a31b-42ca483163fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1651\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.1379\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 1.1161\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 1.0839\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 1.0653\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 1.0417\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.9622\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 1.0055\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.8643\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.7053\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.8465\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.5765\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.6663\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.7446\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 1.0907\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.7752\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.8394\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.8871\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.6086\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.9842\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.5847\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 1.0563\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 1.0455\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.6558\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.8340\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 1.1081\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.8124\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 1.0712\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.9957\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.6111\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.6692\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.9228\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.7413\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.7086\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.5544\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.9747\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.7561\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.6834\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 1.0990\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.8071\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 1.0080\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.6160\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.5741\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.7598\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.7607\n",
            "Epoch: 1,\t Validation Loss: 0.8262,\t Accuracy: 0.6927\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.4992\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.9451\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.6899\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 1.0336\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.5312\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 1.0640\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.7983\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.7891\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.5758\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.8006\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.8326\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.6506\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.8787\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.9209\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 1.0099\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 1.0216\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.7220\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.7231\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.7561\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.7104\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.8761\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.7936\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 1.0513\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.8475\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.7500\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.8324\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.7506\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.7821\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.7606\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.8884\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.9368\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.5569\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 1.1691\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.7258\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.7950\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.7218\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.6135\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.9237\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.9648\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.7162\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.8769\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.7530\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.8928\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.7882\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.7996\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 1.0761\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.7962\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.8259\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 1.0991\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 1.0841\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.8371\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 1.1227\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.9289\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.8159\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.9145\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.7919\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.7817\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.7735\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.7452\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.5447\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.6345\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 1.1185\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.7018\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.9730\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.6460\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.9043\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.6430\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 1.2524\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.7080\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.8929\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.7793\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.7545\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.8931\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.6985\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.8812\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.8456\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.9038\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.8261\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.6831\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.7828\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.8724\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.9036\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.7283\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.7759\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 1.0601\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.8725\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.7442\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.8730\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.8613\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.8234\n",
            "Epoch: 3,\t Validation Loss: 0.8162,\t Accuracy: 0.6927\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.7886\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 1.1336\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.9109\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.9580\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.8699\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.6847\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.7229\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.9476\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.8769\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 1.2427\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.8093\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.9637\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.9474\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.8603\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.9085\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 1.1303\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.6301\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.7818\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.8992\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.8920\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.6137\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.6487\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.7004\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.3869\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.6659\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.6999\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.9107\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.8154\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.5284\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 1.0340\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.8404\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.7245\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 1.0042\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.8693\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 1.2894\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.6836\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.8057\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.9854\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.6870\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.5458\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.5479\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.7731\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.6546\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.8367\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.9382\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.9645\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.8832\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.8520\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.8993\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.9251\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.8384\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.7739\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.8467\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.7421\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 1.0179\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 1.2254\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.8274\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 1.1202\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.6344\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.9137\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.9768\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.6345\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.9171\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.8173\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.6181\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.8099\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.8099\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.9691\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.6656\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.7125\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.7435\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.9265\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.7284\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.6655\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 1.0661\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.8138\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 1.0135\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.7729\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.7611\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.9470\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.6905\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.6652\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.7025\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.9557\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 1.1083\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.6214\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.6772\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.9393\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.7626\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.9103\n",
            "Epoch: 5,\t Validation Loss: 0.8147,\t Accuracy: 0.6927\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.6960\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.8689\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 1.0292\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.8918\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 1.0052\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.6999\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.5886\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.9945\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.9362\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.7203\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.7240\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.9050\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.7469\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.9076\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.6013\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.6905\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 1.0731\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.9237\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.9963\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.5637\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.7352\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.9445\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 1.0697\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.6474\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.9787\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.8411\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.9610\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.8278\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.8195\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.7999\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.7632\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.9881\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.8484\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.6409\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 1.0186\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.6092\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 1.0274\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 1.0162\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.7654\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.6624\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 1.0230\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.6314\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.7955\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.7829\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.8939\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.9256\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 1.0050\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 1.0166\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.8879\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.5449\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.7267\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.9734\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.7198\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.7937\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.6726\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.4717\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 1.0185\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 1.0096\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.6318\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.5207\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.7071\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.9031\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.7166\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 1.0765\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.7328\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.8017\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 1.1398\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 1.1343\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.7979\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.7293\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.6002\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.7835\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.8438\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.8313\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.8623\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.6681\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.8816\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.9777\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.7098\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.7140\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.8331\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.8444\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 1.0738\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.7264\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.7682\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.8696\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.7615\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.9125\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.6487\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.7263\n",
            "Epoch: 7,\t Validation Loss: 0.8147,\t Accuracy: 0.6927\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.7820\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.7771\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.9172\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.9610\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.8893\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.7431\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.7046\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.7066\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.5805\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.6615\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 1.0975\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.9749\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.7030\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.9344\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.9512\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 1.0160\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.6917\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.6914\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.6413\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.6189\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.8801\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 1.1335\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 1.0692\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.7128\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.9315\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.8839\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.7489\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 1.0509\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.6290\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 1.1585\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 1.0297\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.7270\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.7536\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.9606\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.8979\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.7702\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.6317\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.6646\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.7814\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.8291\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.8848\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.6645\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.8874\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.8220\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.7082\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 1.1739\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.9523\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.8642\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.5536\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.8482\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.7577\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.8401\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 1.0499\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.6794\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.8713\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.9367\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.8223\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.6779\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.8828\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 1.0743\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.7178\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.8113\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 1.0387\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.7104\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.8601\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.8009\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.8549\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.7276\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.6657\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.9362\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 1.0089\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.6325\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.6274\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.8356\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.8070\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.8038\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.9297\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.7545\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.8656\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.6303\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 1.1444\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.5539\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.6481\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.4974\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.8213\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 1.0594\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.7209\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.9241\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.6183\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.7799\n",
            "Epoch: 9,\t Validation Loss: 0.8145,\t Accuracy: 0.6927\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.7367\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.9181\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.7225\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.5340\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.9617\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.7887\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.8156\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 1.0575\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.7092\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 1.0719\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.7963\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.6133\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.7493\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.9908\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.6561\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.8858\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.6104\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.9684\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.8003\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.7282\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 1.0503\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 1.0305\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.7843\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 1.0745\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.6850\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 1.0276\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.8279\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.7032\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.9887\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.6720\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 1.0096\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.8876\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.7013\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.8339\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.9006\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.6521\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.5687\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.9703\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.9109\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.5672\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.5953\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.7994\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.8935\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.7651\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.6354\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.9776\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.7221\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.8379\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 1.0563\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.9731\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.7497\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 1.0310\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.7649\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 1.1465\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 1.0614\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.7163\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.8375\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.9168\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.6067\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.9491\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 1.1035\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 1.0622\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.9006\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.7161\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.7924\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.5110\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.8646\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.8599\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.8367\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.8227\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.8723\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 1.0156\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.7954\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.7065\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.7572\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.7674\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.8316\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.7625\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.6894\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.7454\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.9444\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.6547\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 1.0374\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.7207\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.5184\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.7594\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.5672\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 1.0273\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.6447\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.7319\n",
            "Epoch: 11,\t Validation Loss: 0.8146,\t Accuracy: 0.6927\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.9803\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.6728\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.7867\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.8348\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.8257\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.9544\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.9868\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.5409\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.8114\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.9643\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.7394\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.9946\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.8098\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.9789\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.7255\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.6859\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.8918\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.8076\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.9985\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.9864\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.7704\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.4902\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.8476\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.5814\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.5369\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.8538\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.7452\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.7149\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.6984\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.9833\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.9487\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 1.0726\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.6494\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.6557\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 1.2241\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.7069\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.7894\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.6809\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 1.0425\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.9328\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.6377\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.6571\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.7379\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.9356\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 1.0859\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.9296\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.5234\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.9727\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.6973\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.7577\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.7703\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 1.2984\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.8497\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.6264\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.9443\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.6063\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.6978\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.8423\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.6630\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.8225\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.7756\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.9382\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.6562\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.8526\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.7133\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.9927\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.9026\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 1.0201\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.7554\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.8451\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.6590\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.9890\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.9054\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.6553\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.7122\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.7588\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.8329\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.8357\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.9256\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.8622\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 1.0158\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.8275\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.6804\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.9319\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 1.0774\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.9829\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.5588\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.6536\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.9559\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.8900\n",
            "Epoch: 13,\t Validation Loss: 0.8136,\t Accuracy: 0.6927\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.8043\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.6676\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.5784\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.7543\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 1.1821\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.8844\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.8797\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.6849\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.9087\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.6017\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.7259\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.8078\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.9307\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.6576\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.7562\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.7503\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.8597\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 1.0084\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.9879\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.6410\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.8079\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 1.1369\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 1.0710\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.8443\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.7282\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.7658\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.7767\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.8300\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.6807\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 1.0361\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.5315\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 1.1208\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.8367\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.4753\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.8846\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.9958\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.6557\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.6865\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.6559\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.3290\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.7982\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.7584\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.9183\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.7111\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.7378\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.6345\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.8832\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.8285\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.6486\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.5639\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.6528\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 1.1550\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.7495\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.8712\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.9518\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.8958\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.6654\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.8551\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.9419\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.9712\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.9034\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.5439\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.6911\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 1.0067\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.5013\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 1.0391\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.8564\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.8230\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.7200\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 1.1780\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.8643\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.8075\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.8382\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.8080\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.5518\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.8191\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.7541\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 1.0713\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.5337\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.7585\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.7637\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 1.1048\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.6552\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 1.1781\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 1.0116\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.9269\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 1.0271\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 1.0151\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.6612\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 1.0203\n",
            "Epoch: 15,\t Validation Loss: 0.8137,\t Accuracy: 0.6927\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_loop(YourModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ff035d",
      "metadata": {
        "id": "10ff035d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d2b78e-1e92-4892-ccfc-abc29adaf0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9142,\t Accuracy: 0.6004\n"
          ]
        }
      ],
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605399f3",
      "metadata": {
        "id": "605399f3"
      },
      "source": [
        "### L2-Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1befc94e",
      "metadata": {
        "id": "1befc94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4d0a81-37fd-480a-cb64-0149fd749d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1946\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.0370\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.9896\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.8152\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.9839\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.7676\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.8934\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 1.0935\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.7872\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.8388\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.6807\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 1.2282\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.9509\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.7976\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.7888\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.7489\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.8403\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.8912\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.7290\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.8809\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.6692\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 1.1708\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.8765\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.7940\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.8076\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.8063\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.8696\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 1.0263\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.7223\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.9988\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.8308\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.5841\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.7950\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.7312\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.7432\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.6504\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.9222\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.7049\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.9204\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.6996\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.9500\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.8426\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.4806\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.8454\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.9792\n",
            "Epoch: 1,\t Validation Loss: 0.8420,\t Accuracy: 0.6927\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.5780\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 1.0822\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.5473\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.8316\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 1.1257\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.9266\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.7943\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.9694\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.4541\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.8106\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.9274\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.7950\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.5384\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.9572\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.7678\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.6191\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.7347\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 1.3297\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.8333\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.7563\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.8679\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.8354\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.9062\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.6507\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.6234\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.5459\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.6769\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.6954\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.9165\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 1.0134\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 1.2507\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.7644\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.7901\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 1.0126\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.9669\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.5749\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.7945\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 1.1849\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.7886\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.9363\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.6276\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.9433\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.7966\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.8920\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.8352\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.7910\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.8320\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 1.1855\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.6623\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.7979\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.8959\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 1.1035\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.8668\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.4524\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.8894\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.9884\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.8889\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.6558\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.7461\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.7676\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 1.0247\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 1.1273\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.5893\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.6795\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.8585\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.5823\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.8459\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 1.0186\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.7009\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.7917\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.9515\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.8958\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.5986\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.7272\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.9472\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.6593\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.6409\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.9701\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.9927\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 1.0750\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.9015\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.8679\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.8086\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.7487\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.5819\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.9766\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.8613\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.5411\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.6869\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.7094\n",
            "Epoch: 3,\t Validation Loss: 0.8211,\t Accuracy: 0.6927\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.8171\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.8651\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.6279\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.5981\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 1.2226\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.5816\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.8498\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.8579\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.8856\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.8602\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.8006\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.6397\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.6822\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.7841\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.8311\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.8364\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 1.1348\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.9659\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.7326\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.7208\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.8469\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.7008\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 1.0332\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.6940\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 1.2815\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.8357\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.8820\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 1.2531\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.8100\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.6877\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.8489\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.9761\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.6679\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 1.1390\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.4783\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.9006\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.8727\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.9263\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.7236\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 1.1289\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.9061\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.7486\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.7078\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 1.0420\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.8291\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.8064\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 1.0428\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.8448\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.8421\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.8836\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.8568\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.6466\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.9551\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.9135\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.8380\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.7547\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.7276\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.7676\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.8908\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.9698\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.5448\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.7547\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.5945\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.9390\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.6231\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.8823\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.9046\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.6125\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.9407\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.8066\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.5955\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.7900\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 1.1373\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.8451\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.9539\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.7545\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.9352\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.6841\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.8220\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.6977\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 1.0786\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.6859\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 1.0123\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 1.1545\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.7713\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.7105\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.9293\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.8326\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 1.0503\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 1.2681\n",
            "Epoch: 5,\t Validation Loss: 0.8171,\t Accuracy: 0.6927\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.6550\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 1.0562\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.9840\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.9410\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.8735\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.6248\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.7558\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.8133\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.9077\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 1.5609\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.8926\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 1.3172\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.9194\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.9118\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.8611\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.7510\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.8558\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.5720\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.7747\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.9132\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 1.1680\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.6134\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.8052\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.7482\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.6736\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.5293\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.5483\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.7120\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 1.1234\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.7068\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.8441\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.8525\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.7899\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.8453\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.8188\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.6956\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.7605\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.8594\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.8256\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.8242\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.8258\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 1.2049\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 1.0035\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.8489\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.9292\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 1.0196\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.7834\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.7812\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.5717\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.8224\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.9899\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 1.2510\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.6651\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.5534\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.6599\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 1.1317\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.6159\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.8166\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.8603\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.9046\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.8022\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.7266\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.8100\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.8313\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.7577\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.7028\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.7152\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.8480\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 1.1998\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 1.1287\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.7233\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.9153\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.8261\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 1.1607\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 1.1296\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.7192\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 1.2119\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.9300\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.6783\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 1.1061\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.8947\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.9174\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.3520\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.6218\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.8283\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.6349\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 1.1056\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.8120\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.8072\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.8378\n",
            "Epoch: 7,\t Validation Loss: 0.8172,\t Accuracy: 0.6927\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.9244\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.8673\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.6949\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.7624\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.7955\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 1.0800\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 1.1819\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.7401\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.8144\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 1.0081\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 1.1671\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 1.0054\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.9372\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.7632\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.9765\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.6493\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.8145\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.8631\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.8519\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.8240\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 1.2137\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.9592\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.8183\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.8077\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.7487\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.7885\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.9948\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 1.1171\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.9866\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.9331\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.7879\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.5742\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.5627\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.5763\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.7677\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.9203\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.8842\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.6014\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.8361\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.8062\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.8687\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.7833\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.7528\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.7842\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.9017\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.5936\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.5048\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.6861\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.5764\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.5129\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 1.0452\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.5643\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.9784\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.9312\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.7892\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.7825\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.8313\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.7187\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.6499\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.9591\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.5390\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.7793\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.8756\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.9868\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.8276\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.9349\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.7458\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.8098\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.6526\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.5373\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.6272\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.8214\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.6200\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.7412\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.8530\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.8945\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.6864\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.5576\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.9091\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.7573\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.4615\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.6690\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 1.1376\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.8664\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.8420\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 1.0058\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.8903\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.8499\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 1.0600\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.8644\n",
            "Epoch: 9,\t Validation Loss: 0.8165,\t Accuracy: 0.6927\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.6376\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 1.0876\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.9279\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 1.2167\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 1.0687\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.9408\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.8548\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.7769\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.8089\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.8042\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 1.0770\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.8501\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.6869\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.7297\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.8829\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.5574\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.6318\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 1.0294\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.5730\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.9471\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.7285\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.7953\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.8505\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.6149\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.9993\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 1.0124\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.7009\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.6576\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.8756\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.7286\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.7286\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.5238\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 1.0331\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 1.2568\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.8867\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 1.0115\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.4488\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.7637\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.6298\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 1.0866\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.7619\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.5993\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 1.0084\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.9804\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.9052\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.7801\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.7409\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.7105\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.7781\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.6038\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.8645\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.9303\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.5347\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.7273\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.8629\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.9950\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.8025\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.8905\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.9892\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.7904\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.7922\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.6300\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.9069\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.9840\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.9464\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.7755\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.8811\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.7412\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.8789\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.8643\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.7729\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 1.0516\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 1.2703\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.7396\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.8365\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.5464\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.7110\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.6111\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.7892\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.7081\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 1.0627\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.7604\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.7850\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.5445\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.8458\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 1.0479\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.9944\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.6880\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.6599\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.8402\n",
            "Epoch: 11,\t Validation Loss: 0.8160,\t Accuracy: 0.6927\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.6939\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.6820\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.8275\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.8028\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.7145\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 1.1106\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 1.2142\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 1.0873\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.9543\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.7687\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 1.0581\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.8216\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.7016\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.7956\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.8572\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.5555\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.8460\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.7430\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.9013\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.7985\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.7640\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.7231\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.7593\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.8872\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.8807\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.5598\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.8452\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.9394\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.8070\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.9087\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.9106\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.8410\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.7519\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.7687\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.7413\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.9407\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 1.1487\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.7990\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.6734\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.7388\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.6768\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.9317\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.5578\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.9829\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 1.1259\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.7654\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.8771\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.7475\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 1.4583\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.6405\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.8374\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.6864\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.9058\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 1.0863\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.8727\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.5776\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 1.3834\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.6004\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 1.0189\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.6102\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.7164\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.7888\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.7837\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.8540\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.6662\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.7728\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.6838\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 1.0441\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.8198\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.7972\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.8787\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.7371\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.8847\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.5898\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.6036\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.8986\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.9772\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.6894\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.7956\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.8183\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.6026\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.5426\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.8570\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.9667\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 1.0014\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.6505\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.9921\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 1.0004\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 1.0211\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.7066\n",
            "Epoch: 13,\t Validation Loss: 0.8159,\t Accuracy: 0.6927\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.7092\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.8552\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.7595\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.6677\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.7634\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 1.0866\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 1.1379\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.6400\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.6826\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.7595\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.7661\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.5537\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 1.0399\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.6312\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.5725\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.8244\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.8632\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.7839\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.5853\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.6093\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.9216\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.7108\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.8336\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.8850\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.8648\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.7053\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.7903\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.8726\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.6528\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.8723\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.8043\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.7391\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.5783\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.6928\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.9952\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.5557\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.7681\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.7213\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.5375\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.6188\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.6842\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.5333\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.8473\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.6274\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 1.0622\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.6922\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.8838\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.8692\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.7674\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.6693\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.7821\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.9870\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.5112\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.7075\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.7939\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.8180\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.9860\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.9593\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.5073\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.7381\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.6975\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.9671\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.8937\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.8078\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.5778\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.8370\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.5423\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.7326\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.8277\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 1.1448\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.7195\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.7730\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.7847\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 1.0134\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.7390\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.9648\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.9490\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.8837\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.7049\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.8137\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.7946\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.9512\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.8570\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.6797\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.6377\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.9754\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.8052\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.7116\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.8476\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.7015\n",
            "Epoch: 15,\t Validation Loss: 0.8130,\t Accuracy: 0.6927\n"
          ]
        }
      ],
      "source": [
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop_L2(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model\n",
        "trained_model_L2 = train_loop_L2(YourModel)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_model_L2.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_L2(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SYmkjfQ1pRj",
        "outputId": "42b9777d-2691-49d8-ea2b-c963ce76f5a9"
      },
      "id": "7SYmkjfQ1pRj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9193,\t Accuracy: 0.6004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e200dbd0",
      "metadata": {
        "id": "e200dbd0"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811813fc",
      "metadata": {
        "id": "811813fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class YourModel_drop(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 12, kernel_size=3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(12, 12, kernel_size=3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4,4))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(p=0.5)  # 50% Dropout\n",
        "        self.fc = nn.Linear(12*4*4, out_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)  # Dropout nur während Training\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params = model.parameters(), lr=3e-4)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_model_drop = train_loop(YourModel_drop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W1ic5av5H7j",
        "outputId": "12a366c8-5d37-46e2-f4bd-630542477a5f"
      },
      "id": "_W1ic5av5H7j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.0929\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.0745\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.9964\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.8874\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 1.1231\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 1.0615\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.9312\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.6413\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.8140\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 1.0501\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.8749\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 1.1840\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.7739\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 1.1292\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.7792\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.8944\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.8995\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.9590\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.7237\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.8245\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 1.0166\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.7294\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.6578\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.6763\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.5508\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.9969\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.8768\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.7209\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.7854\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 1.1028\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.5591\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 1.1133\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.8483\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 1.0867\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 1.0790\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 1.1156\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 1.0549\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 1.0469\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.5451\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.7690\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.9499\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.6550\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.8893\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.8709\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.8590\n",
            "Epoch: 1,\t Validation Loss: 0.8128,\t Accuracy: 0.6927\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.8810\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.8925\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.7096\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.5397\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 1.0431\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.6389\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.8630\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.7588\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.7225\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.8435\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.8390\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.6203\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.7177\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.8418\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.8779\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.6745\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.8173\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 1.0010\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.7271\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.8569\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.8347\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.6600\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.9242\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.9205\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 1.2393\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 1.0258\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.6316\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.8004\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.9242\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 1.1601\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.7161\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.9173\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.6737\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.7820\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.7123\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 1.0814\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 1.1628\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.5598\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.7887\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.7887\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.8936\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 1.0086\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 1.0330\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.9110\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.8869\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.5891\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.9807\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 1.0329\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 1.0260\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 1.0278\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.7095\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.7828\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.7180\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.9594\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.8781\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.7237\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.5128\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.8130\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.6374\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.8591\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.8450\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 1.0381\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.6198\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.7497\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.8378\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.7690\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.7838\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.7709\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.8456\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.9761\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.4615\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.6722\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.5931\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.6436\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.7681\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.4934\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.8454\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.8893\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.4827\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.6313\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.9168\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.6689\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 1.0049\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.6938\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.5755\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.6388\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.6958\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.5622\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.7393\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.7499\n",
            "Epoch: 3,\t Validation Loss: 0.7324,\t Accuracy: 0.6927\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.6897\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.7396\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.9773\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.6726\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 1.0848\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.8299\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.7769\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.7940\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.7838\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.9490\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.5643\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 1.0601\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.4134\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.6569\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.7244\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.6821\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.9863\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.6680\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.7102\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.6297\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.7849\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.7112\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.7817\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.4778\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.7530\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.7521\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.8192\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.6097\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.6833\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.8186\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.3581\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.7523\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.9082\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.8566\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.5802\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.7044\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.5757\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.8275\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.7443\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.5646\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.7462\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.6183\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.8786\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.8049\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.5268\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.8746\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.7418\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.9306\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.8906\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.9482\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.8120\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.6339\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.6952\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.8167\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.6642\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.6957\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.6857\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.8791\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.6313\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.5235\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.8891\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.6584\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.6590\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.7170\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.6463\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.8532\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.5637\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.6879\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.5432\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.5662\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 1.1772\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.7647\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.6629\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.8320\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.9108\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.7965\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.5465\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.7175\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.6138\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.6884\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.7093\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.7854\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.9265\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.9767\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.7783\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.9563\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.7988\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.4923\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.5793\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.6882\n",
            "Epoch: 5,\t Validation Loss: 0.6662,\t Accuracy: 0.7121\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.8133\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.4118\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.8139\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.6711\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.6636\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.5904\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.7593\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.6631\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.6965\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.6606\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.5862\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.6845\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.5065\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.9351\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.6166\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.8485\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.5490\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.7549\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.3823\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.5455\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.6738\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.6979\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.7786\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.8353\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.4727\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.6899\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.8153\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 1.2098\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.6351\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.4713\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.7628\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.5897\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.9253\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.6716\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.7565\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.6277\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.2972\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.7640\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.5810\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.3677\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.8528\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 1.1167\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.4895\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.7813\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.8943\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.8442\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.6339\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.6847\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.7445\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.6386\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.5245\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.6669\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.6152\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.6970\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.8582\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.9020\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.6731\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.5306\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.5260\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.2401\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.3597\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.6639\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.8585\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.3385\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 1.2067\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.8986\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.7975\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.4877\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.8858\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.7270\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 1.0119\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.3426\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.5031\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.4064\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.7892\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.8994\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.6670\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.7597\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.6641\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.7162\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 1.0202\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.7406\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.6125\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.8771\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.6708\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.6956\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.7792\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.5227\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.8687\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.8189\n",
            "Epoch: 7,\t Validation Loss: 0.6539,\t Accuracy: 0.7371\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.6905\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.6600\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.7538\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.5283\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.7414\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.9144\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.7540\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.8320\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.5742\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.3899\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.7581\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.5109\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.7279\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.6804\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.8138\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.9493\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.5721\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.5711\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.7335\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.6582\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.8313\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.8417\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.6660\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.8450\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.5226\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.9924\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.6251\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.4896\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 1.0541\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.7424\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.5508\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.3676\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.6138\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.5160\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.5332\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.6988\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.8043\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.6433\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.6642\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.7294\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.9380\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.9435\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.8641\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.7199\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.6403\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.7813\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.8473\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.6309\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.8057\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.7626\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.6069\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.4725\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.4740\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.8824\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.5951\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.7506\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.3373\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.5681\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.7040\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.6850\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.4526\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.8348\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.6939\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.5245\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.9607\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.8346\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.6714\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.6859\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.7280\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.4082\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.4911\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.9053\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.8814\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.7341\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 1.2603\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.7333\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.9265\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.7244\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.7060\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.8352\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.5353\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.6279\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.6359\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.5106\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.7881\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.5511\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.6467\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.6400\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.7069\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.8455\n",
            "Epoch: 9,\t Validation Loss: 0.6285,\t Accuracy: 0.7437\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.4568\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 1.0453\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.6115\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.5984\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.6380\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.5515\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.7411\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 1.2193\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.4896\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.7596\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.3166\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.5350\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.5101\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.6457\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.6485\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.7366\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.8327\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.6636\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.6433\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.9208\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.7822\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.6763\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.9286\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.7561\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.4817\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.5336\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.6751\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.4877\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.6856\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.6103\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.4145\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.8532\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.8521\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.7012\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.6278\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.7978\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.6674\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.4207\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.8784\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.8771\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.4562\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.6469\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.9045\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.6962\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.3703\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.7030\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.7954\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.8172\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.4867\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.7566\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.7290\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.5345\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.6858\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.7423\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.3652\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.5517\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.8283\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.7804\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.7198\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.5573\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.5257\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.8959\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.7784\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.6547\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.8839\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.7165\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.6307\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.8390\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.7412\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.6527\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.6662\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.9182\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.4924\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.5261\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.9169\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.4434\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.4654\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.4161\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.5287\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.5975\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.7605\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 1.0002\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.3277\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.8131\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.6708\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.5582\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.4825\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.5587\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.4178\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.6017\n",
            "Epoch: 11,\t Validation Loss: 0.6227,\t Accuracy: 0.7555\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.6158\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.5576\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.6910\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.4730\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.5874\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.5416\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.7083\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.6370\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 1.1564\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.6439\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.5427\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.5508\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.5495\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.7240\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.6830\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.5416\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.7488\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.5440\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.7785\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.5763\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.6221\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.5359\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.6583\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.7404\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.7411\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.8234\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.8255\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.4355\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.7824\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.6202\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.6439\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.5856\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.7501\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.4367\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.5985\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.4908\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.7668\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.6618\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.6019\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.6652\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.7149\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.6083\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.8627\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.6388\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.7898\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.4636\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.7012\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.5095\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.7901\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.5670\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.8933\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.6937\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.6625\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.5590\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.7659\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.5699\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.6478\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.6184\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.4827\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.9374\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.8185\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 1.0399\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.5390\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.7781\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.4991\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 1.1878\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.5976\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.6672\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.5638\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.4245\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.5899\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.6512\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.5860\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.9265\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.5361\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.5141\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.7436\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.4999\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.5755\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.6097\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 1.0668\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.8753\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.2579\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.3657\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.7230\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.3270\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.5428\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.4788\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.7457\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.7065\n",
            "Epoch: 13,\t Validation Loss: 0.6039,\t Accuracy: 0.7595\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.4692\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.3880\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.7155\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.9606\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.6978\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.6900\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.7371\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.7959\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.6161\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.6104\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.7747\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.7244\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.7624\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.5657\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.7283\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.9172\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.5766\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.5939\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.4824\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.6920\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.6282\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.6352\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.9741\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.9144\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.6774\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.7052\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.8347\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.5364\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.6565\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.9365\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.6324\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.5944\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.6277\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.8887\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.4401\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.6944\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.5764\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.6749\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.7490\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.6632\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.6588\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.7668\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.4558\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.7116\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.4521\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.3504\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.7288\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.5271\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.9235\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.8397\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.4253\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.5414\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.9317\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.4308\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.5247\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.7362\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.2038\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.5807\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.4735\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.5039\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.6251\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.6389\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.4688\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.5354\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.6831\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.6516\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.7398\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.7206\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.5186\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.8173\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.7518\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.5765\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.8150\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.5913\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.3831\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.6384\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.7529\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.5140\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.8535\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.8014\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.6430\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.7732\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.5559\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.8102\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.6324\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.4523\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.5240\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.9638\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.4941\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.5808\n",
            "Epoch: 15,\t Validation Loss: 0.5928,\t Accuracy: 0.7674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_model_drop.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_drop(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmLqv5qp5rOJ",
        "outputId": "c1912d6a-593b-40b9-aeaf-12df2916eac3"
      },
      "id": "bmLqv5qp5rOJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6567,\t Accuracy: 0.6626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combination"
      ],
      "metadata": {
        "id": "vx78ON7Cks2-"
      },
      "id": "vx78ON7Cks2-"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "slNvrSeKqty9"
      },
      "id": "slNvrSeKqty9"
    },
    {
      "cell_type": "code",
      "source": [
        "trained_comb_mod= train_loop_L2(YourModel_drop)"
      ],
      "metadata": {
        "id": "4TQkBtumqsFv"
      },
      "id": "4TQkBtumqsFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_comb_mod.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_drop(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "qpVzVkBqmIxx"
      },
      "id": "qpVzVkBqmIxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0a23885a",
      "metadata": {
        "id": "0a23885a"
      },
      "source": [
        "#### Chapter 3.3 - Batch Normalization\n",
        "\n",
        "Batch Normalization is a technique to improve the training of deep neural networks by normalizing the inputs to each layer. It was introduced to address the problem of internal covariate shift, which refers to the change in the distribution of layer inputs during training as the parameters of the previous layers change.\n",
        "\n",
        "**Intuition:**\n",
        "The idea behind Batch Normalization is to normalize the inputs to each layer so that they have a mean of 0 and a standard deviation of 1. This ensures that the inputs to each layer are on a similar scale, which helps the network learn faster and more effectively. By normalizing the inputs, Batch Normalization reduces the sensitivity of the network to the initialization of weights and allows for the use of higher learning rates.\n",
        "\n",
        "**How it works:**\n",
        "1. For each mini-batch, Batch Normalization computes the mean and variance of the inputs.\n",
        "2. The inputs are then normalized using these statistics:\n",
        "    $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "    where $\\mu$ is the mean, $\\sigma^2$ is the variance, and $\\epsilon$ is a small constant added for numerical stability.\n",
        "3. To allow the network to learn the optimal scale and shift for the normalized inputs, two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), are introduced:\n",
        "    $y = \\gamma \\hat{x} + \\beta$\n",
        "\n",
        "**Problems it solves:**\n",
        "1. **Internal Covariate Shift:** By normalizing the inputs to each layer, Batch Normalization reduces the changes in the distribution of layer inputs during training, making the optimization process more stable.\n",
        "2. **Faster Training:** Normalized inputs allow for the use of higher learning rates, leading to faster convergence.\n",
        "3. **Regularization Effect:** Batch Normalization introduces some noise due to the mini-batch statistics, which acts as a form of regularization and reduces the need for other regularization techniques like Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff313393",
      "metadata": {
        "id": "ff313393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "0abdcef7-1be6-4367-a16b-43011336fcd1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1135413189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here is how to add it into your model as a layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'num_features' is not defined"
          ]
        }
      ],
      "source": [
        "# Here is how to add it into your model as a layer: num_features muss vorher assigned werden\n",
        "\n",
        "bn = torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95af6dfc",
      "metadata": {
        "id": "95af6dfc"
      },
      "source": [
        "#### Chapter 3.4 - Modern Computer Vision Models\n",
        "\n",
        "AlexNet (original paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) is in many senses the grandfather of modern neural networks, being the first one to successfully combine multiple GPUs for training with a deep neural network. While it is no longer in use today, the lessons learned from AlexNet very much are, and multi-GPU setups and deep convolutional neural networks remain a staple of computer vision methods.\n",
        "\n",
        "Modern Computer Vision uses a number of different models, but perhaps none is as prolific as the original ResNet, in particular the ResNet-50. Even though it is far from the strongest model available today, its flexibility, modest size, and robust performance across tasks makes it a favorite, both in general computer vision and medical computer vision, where it is commonly used as the encoder in segmentation models (more on that later). The original paper (https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) has garnered almost 300'000 citations, and its descendants have dominated challenges and paper submissions in the field for a significant amount of time.\n",
        "\n",
        "**Task 2 (up to 6 points)**: Your task is to write one of these two modern models from scratch. The points are awarded for correctly implementing these models. You can choose your own difficulty here, and can earn fewer or more points, depending on which you feel more comfortable building. AlexNet requires only components that you have already seen last week - convolutions, pooling, and linear layers, while ResNet requires you to build skip connections and bottleneck blocks from scratch.\n",
        "\n",
        "Option 1 - AlexNet **(4 points)**:\n",
        "- Building the model **(2 points)**\n",
        "- You do not have to implement the parts where multiple GPUs are required\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "Option 2 - ResNet-50 **(7 points)**\n",
        "- Correctly implementing Skip Connections **(1 point)**\n",
        "- Correctly implementing Residual/Bottleneck Blocks **(3 points)**\n",
        "- Correctly building the ResNet from these Blocks **(1 point)**\n",
        "- For BatchNorm you are allowed to simply use the existing implementation\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "You must verify that your model actually trains and is capable of solving the classification task on LiTS 2017. You should be able to explain every piece of code to the tutors that grade your solution, so if you use any help in building the model (e.g. Chat-GPT, Cursor, etc.), be prepared to explain what code blocks do what, and why you implemented them in the specific way you did, and not any other. The points are awarded for programming *and* understanding!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c7122f",
      "metadata": {
        "id": "62c7122f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as tt\n",
        "train_transforms = tt.Compose([\n",
        "    tt.Resize((224, 224)), # Add resize operation\n",
        "    tt.RandomHorizontalFlip(),\n",
        "    tt.RandomRotation(10),\n",
        "    tt.RandomAffine(degrees=15),\n",
        "    tt.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
        "])"
      ],
      "metadata": {
        "id": "1NSI8ESE-9NG"
      },
      "id": "1NSI8ESE-9NG",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=3, in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "\n",
        "            nn.Conv2d(64, 192, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "\n",
        "            nn.Conv2d(192, 384, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(384, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "\n",
        "        # ---- FIX: compute output size automatically ----\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, 256, 256)\n",
        "            out = self.features(dummy)\n",
        "            self.flatten_dim = out.shape[1] * out.shape[2] * out.shape[3]\n",
        "\n",
        "        # -----------------------------------------------\n",
        "        # No hardcoded 9216 — we USE flatten_dim instead.\n",
        "        # -----------------------------------------------\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(self.flatten_dim, 4096),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)   # uses flatten_dim automatically\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "oY97D2zPBEiN"
      },
      "id": "oY97D2zPBEiN",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_alex(model_class):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model_class(in_channels=1).to(device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # apply transforms on each sample in batch\n",
        "            data = torch.stack([train_transforms(img) for img in data]).to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # -------------------------\n",
        "        # Validation every 2 epochs\n",
        "        # -------------------------\n",
        "        if epoch % 2 == 0:\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            val_loss_accum = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for data, targets in val_dataloader:\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "\n",
        "                    val_loss = loss_criterion(predictions, targets)\n",
        "                    val_loss_accum += val_loss.item() * data.size(0)\n",
        "\n",
        "                    predicted = predictions.argmax(dim=1)\n",
        "                    correct += (predicted == targets).sum().item()\n",
        "                    total += targets.size(0)\n",
        "\n",
        "            print(f\"Validation → Epoch {epoch+1}, \"\n",
        "                  f\"Loss: {val_loss_accum/total:.4f}, \"\n",
        "                  f\"Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_alex = train_loop_alex(AlexNet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JLxcr-mt_MNN",
        "outputId": "07f45289-0bf4-4982-dff2-a5b73b020fbf"
      },
      "id": "JLxcr-mt_MNN",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [0], Loss: 1.1051\n",
            "Epoch [1/5], Step [50], Loss: 0.8513\n",
            "Epoch [1/5], Step [100], Loss: 0.7731\n",
            "Epoch [1/5], Step [150], Loss: 0.5907\n",
            "Epoch [1/5], Step [200], Loss: 0.2994\n",
            "Epoch [1/5], Step [250], Loss: 0.6432\n",
            "Epoch [1/5], Step [300], Loss: 0.6057\n",
            "Epoch [1/5], Step [350], Loss: 0.4740\n",
            "Epoch [1/5], Step [400], Loss: 0.3571\n",
            "Epoch [1/5], Step [450], Loss: 0.2271\n",
            "Epoch [1/5], Step [500], Loss: 0.3697\n",
            "Epoch [1/5], Step [550], Loss: 0.9232\n",
            "Epoch [1/5], Step [600], Loss: 0.6057\n",
            "Epoch [1/5], Step [650], Loss: 0.5990\n",
            "Epoch [1/5], Step [700], Loss: 0.4539\n",
            "Epoch [1/5], Step [750], Loss: 0.3418\n",
            "Epoch [1/5], Step [800], Loss: 0.1556\n",
            "Epoch [1/5], Step [850], Loss: 0.3036\n",
            "Epoch [1/5], Step [900], Loss: 0.4551\n",
            "Epoch [1/5], Step [950], Loss: 0.1300\n",
            "Epoch [1/5], Step [1000], Loss: 0.2444\n",
            "Epoch [1/5], Step [1050], Loss: 0.1945\n",
            "Epoch [1/5], Step [1100], Loss: 0.6152\n",
            "Epoch [1/5], Step [1150], Loss: 0.2923\n",
            "Epoch [1/5], Step [1200], Loss: 0.3608\n",
            "Epoch [1/5], Step [1250], Loss: 0.5189\n",
            "Epoch [1/5], Step [1300], Loss: 0.6070\n",
            "Epoch [1/5], Step [1350], Loss: 0.1859\n",
            "Epoch [1/5], Step [1400], Loss: 0.1649\n",
            "Epoch [1/5], Step [1450], Loss: 0.2505\n",
            "Epoch [1/5], Step [1500], Loss: 0.5612\n",
            "Epoch [1/5], Step [1550], Loss: 0.2189\n",
            "Epoch [1/5], Step [1600], Loss: 0.1238\n",
            "Epoch [1/5], Step [1650], Loss: 0.2977\n",
            "Epoch [1/5], Step [1700], Loss: 0.2629\n",
            "Epoch [1/5], Step [1750], Loss: 0.1850\n",
            "Epoch [1/5], Step [1800], Loss: 0.2895\n",
            "Epoch [1/5], Step [1850], Loss: 0.3726\n",
            "Epoch [1/5], Step [1900], Loss: 0.1776\n",
            "Epoch [1/5], Step [1950], Loss: 0.6724\n",
            "Epoch [1/5], Step [2000], Loss: 0.4314\n",
            "Epoch [1/5], Step [2050], Loss: 0.2488\n",
            "Epoch [1/5], Step [2100], Loss: 0.1728\n",
            "Epoch [1/5], Step [2150], Loss: 0.1970\n",
            "Epoch [1/5], Step [2200], Loss: 0.1828\n",
            "Validation → Epoch 1, Loss: 0.3812, Accuracy: 0.8264\n",
            "Epoch [2/5], Step [0], Loss: 0.2643\n",
            "Epoch [2/5], Step [50], Loss: 0.3751\n",
            "Epoch [2/5], Step [100], Loss: 0.1509\n",
            "Epoch [2/5], Step [150], Loss: 0.7078\n",
            "Epoch [2/5], Step [200], Loss: 0.2938\n",
            "Epoch [2/5], Step [250], Loss: 0.2559\n",
            "Epoch [2/5], Step [300], Loss: 0.2667\n",
            "Epoch [2/5], Step [350], Loss: 0.3167\n",
            "Epoch [2/5], Step [400], Loss: 0.3954\n",
            "Epoch [2/5], Step [450], Loss: 0.4201\n",
            "Epoch [2/5], Step [500], Loss: 0.1222\n",
            "Epoch [2/5], Step [550], Loss: 0.4733\n",
            "Epoch [2/5], Step [600], Loss: 0.5171\n",
            "Epoch [2/5], Step [650], Loss: 0.2387\n",
            "Epoch [2/5], Step [700], Loss: 0.2358\n",
            "Epoch [2/5], Step [750], Loss: 0.2652\n",
            "Epoch [2/5], Step [800], Loss: 0.5992\n",
            "Epoch [2/5], Step [850], Loss: 0.1400\n",
            "Epoch [2/5], Step [900], Loss: 0.2795\n",
            "Epoch [2/5], Step [950], Loss: 0.1990\n",
            "Epoch [2/5], Step [1000], Loss: 0.3153\n",
            "Epoch [2/5], Step [1050], Loss: 0.1500\n",
            "Epoch [2/5], Step [1100], Loss: 0.2023\n",
            "Epoch [2/5], Step [1150], Loss: 0.1740\n",
            "Epoch [2/5], Step [1200], Loss: 0.4581\n",
            "Epoch [2/5], Step [1250], Loss: 0.2515\n",
            "Epoch [2/5], Step [1300], Loss: 0.1588\n",
            "Epoch [2/5], Step [1350], Loss: 0.3328\n",
            "Epoch [2/5], Step [1400], Loss: 0.2119\n",
            "Epoch [2/5], Step [1450], Loss: 0.2600\n",
            "Epoch [2/5], Step [1500], Loss: 0.3693\n",
            "Epoch [2/5], Step [1550], Loss: 0.0962\n",
            "Epoch [2/5], Step [1600], Loss: 0.2053\n",
            "Epoch [2/5], Step [1650], Loss: 0.1819\n",
            "Epoch [2/5], Step [1700], Loss: 0.2103\n",
            "Epoch [2/5], Step [1750], Loss: 0.3863\n",
            "Epoch [2/5], Step [1800], Loss: 0.2441\n",
            "Epoch [2/5], Step [1850], Loss: 0.2917\n",
            "Epoch [2/5], Step [1900], Loss: 0.3294\n",
            "Epoch [2/5], Step [1950], Loss: 0.2766\n",
            "Epoch [2/5], Step [2000], Loss: 0.3734\n",
            "Epoch [2/5], Step [2050], Loss: 0.4665\n",
            "Epoch [2/5], Step [2100], Loss: 0.1203\n",
            "Epoch [2/5], Step [2150], Loss: 0.7538\n",
            "Epoch [2/5], Step [2200], Loss: 0.2923\n",
            "Epoch [3/5], Step [0], Loss: 0.3559\n",
            "Epoch [3/5], Step [50], Loss: 0.2031\n",
            "Epoch [3/5], Step [100], Loss: 0.1272\n",
            "Epoch [3/5], Step [150], Loss: 0.2225\n",
            "Epoch [3/5], Step [200], Loss: 0.1597\n",
            "Epoch [3/5], Step [250], Loss: 0.2252\n",
            "Epoch [3/5], Step [300], Loss: 0.2206\n",
            "Epoch [3/5], Step [350], Loss: 0.2469\n",
            "Epoch [3/5], Step [400], Loss: 0.4104\n",
            "Epoch [3/5], Step [450], Loss: 0.6046\n",
            "Epoch [3/5], Step [500], Loss: 0.2133\n",
            "Epoch [3/5], Step [550], Loss: 0.1941\n",
            "Epoch [3/5], Step [600], Loss: 0.0395\n",
            "Epoch [3/5], Step [650], Loss: 0.4693\n",
            "Epoch [3/5], Step [700], Loss: 0.2264\n",
            "Epoch [3/5], Step [750], Loss: 0.1827\n",
            "Epoch [3/5], Step [800], Loss: 0.1844\n",
            "Epoch [3/5], Step [850], Loss: 0.1038\n",
            "Epoch [3/5], Step [900], Loss: 0.2981\n",
            "Epoch [3/5], Step [950], Loss: 0.0972\n",
            "Epoch [3/5], Step [1000], Loss: 0.2862\n",
            "Epoch [3/5], Step [1050], Loss: 0.1466\n",
            "Epoch [3/5], Step [1100], Loss: 0.1580\n",
            "Epoch [3/5], Step [1150], Loss: 0.3429\n",
            "Epoch [3/5], Step [1200], Loss: 0.3133\n",
            "Epoch [3/5], Step [1250], Loss: 0.1318\n",
            "Epoch [3/5], Step [1300], Loss: 0.1574\n",
            "Epoch [3/5], Step [1350], Loss: 0.1916\n",
            "Epoch [3/5], Step [1400], Loss: 0.2208\n",
            "Epoch [3/5], Step [1450], Loss: 0.1799\n",
            "Epoch [3/5], Step [1500], Loss: 0.1760\n",
            "Epoch [3/5], Step [1550], Loss: 0.0627\n",
            "Epoch [3/5], Step [1600], Loss: 0.2447\n",
            "Epoch [3/5], Step [1650], Loss: 0.1991\n",
            "Epoch [3/5], Step [1700], Loss: 0.2437\n",
            "Epoch [3/5], Step [1750], Loss: 0.3061\n",
            "Epoch [3/5], Step [1800], Loss: 0.3139\n",
            "Epoch [3/5], Step [1850], Loss: 0.6408\n",
            "Epoch [3/5], Step [1900], Loss: 0.2559\n",
            "Epoch [3/5], Step [1950], Loss: 0.2034\n",
            "Epoch [3/5], Step [2000], Loss: 0.2585\n",
            "Epoch [3/5], Step [2050], Loss: 0.1315\n",
            "Epoch [3/5], Step [2100], Loss: 0.0506\n",
            "Epoch [3/5], Step [2150], Loss: 0.2981\n",
            "Epoch [3/5], Step [2200], Loss: 0.1570\n",
            "Validation → Epoch 3, Loss: 0.5183, Accuracy: 0.8148\n",
            "Epoch [4/5], Step [0], Loss: 0.2241\n",
            "Epoch [4/5], Step [50], Loss: 0.2504\n",
            "Epoch [4/5], Step [100], Loss: 0.1389\n",
            "Epoch [4/5], Step [150], Loss: 0.1153\n",
            "Epoch [4/5], Step [200], Loss: 0.0194\n",
            "Epoch [4/5], Step [250], Loss: 0.3899\n",
            "Epoch [4/5], Step [300], Loss: 0.1153\n",
            "Epoch [4/5], Step [350], Loss: 0.3358\n",
            "Epoch [4/5], Step [400], Loss: 0.0542\n",
            "Epoch [4/5], Step [450], Loss: 0.1720\n",
            "Epoch [4/5], Step [500], Loss: 0.0618\n",
            "Epoch [4/5], Step [550], Loss: 0.0825\n",
            "Epoch [4/5], Step [600], Loss: 0.5079\n",
            "Epoch [4/5], Step [650], Loss: 0.0686\n",
            "Epoch [4/5], Step [700], Loss: 0.1160\n",
            "Epoch [4/5], Step [750], Loss: 0.0637\n",
            "Epoch [4/5], Step [800], Loss: 0.1129\n",
            "Epoch [4/5], Step [850], Loss: 0.1408\n",
            "Epoch [4/5], Step [900], Loss: 0.1813\n",
            "Epoch [4/5], Step [950], Loss: 0.0563\n",
            "Epoch [4/5], Step [1000], Loss: 0.2326\n",
            "Epoch [4/5], Step [1050], Loss: 0.1649\n",
            "Epoch [4/5], Step [1100], Loss: 0.0986\n",
            "Epoch [4/5], Step [1150], Loss: 0.4811\n",
            "Epoch [4/5], Step [1200], Loss: 0.0581\n",
            "Epoch [4/5], Step [1250], Loss: 0.3914\n",
            "Epoch [4/5], Step [1300], Loss: 0.0756\n",
            "Epoch [4/5], Step [1350], Loss: 0.1368\n",
            "Epoch [4/5], Step [1400], Loss: 0.1183\n",
            "Epoch [4/5], Step [1450], Loss: 0.5102\n",
            "Epoch [4/5], Step [1500], Loss: 0.1290\n",
            "Epoch [4/5], Step [1550], Loss: 0.0633\n",
            "Epoch [4/5], Step [1600], Loss: 0.2172\n",
            "Epoch [4/5], Step [1650], Loss: 0.1410\n",
            "Epoch [4/5], Step [1700], Loss: 0.2403\n",
            "Epoch [4/5], Step [1750], Loss: 0.0369\n",
            "Epoch [4/5], Step [1800], Loss: 0.0758\n",
            "Epoch [4/5], Step [1850], Loss: 0.2087\n",
            "Epoch [4/5], Step [1900], Loss: 0.1708\n",
            "Epoch [4/5], Step [1950], Loss: 0.0441\n",
            "Epoch [4/5], Step [2000], Loss: 0.1682\n",
            "Epoch [4/5], Step [2050], Loss: 0.1268\n",
            "Epoch [4/5], Step [2100], Loss: 0.0722\n",
            "Epoch [4/5], Step [2150], Loss: 0.1024\n",
            "Epoch [4/5], Step [2200], Loss: 0.0634\n",
            "Epoch [5/5], Step [0], Loss: 0.0179\n",
            "Epoch [5/5], Step [50], Loss: 0.0655\n",
            "Epoch [5/5], Step [100], Loss: 0.0818\n",
            "Epoch [5/5], Step [150], Loss: 0.3374\n",
            "Epoch [5/5], Step [200], Loss: 0.0196\n",
            "Epoch [5/5], Step [250], Loss: 0.1465\n",
            "Epoch [5/5], Step [300], Loss: 0.2288\n",
            "Epoch [5/5], Step [350], Loss: 0.3819\n",
            "Epoch [5/5], Step [400], Loss: 0.1899\n",
            "Epoch [5/5], Step [450], Loss: 0.2154\n",
            "Epoch [5/5], Step [500], Loss: 0.1028\n",
            "Epoch [5/5], Step [550], Loss: 0.3958\n",
            "Epoch [5/5], Step [600], Loss: 0.1332\n",
            "Epoch [5/5], Step [650], Loss: 0.3083\n",
            "Epoch [5/5], Step [700], Loss: 0.1973\n",
            "Epoch [5/5], Step [750], Loss: 0.2691\n",
            "Epoch [5/5], Step [800], Loss: 0.6049\n",
            "Epoch [5/5], Step [850], Loss: 0.1013\n",
            "Epoch [5/5], Step [900], Loss: 0.2011\n",
            "Epoch [5/5], Step [950], Loss: 0.1633\n",
            "Epoch [5/5], Step [1000], Loss: 0.1732\n",
            "Epoch [5/5], Step [1050], Loss: 0.3715\n",
            "Epoch [5/5], Step [1100], Loss: 0.2471\n",
            "Epoch [5/5], Step [1150], Loss: 0.1235\n",
            "Epoch [5/5], Step [1200], Loss: 0.1774\n",
            "Epoch [5/5], Step [1250], Loss: 0.2609\n",
            "Epoch [5/5], Step [1300], Loss: 0.1149\n",
            "Epoch [5/5], Step [1350], Loss: 0.0646\n",
            "Epoch [5/5], Step [1400], Loss: 0.1631\n",
            "Epoch [5/5], Step [1450], Loss: 0.1291\n",
            "Epoch [5/5], Step [1500], Loss: 0.0053\n",
            "Epoch [5/5], Step [1550], Loss: 0.0298\n",
            "Epoch [5/5], Step [1600], Loss: 0.2993\n",
            "Epoch [5/5], Step [1650], Loss: 0.2562\n",
            "Epoch [5/5], Step [1700], Loss: 0.1039\n",
            "Epoch [5/5], Step [1750], Loss: 0.0030\n",
            "Epoch [5/5], Step [1800], Loss: 0.2005\n",
            "Epoch [5/5], Step [1850], Loss: 0.2546\n",
            "Epoch [5/5], Step [1900], Loss: 0.0586\n",
            "Epoch [5/5], Step [1950], Loss: 0.4322\n",
            "Epoch [5/5], Step [2000], Loss: 0.1158\n",
            "Epoch [5/5], Step [2050], Loss: 0.1845\n",
            "Epoch [5/5], Step [2100], Loss: 0.0714\n",
            "Epoch [5/5], Step [2150], Loss: 0.0288\n",
            "Epoch [5/5], Step [2200], Loss: 0.1659\n",
            "Validation → Epoch 5, Loss: 0.4132, Accuracy: 0.8773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "trained_alex.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "all_probs = []  # <-- store softmax probabilities for all batches\n",
        "\n",
        "hits = 0\n",
        "losses = []\n",
        "batch_sizes = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        predictions = trained_alex(data)            # logits\n",
        "        probs = torch.softmax(predictions, dim=1)  # softmax probabilities\n",
        "        all_probs.append(probs.cpu().numpy())      # save them\n",
        "\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size(0))\n",
        "\n",
        "        class_predictions = torch.argmax(predictions, dim=1).flatten()\n",
        "        hits += (class_predictions == targets).sum().item()\n",
        "\n",
        "        all_preds.extend(class_predictions.cpu().tolist())\n",
        "        all_targets.extend(targets.cpu().tolist())\n",
        "\n",
        "# Compute overall metrics\n",
        "accuracy = hits / len(test_dataloader.dataset)\n",
        "avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "\n",
        "print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "# --- Precision-Recall curves (correct multi-class version) ---\n",
        "\n",
        "num_classes = max(all_targets) + 1\n",
        "\n",
        "# One-hot encode labels\n",
        "y_true = torch.nn.functional.one_hot(\n",
        "    torch.tensor(all_targets), num_classes=num_classes\n",
        ").numpy()\n",
        "\n",
        "# Concatenate all softmax probabilities\n",
        "y_scores = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "assert y_scores.shape[0] == y_true.shape[0], \"Mismatch fixed!\"\n",
        "\n",
        "# Plot PR curve for each class\n",
        "for i in range(num_classes):\n",
        "    precision, recall, _ = precision_recall_curve(y_true[:, i], y_scores[:, i])\n",
        "    disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "    disp.plot()\n",
        "    plt.title(f'Precision-Recall curve for class {i}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "4KwIP9DaHoFM",
        "outputId": "01e2b7a2-bae0-4c4a-aeee-fc86ff532211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1785
        }
      },
      "id": "4KwIP9DaHoFM",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5590,\t Accuracy: 0.8282\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOVtxQQh_LFf"
      },
      "id": "rOVtxQQh_LFf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
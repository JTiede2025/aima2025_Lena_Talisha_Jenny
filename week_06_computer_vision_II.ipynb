{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5a61491",
      "metadata": {
        "id": "a5a61491"
      },
      "source": [
        "### Chapter 3 - Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77f8126",
      "metadata": {
        "id": "c77f8126"
      },
      "source": [
        "**This week's exercise has 3 tasks, for a total of 10.5 points. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building core components of modern PyTorch models\n",
        "- Assembling modern PyTorch models from components\n",
        "- Training a modern model on a real-world task and achieving passable results\n",
        "\n",
        "**Note**: Since you have already proven that you are capable of creating the core components of a typical training loop yourself, we will provide some utility code for this section. This is done so that you can focus on the important parts of this lesson, and to help us debug your code in case you need help."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4f7614",
      "metadata": {
        "id": "8e4f7614"
      },
      "source": [
        "#### Chapter 3.1 - Data Augmentation\n",
        "\n",
        "**What is data augmentation?**\n",
        "Have you ever lost your glasses and then squinted, or tried to look through a rainy window? Or looked at a false color image, maybe a forest where the trees are blue and the sky green? You can usually make an educated guess what you are looking at, even though the image you see is different than usual. This is, essentialy, what data augmentation is. It's the same data, still recognizable, but slightly altered in some way.\n",
        "\n",
        "**What is that useful for?**\n",
        "Let me begin with an anecdote that you've probably heard in the lectures. Say you have pictures of cats and dogs, and want your model to tell the two apart. How many people you know go to the park with their dogs? I imagine many. Hence, many images of dogs are dogs lying on the grass. The same is generally untrue for cats, at least I have never heard of anyone walking their cat to the park. At any rate, here is what happens when I train a neural network on these images: The model takes a shortcut. It sees a lot of green and the correct answer for these pictures is always \"Dog\". It learns \"Green = Dog\". This is what we call overfitting. We have overfitted to the existence of green in the background as a quintessential part of what makes a dog. Sometimes, we get away with this, if our data always has this correlation.\n",
        "\n",
        "Now I get some new data. A bunch of people have taken pictures of their cats, sunbathing on the terrace. The garden is in the background. Lots of green. The model, in its infinite wisdom, will at first guess that these images are of dogs. Clearly, our model's ability to tell apart cats and dogs has not generalized to this new dataset.\n",
        "\n",
        "So how can we prevent the model from taking shortcuts and encourage learning information that generalizes? We force these generalizations in training. If I gave you an image of a dog, but the grass was brown, and the dog green, you could still identify it as a dog, instead of a cat, right? And so should the model, if we can manage it. So let's also make it train using pictures of cats and dogs where the colours are different or removed. Suddenly, the shortcut solution is no longer useful, and the model must rely on shape, texture, or contextual information like a leash. The practice of color change described is a practical and useful data augmentation that is used in state-of-the-art image recognition.\n",
        "\n",
        "In addition to color changes, there is a myriad of other techniques, such as cropping, image rotation or flipping, edge filters, solarization, random noise, and many, many more. Basically, anything that you believe may eventually show up in testing data and that you want the model to generalize to, can be made into a corresponding data augmentation.\n",
        "\n",
        "**How do we use data augmentations in practice?**\n",
        "There are two ways of adding data augmentation during training. Either, you can implement it inside of your dataset, so that it only returns augmented image tensors, or right before feeding your image tensors into your model. Both options are acceptable and come with advantages and disadvantages, although the more common way is to separate dataset and augmentations. We also showcase the native PyTorch way of augmenting data below.\n",
        "\n",
        "If you are particularly eager, or want to try your hand at making image augmentation functions yourself, it can be fun and is definitely good practice. However, PyTorch comes with a large selection of image augmentations right out of the box, and in the following chapter, we will look at how to make use of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e352a55a",
      "metadata": {
        "id": "e352a55a",
        "outputId": "a4da0769-dc5b-4a41-b602-65b4b8876cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.transforms.functional as ttf\n",
        "\n",
        "# Torchvision contains two ways of utilizing transforms:\n",
        "# Functional and Composed.\n",
        "\n",
        "# Functional does what it advertises - it is a function which\n",
        "# you can use on your tensors. Here is an example which performs\n",
        "# a center crop:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transformed_images = ttf.center_crop(img = dummy_images, output_size = (128, 128))\n",
        "print(transformed_images.size())\n",
        "\n",
        "# Functional transforms have the inherent advantage of giving the\n",
        "# user very fine-grained control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "55a993c1",
      "metadata": {
        "id": "55a993c1",
        "outputId": "937701f5-e808-4c8b-f98b-95d187eff3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# The alternative is the so-called Composed form, which uses\n",
        "# classes to achieve the same result. We make a Composed Transform\n",
        "# like so:\n",
        "\n",
        "dummy_images = torch.rand((16, 1, 256, 256))\n",
        "transforms = tt.Compose([\n",
        "    tt.RandomCrop(size = (128, 128)),\n",
        "    tt.RandomHorizontalFlip()\n",
        "])\n",
        "transformed_images = transforms(dummy_images)\n",
        "print(transformed_images.size())\n",
        "\n",
        "# As you can see, Compose offers us the option of sequentially\n",
        "# executing multiple transformations in a single line of code.\n",
        "# We also get the option of using randomized augmentations,\n",
        "# where the randomization is already done for us.\n",
        "\n",
        "# In practice, either style of writing transformations is fine.\n",
        "# In fact, they are equivalent, as Compose calls the functional\n",
        "# versions of the transforms under the hood. In the case of\n",
        "# randomized augmentations, the class handles all the randomizing\n",
        "# and then calls the functional transform with the random inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b4a34c",
      "metadata": {
        "id": "b8b4a34c"
      },
      "source": [
        "**Task 1 (1+1 points)**: A complete list of Torchvision's available transforms can be found here: https://docs.pytorch.org/vision/0.9/transforms.html. Consider the task we are working on right now - working with CT images from the LiTS 2017 dataset. Which data augmentations strike you as a good idea to add to our training **(1 point)**? Which do you think are a bad idea or cannot work at all **(1 point)**? Are there any which are missing in Torchvision? If you don't know what they do, try them out and judge for yourselves. Can you think of other image types with other physics behind them? Are the rules for them going to be different?\n",
        "\n",
        "There are no definitely correct or incorrect answers here. The goal for this task is for you to be able to argue your case convincingly (to us) and think closely about your dataset. You can test your assumptions when completing the other tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOT HELPFUL:\n",
        "classtorchvision.transforms.CenterCrop(size)  -Could potentially crop of the tumor and then our truths are suddenly wrong ---> not good\n",
        "\n",
        "torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)[source]\n",
        "Randomly change the brightness, contrast, saturation and hue of an image. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions. If img is PIL Image, mode “1”, “L”, “I”, “F” and modes with transparency (alpha channel) are not supported.\n",
        "--> if really random and not consistent in the image it could make areas appear like a tumor or make a tumor disappear (by making the surrounding area too bright)\n",
        "\n",
        "torchvision.transforms.FiveCrop(size)[source]\n",
        "Crop the given image into four corners and the central crop. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n",
        "-->a crop can cut the tumor partially or completely out of a given crop, which can harm detection performance, not helpfull\n",
        "\n",
        "torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')\n",
        "\n",
        "\n",
        "Unnecessary\n",
        "transform.resize/scale"
      ],
      "metadata": {
        "id": "bPZRkVpRjy_e"
      },
      "id": "bPZRkVpRjy_e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "HELPFUL:\n",
        "torchvision.transforms.Grayscale(num_output_channels=1)\n",
        "Convert image to grayscale. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions\n",
        "-> good, less calculating capacity, maybe better contrast\n",
        "\n",
        "\n",
        "classtorchvision.transforms.Pad(padding, fill=0, padding_mode='constant')[source]\n",
        "Pad the given image on all sides with the given “pad” value. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means at most 2 leading dimensions for mode reflect and symmetric, at most 3 leading dimensions for mode edge, and an arbitrary number of leading dimensions for mode constant\n",
        "-> helpful to provide context near edge, good for tumors on the edge of the scan\n",
        "\n",
        "classtorchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=<InterpolationMode.NEAREST: 'nearest'>, fill=0, fillcolor=None, resample=None)[source]\n",
        "Random affine transformation of the image keeping center invariant. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions. (ROTATION; Sliding; etc)\n",
        "-> helps to not learn just the spot of the tumor\n",
        "\n",
        "classtorchvision.transforms.RandomApply(transforms, p=0.5)\n",
        "You can define what transformations should be done, but they are only done randomly with certain percentages\n",
        "-> very good, because it doesn`T transform all data\n",
        "\n",
        "All random functions of those things are also good e.g\n",
        "random.grayscale\n",
        "random.horizontal.flip /vertical.flip\n",
        "random.perspective\n",
        "random.rotation\n",
        "\n",
        "\n",
        "torchvision.transforms.GaussianBlur(kernel_size, sigma=(0.1, 2.0))\n",
        "-> including bad quality\n"
      ],
      "metadata": {
        "id": "bj5SyEoblnXM"
      },
      "id": "bj5SyEoblnXM"
    },
    {
      "cell_type": "markdown",
      "id": "325ff768",
      "metadata": {
        "id": "325ff768"
      },
      "source": [
        "#### Chapter 3.2 - Regularization Techniques\n",
        "\n",
        "While there are multiple definitions or guidelines for what regularization is supposed to do (see lectures), in terms of practical concerns, all regularization techniques have the same aim, expressed through different means: Improving some aspect of the performance of your deep learning models. We differentiate them, broadly, from data augmentations, because regularization techniques generally concern themselves with the learning process, e.g. loss function modifications, learning rate optimizations, temporary model modifications, etc., and *not* the underlying data in our model training.\n",
        "\n",
        "There are a number of different strategies, far too many to list all of them here, but a few particularly successful ones have made it into common use - so much so that they are more prevalent than regularization-less, \"vanilla\" deep learning. These fall into different groups, briefly discussed below.\n",
        "\n",
        "#### Additional Loss Components\n",
        "\n",
        "The loss function for any given modern optimization task is typically continuous and not always smooth everywhere. As a consequence, there are many different parameter configurations in a model that result in the same train-time loss. Not all of these express the same behavior during training or testing, however. When we modify our loss to penalize certain training behaviors, we allow the training process to select for models and parameters that give models that generalize better, converge to a solution faster, etc.,  despite often expressing the same training loss. Let's look at some examples that should be familiar from the lecture:\n",
        "\n",
        "**L1 Loss** - Often also called LASSO, L1 Loss is a penalty term added to the normal loss during training, which is defined as:\n",
        "$L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|$. Growing linearly with parameter magnitude, we penalize the model. This, in turn, forces the model to use fewer and smaller weights - relying on more weights than it needs, and thus probably overfitting, is disincentivized. Similarly, we just forced our model to stick to weights near zero, which we already know is generally a preferable area for model activations to stick to.\n",
        "\n",
        "**L2 Loss** - L2 Loss is the more popular cousin of the L1 Loss, which does approximately the same thing, except the penalty is equal to the sum over all squared parameter magnitudes: $L_{LASSO} = \\sum_{p=1}^{P} |\\Theta_{P}|^2$ The reasoning behind it is similar, but it has seen far more practical adoption.\n",
        "\n",
        "**Weight Decay** - An operation that effectively performs the same duty, weight decay reduces the magnitude of weights after each backward pass, for example by subtracting a small constant or multiplying with a factor. In essence, this eliminates parameters which are rarely \"used\" and were thus likely involved in overfitting on a small amount of data anyway. Parameters that are regularly updated (and therefore probably useful), will always remain near their optimal value despite weight decay. Interestingly, Weight Decay is mathematically equivalent to L2 Loss in terms of net parameter updates.\n",
        "\n",
        "#### Training Strategies\n",
        "**Early Stopping**: For early stopping, we monitor the performance of the model on a validation set and stop training when the performance stops improving. This ensures that the model does not continue to train on the training data and potentially overfit, while also saving computational resources.\n",
        "\n",
        "**Dropout**: Dropout is a regularization technique where, during training, a random subset of neurons in a layer is \"dropped out\" (set to zero) for each forward pass. This prevents the network from relying too heavily on specific neurons and encourages it to learn more robust and generalized features. During inference, all neurons are used, but their outputs are scaled to account for the dropout during training.\n",
        "\n",
        "**Learning Rate Scheduling (LR Scheduling)**: Learning rate scheduling involves dynamically adjusting the learning rate during training. A high learning rate at the start helps the model converge quickly, while a lower learning rate later allows for fine-tuning. Common strategies include step decay, exponential decay, and cosine annealing. Proper learning rate scheduling can lead to faster convergence and better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cd62d1",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "f6cd62d1"
      },
      "source": [
        "**Task 1.5 (3 x 0.5 points)**: Let us implement and compare the effects of different regularization techniques on a simple neural network. To do so, follow these steps:\n",
        "\n",
        "1. Create a small neural network (e.g., 2-3 layers) and train it on the LiTS dataset without any regularization. Record the training and validation accuracy/loss. (P.S.: You can use the model from last week's exercise as a starting point.)\n",
        "2. Add L2 regularization to the model and observe how it affects the training and validation performance (Check the Adam optimizer documentation to find out how to add this regularization). Compare the results with the unregularized model.\n",
        "3. Add dropout to the model and repeat the training process (check the PyTorch documentation to find out how to add this regularization - you do not need to implement it yourself). Compare the results with the previous models.\n",
        "\n",
        "For each step, make sure to copy the relevant code snippets into a new cell, instead of modifying the existing code. This way, we can keep track of the different versions of the model and their performances.\n",
        "\n",
        "You might want to look up the documentation for implementing these techniques in PyTorch.\n",
        "\n",
        "For each regularization technique, explain how it impacts the model's performance and generalization. Which combination of techniques works best for this dataset? Why do you think that is the case?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as ttf\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL"
      ],
      "metadata": {
        "id": "No91MSYuileo"
      },
      "id": "No91MSYuileo",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download our data again:\n",
        "#!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "#!rm -rf ./sample_data/\n",
        "!rm -rf ./Clean_LiTS\n",
        "!unzip -qq ./drive/MyDrive/Clean_LiTS.zip -d .\n",
        "#!rm ./Clean_LiTS.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysWOilT4ioyz",
        "outputId": "c45a0821-3916-40bb-aef6-737a8a5d71fd"
      },
      "id": "ysWOilT4ioyz",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f91bab93",
      "metadata": {
        "id": "f91bab93"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class LiTS_Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    For our sample solution, we go for the easier variant.\n",
        "\n",
        "    In this specific dataset, we don't load the images until we need them - for a\n",
        "    short training, or limited resources, this is good behavior. If you have the\n",
        "    necessary RAM to pre-load all of your data, you don't have to load the data\n",
        "    multiple times, and save compute costs in the long run. The downside is that\n",
        "    when you are trying to debug, you wait for ages every time, and if you simply\n",
        "    do not have the compute resources, you can't even do it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv: str, mode: str):\n",
        "\n",
        "        self.csv = csv\n",
        "        self.data = pd.read_csv(self.csv)\n",
        "        self.mode = mode\n",
        "        assert mode in [\"train\", \"val\", \"test\"] # has to be train, val, or test data - if not, assert throws an error\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file = self.data.loc[idx, \"filename\"]\n",
        "        with PIL.Image.open(f\"./Clean_LiTS/{self.mode}/{file}\") as f:\n",
        "            f = f.convert(\"L\")\n",
        "            image = ttf.pil_to_tensor(f)\n",
        "\n",
        "        image = image.to(dtype = torch.float32)\n",
        "        image -= torch.min(image)\n",
        "        image /= torch.max(image)\n",
        "\n",
        "        liver_visible = self.data.loc[idx, \"liver_visible\"]\n",
        "        lesion_visible = self.data.loc[idx, \"lesion_visible\"]\n",
        "        # Note that targets must have the data type torch.long - a 64-bit integer,\n",
        "        # unlike the image tensor, which is usually a 32-bit float, the default\n",
        "        # dtype for tensors when none is given\n",
        "        if lesion_visible and liver_visible:\n",
        "            target = torch.tensor(2, dtype = torch.long)\n",
        "        elif not lesion_visible and liver_visible:\n",
        "            target = torch.tensor(1, dtype = torch.long)\n",
        "        elif not lesion_visible and not liver_visible:\n",
        "            target = torch.tensor(0, dtype = torch.long)\n",
        "        else:\n",
        "            print(\n",
        "                idx,\n",
        "                lesion_visible,\n",
        "                liver_visible,\n",
        "                self.data.loc[idx, \"liver_visible\"],\n",
        "                self.data.loc[idx, \"lesion_visible\"],\n",
        "                self.data.loc[idx, \"filename\"]\n",
        "                )\n",
        "            raise ValueError(\"Invalid target\")\n",
        "\n",
        "        return image, target\n",
        "\n",
        "train_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/train_classes.csv\", mode=\"train\")\n",
        "val_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/val_classes.csv\", mode=\"val\")\n",
        "test_dataset = LiTS_Dataset(csv = \"./Clean_LiTS/test_classes.csv\", mode=\"test\")\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 0,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    drop_last = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bdaccb5c",
      "metadata": {
        "id": "bdaccb5c"
      },
      "outputs": [],
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#class Really_Good_Model_forLIST(nn.Module):\n",
        "#    \"\"\"\n",
        "#    Ein kleines CNN, das am Ende 3 Klassen ausgibt.\n",
        "#    \"\"\"\n",
        "#\n",
        "#    def __init__(self, input_shape):\n",
        "#        super().__init__()\n",
        "\n",
        "\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 12, kernel_size=3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(12, 12, kernel_size=3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))  # auf 1x1 reduzieren\n",
        "        self.fc = nn.Linear(12, out_classes)  # 12 Kanäle -> out_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5efe25fa",
      "metadata": {
        "id": "5efe25fa"
      },
      "outputs": [],
      "source": [
        "# Now for the training loop\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9e04157a",
      "metadata": {
        "id": "9e04157a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f7a9ccc2-6488-40e6-a31b-42ca483163fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1651\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.1379\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 1.1161\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 1.0839\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 1.0653\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 1.0417\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.9622\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 1.0055\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.8643\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.7053\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.8465\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 0.5765\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.6663\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.7446\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 1.0907\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.7752\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.8394\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.8871\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.6086\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.9842\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.5847\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 1.0563\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 1.0455\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.6558\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.8340\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 1.1081\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.8124\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 1.0712\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.9957\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.6111\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.6692\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.9228\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.7413\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.7086\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.5544\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.9747\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.7561\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.6834\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 1.0990\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.8071\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 1.0080\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.6160\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.5741\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.7598\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.7607\n",
            "Epoch: 1,\t Validation Loss: 0.8262,\t Accuracy: 0.6927\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.4992\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.9451\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.6899\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 1.0336\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 0.5312\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 1.0640\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.7983\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.7891\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.5758\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.8006\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.8326\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.6506\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.8787\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.9209\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 1.0099\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 1.0216\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.7220\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 0.7231\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.7561\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.7104\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.8761\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.7936\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 1.0513\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.8475\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.7500\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.8324\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.7506\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.7821\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.7606\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 0.8884\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.9368\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.5569\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 1.1691\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.7258\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.7950\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.7218\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.6135\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.9237\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.9648\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.7162\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.8769\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.7530\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.8928\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.7882\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.7996\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 1.0761\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.7962\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 0.8259\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 1.0991\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 1.0841\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.8371\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 1.1227\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.9289\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.8159\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.9145\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.7919\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.7817\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.7735\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.7452\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.5447\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.6345\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 1.1185\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.7018\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.9730\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.6460\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.9043\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.6430\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 1.2524\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.7080\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.8929\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.7793\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.7545\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.8931\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.6985\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.8812\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.8456\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.9038\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.8261\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.6831\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.7828\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.8724\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.9036\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.7283\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.7759\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 1.0601\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.8725\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.7442\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.8730\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.8613\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.8234\n",
            "Epoch: 3,\t Validation Loss: 0.8162,\t Accuracy: 0.6927\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.7886\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 1.1336\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.9109\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.9580\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 0.8699\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.6847\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.7229\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.9476\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.8769\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 1.2427\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.8093\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.9637\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.9474\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.8603\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.9085\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 1.1303\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.6301\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.7818\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.8992\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.8920\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.6137\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.6487\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.7004\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.3869\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.6659\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.6999\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.9107\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.8154\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.5284\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 1.0340\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.8404\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.7245\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 1.0042\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.8693\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 1.2894\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.6836\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.8057\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.9854\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.6870\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.5458\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.5479\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.7731\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.6546\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.8367\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.9382\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.9645\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.8832\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.8520\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.8993\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.9251\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.8384\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.7739\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.8467\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.7421\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 1.0179\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 1.2254\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.8274\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 1.1202\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.6344\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.9137\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.9768\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.6345\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.9171\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.8173\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.6181\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.8099\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.8099\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.9691\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.6656\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.7125\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.7435\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.9265\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.7284\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.6655\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 1.0661\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.8138\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 1.0135\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.7729\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.7611\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.9470\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.6905\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.6652\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.7025\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.9557\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 1.1083\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.6214\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.6772\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.9393\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.7626\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.9103\n",
            "Epoch: 5,\t Validation Loss: 0.8147,\t Accuracy: 0.6927\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.6960\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.8689\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 1.0292\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.8918\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 1.0052\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.6999\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.5886\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.9945\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.9362\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.7203\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.7240\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.9050\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.7469\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.9076\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.6013\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.6905\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 1.0731\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.9237\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.9963\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.5637\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.7352\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.9445\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 1.0697\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.6474\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.9787\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.8411\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.9610\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.8278\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.8195\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.7999\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.7632\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.9881\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.8484\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.6409\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 1.0186\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.6092\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 1.0274\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 1.0162\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.7654\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.6624\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 1.0230\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 0.6314\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.7955\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.7829\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.8939\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.9256\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 1.0050\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 1.0166\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.8879\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.5449\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.7267\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.9734\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.7198\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.7937\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.6726\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.4717\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 1.0185\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 1.0096\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.6318\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.5207\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.7071\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.9031\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.7166\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 1.0765\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.7328\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.8017\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 1.1398\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 1.1343\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.7979\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.7293\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.6002\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.7835\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.8438\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.8313\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.8623\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.6681\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.8816\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.9777\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.7098\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.7140\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.8331\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.8444\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 1.0738\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.7264\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.7682\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.8696\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.7615\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.9125\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.6487\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.7263\n",
            "Epoch: 7,\t Validation Loss: 0.8147,\t Accuracy: 0.6927\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.7820\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.7771\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.9172\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.9610\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.8893\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.7431\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.7046\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.7066\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.5805\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.6615\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 1.0975\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.9749\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.7030\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.9344\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.9512\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 1.0160\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.6917\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.6914\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.6413\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.6189\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.8801\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 1.1335\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 1.0692\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.7128\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.9315\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.8839\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.7489\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 1.0509\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.6290\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 1.1585\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 1.0297\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.7270\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.7536\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.9606\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.8979\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.7702\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.6317\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.6646\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.7814\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.8291\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.8848\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.6645\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.8874\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.8220\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.7082\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 1.1739\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.9523\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.8642\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.5536\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.8482\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.7577\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.8401\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 1.0499\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.6794\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.8713\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.9367\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.8223\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.6779\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.8828\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 1.0743\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.7178\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.8113\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 1.0387\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.7104\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.8601\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.8009\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.8549\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.7276\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.6657\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.9362\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 1.0089\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.6325\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.6274\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.8356\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.8070\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.8038\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.9297\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.7545\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.8656\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.6303\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 1.1444\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.5539\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.6481\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.4974\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.8213\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 1.0594\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.7209\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.9241\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.6183\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.7799\n",
            "Epoch: 9,\t Validation Loss: 0.8145,\t Accuracy: 0.6927\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.7367\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 0.9181\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.7225\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.5340\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.9617\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.7887\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.8156\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 1.0575\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.7092\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 1.0719\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.7963\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.6133\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.7493\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.9908\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.6561\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.8858\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.6104\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.9684\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.8003\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.7282\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 1.0503\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 1.0305\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.7843\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 1.0745\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.6850\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 1.0276\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.8279\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.7032\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.9887\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.6720\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 1.0096\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.8876\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.7013\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.8339\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.9006\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.6521\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.5687\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.9703\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.9109\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.5672\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.5953\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.7994\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.8935\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.7651\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.6354\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.9776\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.7221\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.8379\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 1.0563\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.9731\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.7497\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 1.0310\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.7649\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 1.1465\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 1.0614\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.7163\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.8375\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.9168\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.6067\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.9491\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 1.1035\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 1.0622\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.9006\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.7161\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.7924\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.5110\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.8646\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.8599\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.8367\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.8227\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.8723\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 1.0156\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.7954\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.7065\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.7572\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.7674\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.8316\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.7625\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.6894\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.7454\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.9444\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.6547\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 1.0374\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.7207\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.5184\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.7594\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.5672\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 1.0273\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.6447\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.7319\n",
            "Epoch: 11,\t Validation Loss: 0.8146,\t Accuracy: 0.6927\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.9803\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.6728\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.7867\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.8348\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.8257\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.9544\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.9868\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.5409\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.8114\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.9643\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.7394\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.9946\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.8098\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.9789\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.7255\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.6859\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.8918\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.8076\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.9985\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.9864\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.7704\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.4902\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.8476\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.5814\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.5369\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.8538\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.7452\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.7149\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.6984\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.9833\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.9487\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 1.0726\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.6494\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.6557\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 1.2241\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.7069\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.7894\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.6809\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 1.0425\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.9328\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.6377\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.6571\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.7379\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.9356\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 1.0859\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.9296\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.5234\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.9727\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.6973\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.7577\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.7703\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 1.2984\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.8497\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.6264\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.9443\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.6063\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.6978\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.8423\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.6630\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.8225\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.7756\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.9382\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.6562\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.8526\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.7133\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.9927\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.9026\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 1.0201\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.7554\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.8451\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.6590\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.9890\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.9054\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.6553\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.7122\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.7588\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.8329\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.8357\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.9256\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.8622\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 1.0158\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.8275\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.6804\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.9319\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 1.0774\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.9829\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.5588\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.6536\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.9559\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.8900\n",
            "Epoch: 13,\t Validation Loss: 0.8136,\t Accuracy: 0.6927\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.8043\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.6676\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.5784\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.7543\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 1.1821\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.8844\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.8797\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.6849\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.9087\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.6017\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.7259\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.8078\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.9307\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.6576\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.7562\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.7503\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.8597\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 1.0084\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.9879\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.6410\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.8079\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 1.1369\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 1.0710\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.8443\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.7282\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.7658\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.7767\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.8300\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.6807\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 1.0361\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.5315\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 1.1208\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.8367\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.4753\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.8846\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.9958\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.6557\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.6865\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.6559\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.3290\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.7982\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.7584\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.9183\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.7111\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.7378\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.6345\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.8832\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.8285\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.6486\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.5639\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.6528\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 1.1550\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.7495\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.8712\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.9518\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.8958\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.6654\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.8551\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.9419\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.9712\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.9034\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.5439\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.6911\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 1.0067\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.5013\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 1.0391\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.8564\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.8230\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.7200\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 1.1780\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.8643\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.8075\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.8382\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.8080\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.5518\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.8191\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.7541\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 1.0713\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.5337\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.7585\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.7637\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 1.1048\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.6552\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 1.1781\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 1.0116\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.9269\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 1.0271\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 1.0151\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.6612\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 1.0203\n",
            "Epoch: 15,\t Validation Loss: 0.8137,\t Accuracy: 0.6927\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_loop(YourModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "10ff035d",
      "metadata": {
        "id": "10ff035d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "22d2b78e-1e92-4892-ccfc-abc29adaf0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9142,\t Accuracy: 0.6004\n"
          ]
        }
      ],
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605399f3",
      "metadata": {
        "id": "605399f3"
      },
      "source": [
        "### L2-Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1befc94e",
      "metadata": {
        "id": "1befc94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4d0a81-37fd-480a-cb64-0149fd749d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.1946\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.0370\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.9896\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.8152\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 0.9839\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 0.7676\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.8934\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 1.0935\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.7872\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 0.8388\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.6807\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 1.2282\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.9509\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 0.7976\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.7888\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.7489\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.8403\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.8912\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.7290\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.8809\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 0.6692\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 1.1708\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.8765\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.7940\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.8076\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.8063\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.8696\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 1.0263\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.7223\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 0.9988\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.8308\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 0.5841\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.7950\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 0.7312\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 0.7432\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 0.6504\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 0.9222\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 0.7049\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.9204\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.6996\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.9500\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.8426\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.4806\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.8454\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.9792\n",
            "Epoch: 1,\t Validation Loss: 0.8420,\t Accuracy: 0.6927\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.5780\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 1.0822\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.5473\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.8316\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 1.1257\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.9266\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.7943\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.9694\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.4541\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.8106\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.9274\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.7950\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.5384\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.9572\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.7678\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.6191\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.7347\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 1.3297\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.8333\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.7563\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.8679\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.8354\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.9062\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.6507\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 0.6234\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 0.5459\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.6769\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.6954\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.9165\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 1.0134\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 1.2507\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.7644\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.7901\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 1.0126\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.9669\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 0.5749\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 0.7945\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 1.1849\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.7886\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.9363\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.6276\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 0.9433\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 0.7966\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.8920\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.8352\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.7910\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.8320\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 1.1855\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 0.6623\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 0.7979\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.8959\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 1.1035\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.8668\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.4524\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.8894\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.9884\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.8889\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.6558\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.7461\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.7676\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 1.0247\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 1.1273\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.5893\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.6795\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.8585\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.5823\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.8459\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 1.0186\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.7009\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.7917\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.9515\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.8958\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.5986\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.7272\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.9472\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.6593\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.6409\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.9701\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.9927\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 1.0750\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.9015\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.8679\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 0.8086\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.7487\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.5819\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.9766\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.8613\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.5411\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.6869\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.7094\n",
            "Epoch: 3,\t Validation Loss: 0.8211,\t Accuracy: 0.6927\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.8171\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.8651\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.6279\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.5981\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 1.2226\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.5816\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.8498\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.8579\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.8856\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.8602\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.8006\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 0.6397\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.6822\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.7841\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.8311\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.8364\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 1.1348\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.9659\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.7326\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.7208\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.8469\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.7008\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 1.0332\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.6940\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 1.2815\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.8357\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.8820\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 1.2531\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.8100\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.6877\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.8489\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.9761\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.6679\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 1.1390\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.4783\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.9006\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.8727\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.9263\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.7236\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 1.1289\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.9061\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.7486\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.7078\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 1.0420\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.8291\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.8064\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 1.0428\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.8448\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.8421\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.8836\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.8568\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.6466\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.9551\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.9135\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.8380\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.7547\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.7276\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.7676\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.8908\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.9698\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.5448\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.7547\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.5945\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.9390\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.6231\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.8823\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.9046\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.6125\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.9407\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.8066\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 0.5955\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.7900\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 1.1373\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.8451\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.9539\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.7545\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.9352\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.6841\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.8220\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.6977\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 1.0786\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.6859\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 1.0123\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 1.1545\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.7713\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.7105\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.9293\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.8326\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 1.0503\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 1.2681\n",
            "Epoch: 5,\t Validation Loss: 0.8171,\t Accuracy: 0.6927\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.6550\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 1.0562\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.9840\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.9410\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.8735\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.6248\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.7558\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.8133\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.9077\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 1.5609\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.8926\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 1.3172\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.9194\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.9118\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.8611\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.7510\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.8558\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.5720\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.7747\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.9132\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 1.1680\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.6134\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.8052\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.7482\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.6736\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.5293\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.5483\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 0.7120\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 1.1234\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.7068\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.8441\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.8525\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.7899\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.8453\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.8188\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.6956\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.7605\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.8594\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.8256\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.8242\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.8258\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 1.2049\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 1.0035\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.8489\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.9292\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 1.0196\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.7834\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.7812\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.5717\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.8224\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.9899\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 1.2510\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.6651\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.5534\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.6599\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 1.1317\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.6159\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.8166\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.8603\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.9046\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.8022\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.7266\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.8100\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.8313\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 0.7577\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.7028\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.7152\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.8480\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 1.1998\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 1.1287\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 0.7233\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.9153\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.8261\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 1.1607\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 1.1296\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.7192\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 1.2119\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.9300\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.6783\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 1.1061\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 0.8947\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.9174\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.3520\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.6218\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.8283\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.6349\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 1.1056\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.8120\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.8072\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.8378\n",
            "Epoch: 7,\t Validation Loss: 0.8172,\t Accuracy: 0.6927\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.9244\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.8673\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.6949\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.7624\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.7955\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 1.0800\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 1.1819\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.7401\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.8144\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 1.0081\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 1.1671\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 1.0054\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.9372\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.7632\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.9765\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.6493\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.8145\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.8631\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.8519\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.8240\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 1.2137\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.9592\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.8183\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.8077\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.7487\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.7885\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.9948\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 1.1171\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 0.9866\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.9331\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.7879\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.5742\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.5627\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.5763\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.7677\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.9203\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.8842\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.6014\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.8361\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.8062\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.8687\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.7833\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.7528\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.7842\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.9017\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.5936\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.5048\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.6861\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.5764\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.5129\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 1.0452\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.5643\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.9784\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.9312\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.7892\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.7825\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.8313\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.7187\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.6499\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.9591\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.5390\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.7793\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.8756\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.9868\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.8276\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.9349\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.7458\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.8098\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.6526\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.5373\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.6272\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.8214\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.6200\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.7412\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 0.8530\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.8945\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.6864\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.5576\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.9091\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.7573\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.4615\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.6690\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 1.1376\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.8664\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.8420\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 1.0058\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.8903\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.8499\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 1.0600\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.8644\n",
            "Epoch: 9,\t Validation Loss: 0.8165,\t Accuracy: 0.6927\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.6376\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 1.0876\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.9279\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 1.2167\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 1.0687\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.9408\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.8548\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 0.7769\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.8089\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.8042\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 1.0770\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.8501\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.6869\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.7297\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.8829\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.5574\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.6318\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 1.0294\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.5730\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.9471\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.7285\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.7953\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.8505\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.6149\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.9993\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 1.0124\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.7009\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.6576\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.8756\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.7286\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.7286\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.5238\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 1.0331\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 1.2568\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.8867\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 1.0115\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.4488\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.7637\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.6298\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 1.0866\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.7619\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.5993\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 1.0084\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.9804\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.9052\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.7801\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.7409\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.7105\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.7781\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.6038\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.8645\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.9303\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.5347\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.7273\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.8629\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.9950\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.8025\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.8905\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.9892\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.7904\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.7922\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.6300\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.9069\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.9840\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.9464\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.7755\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.8811\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.7412\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.8789\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.8643\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.7729\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 1.0516\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 1.2703\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.7396\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.8365\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.5464\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.7110\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.6111\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.7892\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.7081\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 1.0627\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 0.7604\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.7850\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.5445\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.8458\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 1.0479\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.9944\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.6880\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.6599\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.8402\n",
            "Epoch: 11,\t Validation Loss: 0.8160,\t Accuracy: 0.6927\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.6939\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.6820\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.8275\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.8028\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.7145\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 1.1106\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 1.2142\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 1.0873\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 0.9543\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.7687\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 1.0581\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.8216\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.7016\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.7956\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.8572\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.5555\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.8460\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.7430\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.9013\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.7985\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.7640\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.7231\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.7593\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.8872\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.8807\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.5598\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.8452\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.9394\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.8070\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.9087\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.9106\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.8410\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.7519\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.7687\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.7413\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.9407\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 1.1487\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.7990\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.6734\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.7388\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.6768\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.9317\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.5578\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.9829\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 1.1259\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.7654\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.8771\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.7475\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 1.4583\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.6405\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.8374\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.6864\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.9058\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 1.0863\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.8727\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.5776\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 1.3834\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.6004\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 1.0189\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.6102\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.7164\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 0.7888\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.7837\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.8540\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.6662\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 0.7728\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.6838\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 1.0441\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.8198\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.7972\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.8787\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.7371\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.8847\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.5898\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.6036\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.8986\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.9772\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.6894\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.7956\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.8183\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 0.6026\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.5426\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.8570\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.9667\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 1.0014\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.6505\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.9921\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 1.0004\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 1.0211\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.7066\n",
            "Epoch: 13,\t Validation Loss: 0.8159,\t Accuracy: 0.6927\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.7092\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.8552\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.7595\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.6677\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.7634\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 1.0866\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 1.1379\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.6400\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.6826\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.7595\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.7661\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.5537\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 1.0399\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.6312\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.5725\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.8244\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.8632\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.7839\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.5853\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.6093\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.9216\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.7108\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.8336\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.8850\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.8648\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.7053\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.7903\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.8726\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.6528\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.8723\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.8043\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.7391\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.5783\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.6928\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.9952\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.5557\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.7681\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.7213\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.5375\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.6188\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.6842\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.5333\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.8473\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.6274\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 1.0622\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.6922\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.8838\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.8692\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.7674\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.6693\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.7821\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.9870\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.5112\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.7075\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.7939\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.8180\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.9860\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.9593\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.5073\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.7381\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.6975\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.9671\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.8937\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.8078\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.5778\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.8370\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.5423\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.7326\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.8277\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 1.1448\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.7195\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.7730\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.7847\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 1.0134\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.7390\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.9648\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.9490\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.8837\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.7049\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.8137\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.7946\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.9512\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.8570\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.6797\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.6377\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.9754\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.8052\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.7116\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.8476\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.7015\n",
            "Epoch: 15,\t Validation Loss: 0.8130,\t Accuracy: 0.6927\n"
          ]
        }
      ],
      "source": [
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop_L2(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model\n",
        "trained_model_L2 = train_loop_L2(YourModel)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_model_L2.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_L2(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SYmkjfQ1pRj",
        "outputId": "42b9777d-2691-49d8-ea2b-c963ce76f5a9"
      },
      "id": "7SYmkjfQ1pRj",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9193,\t Accuracy: 0.6004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e200dbd0",
      "metadata": {
        "id": "e200dbd0"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "811813fc",
      "metadata": {
        "id": "811813fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class YourModel_drop(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 12, kernel_size=3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(12, 12, kernel_size=3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4,4))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(p=0.5)  # 50% Dropout\n",
        "        self.fc = nn.Linear(12*4*4, out_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)  # Dropout nur während Training\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop(your_model):\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params = model.parameters(), lr=3e-4)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_model_drop = train_loop(YourModel_drop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_W1ic5av5H7j",
        "outputId": "12a366c8-5d37-46e2-f4bd-630542477a5f"
      },
      "id": "_W1ic5av5H7j",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]\t Step [1/2217]\t Loss: 1.0929\n",
            "Epoch [1/15]\t Step [51/2217]\t Loss: 1.0745\n",
            "Epoch [1/15]\t Step [101/2217]\t Loss: 0.9964\n",
            "Epoch [1/15]\t Step [151/2217]\t Loss: 0.8874\n",
            "Epoch [1/15]\t Step [201/2217]\t Loss: 1.1231\n",
            "Epoch [1/15]\t Step [251/2217]\t Loss: 1.0615\n",
            "Epoch [1/15]\t Step [301/2217]\t Loss: 0.9312\n",
            "Epoch [1/15]\t Step [351/2217]\t Loss: 0.6413\n",
            "Epoch [1/15]\t Step [401/2217]\t Loss: 0.8140\n",
            "Epoch [1/15]\t Step [451/2217]\t Loss: 1.0501\n",
            "Epoch [1/15]\t Step [501/2217]\t Loss: 0.8749\n",
            "Epoch [1/15]\t Step [551/2217]\t Loss: 1.1840\n",
            "Epoch [1/15]\t Step [601/2217]\t Loss: 0.7739\n",
            "Epoch [1/15]\t Step [651/2217]\t Loss: 1.1292\n",
            "Epoch [1/15]\t Step [701/2217]\t Loss: 0.7792\n",
            "Epoch [1/15]\t Step [751/2217]\t Loss: 0.8944\n",
            "Epoch [1/15]\t Step [801/2217]\t Loss: 0.8995\n",
            "Epoch [1/15]\t Step [851/2217]\t Loss: 0.9590\n",
            "Epoch [1/15]\t Step [901/2217]\t Loss: 0.7237\n",
            "Epoch [1/15]\t Step [951/2217]\t Loss: 0.8245\n",
            "Epoch [1/15]\t Step [1001/2217]\t Loss: 1.0166\n",
            "Epoch [1/15]\t Step [1051/2217]\t Loss: 0.7294\n",
            "Epoch [1/15]\t Step [1101/2217]\t Loss: 0.6578\n",
            "Epoch [1/15]\t Step [1151/2217]\t Loss: 0.6763\n",
            "Epoch [1/15]\t Step [1201/2217]\t Loss: 0.5508\n",
            "Epoch [1/15]\t Step [1251/2217]\t Loss: 0.9969\n",
            "Epoch [1/15]\t Step [1301/2217]\t Loss: 0.8768\n",
            "Epoch [1/15]\t Step [1351/2217]\t Loss: 0.7209\n",
            "Epoch [1/15]\t Step [1401/2217]\t Loss: 0.7854\n",
            "Epoch [1/15]\t Step [1451/2217]\t Loss: 1.1028\n",
            "Epoch [1/15]\t Step [1501/2217]\t Loss: 0.5591\n",
            "Epoch [1/15]\t Step [1551/2217]\t Loss: 1.1133\n",
            "Epoch [1/15]\t Step [1601/2217]\t Loss: 0.8483\n",
            "Epoch [1/15]\t Step [1651/2217]\t Loss: 1.0867\n",
            "Epoch [1/15]\t Step [1701/2217]\t Loss: 1.0790\n",
            "Epoch [1/15]\t Step [1751/2217]\t Loss: 1.1156\n",
            "Epoch [1/15]\t Step [1801/2217]\t Loss: 1.0549\n",
            "Epoch [1/15]\t Step [1851/2217]\t Loss: 1.0469\n",
            "Epoch [1/15]\t Step [1901/2217]\t Loss: 0.5451\n",
            "Epoch [1/15]\t Step [1951/2217]\t Loss: 0.7690\n",
            "Epoch [1/15]\t Step [2001/2217]\t Loss: 0.9499\n",
            "Epoch [1/15]\t Step [2051/2217]\t Loss: 0.6550\n",
            "Epoch [1/15]\t Step [2101/2217]\t Loss: 0.8893\n",
            "Epoch [1/15]\t Step [2151/2217]\t Loss: 0.8709\n",
            "Epoch [1/15]\t Step [2201/2217]\t Loss: 0.8590\n",
            "Epoch: 1,\t Validation Loss: 0.8128,\t Accuracy: 0.6927\n",
            "Epoch [2/15]\t Step [1/2217]\t Loss: 0.8810\n",
            "Epoch [2/15]\t Step [51/2217]\t Loss: 0.8925\n",
            "Epoch [2/15]\t Step [101/2217]\t Loss: 0.7096\n",
            "Epoch [2/15]\t Step [151/2217]\t Loss: 0.5397\n",
            "Epoch [2/15]\t Step [201/2217]\t Loss: 1.0431\n",
            "Epoch [2/15]\t Step [251/2217]\t Loss: 0.6389\n",
            "Epoch [2/15]\t Step [301/2217]\t Loss: 0.8630\n",
            "Epoch [2/15]\t Step [351/2217]\t Loss: 0.7588\n",
            "Epoch [2/15]\t Step [401/2217]\t Loss: 0.7225\n",
            "Epoch [2/15]\t Step [451/2217]\t Loss: 0.8435\n",
            "Epoch [2/15]\t Step [501/2217]\t Loss: 0.8390\n",
            "Epoch [2/15]\t Step [551/2217]\t Loss: 0.6203\n",
            "Epoch [2/15]\t Step [601/2217]\t Loss: 0.7177\n",
            "Epoch [2/15]\t Step [651/2217]\t Loss: 0.8418\n",
            "Epoch [2/15]\t Step [701/2217]\t Loss: 0.8779\n",
            "Epoch [2/15]\t Step [751/2217]\t Loss: 0.6745\n",
            "Epoch [2/15]\t Step [801/2217]\t Loss: 0.8173\n",
            "Epoch [2/15]\t Step [851/2217]\t Loss: 1.0010\n",
            "Epoch [2/15]\t Step [901/2217]\t Loss: 0.7271\n",
            "Epoch [2/15]\t Step [951/2217]\t Loss: 0.8569\n",
            "Epoch [2/15]\t Step [1001/2217]\t Loss: 0.8347\n",
            "Epoch [2/15]\t Step [1051/2217]\t Loss: 0.6600\n",
            "Epoch [2/15]\t Step [1101/2217]\t Loss: 0.9242\n",
            "Epoch [2/15]\t Step [1151/2217]\t Loss: 0.9205\n",
            "Epoch [2/15]\t Step [1201/2217]\t Loss: 1.2393\n",
            "Epoch [2/15]\t Step [1251/2217]\t Loss: 1.0258\n",
            "Epoch [2/15]\t Step [1301/2217]\t Loss: 0.6316\n",
            "Epoch [2/15]\t Step [1351/2217]\t Loss: 0.8004\n",
            "Epoch [2/15]\t Step [1401/2217]\t Loss: 0.9242\n",
            "Epoch [2/15]\t Step [1451/2217]\t Loss: 1.1601\n",
            "Epoch [2/15]\t Step [1501/2217]\t Loss: 0.7161\n",
            "Epoch [2/15]\t Step [1551/2217]\t Loss: 0.9173\n",
            "Epoch [2/15]\t Step [1601/2217]\t Loss: 0.6737\n",
            "Epoch [2/15]\t Step [1651/2217]\t Loss: 0.7820\n",
            "Epoch [2/15]\t Step [1701/2217]\t Loss: 0.7123\n",
            "Epoch [2/15]\t Step [1751/2217]\t Loss: 1.0814\n",
            "Epoch [2/15]\t Step [1801/2217]\t Loss: 1.1628\n",
            "Epoch [2/15]\t Step [1851/2217]\t Loss: 0.5598\n",
            "Epoch [2/15]\t Step [1901/2217]\t Loss: 0.7887\n",
            "Epoch [2/15]\t Step [1951/2217]\t Loss: 0.7887\n",
            "Epoch [2/15]\t Step [2001/2217]\t Loss: 0.8936\n",
            "Epoch [2/15]\t Step [2051/2217]\t Loss: 1.0086\n",
            "Epoch [2/15]\t Step [2101/2217]\t Loss: 1.0330\n",
            "Epoch [2/15]\t Step [2151/2217]\t Loss: 0.9110\n",
            "Epoch [2/15]\t Step [2201/2217]\t Loss: 0.8869\n",
            "Epoch [3/15]\t Step [1/2217]\t Loss: 0.5891\n",
            "Epoch [3/15]\t Step [51/2217]\t Loss: 0.9807\n",
            "Epoch [3/15]\t Step [101/2217]\t Loss: 1.0329\n",
            "Epoch [3/15]\t Step [151/2217]\t Loss: 1.0260\n",
            "Epoch [3/15]\t Step [201/2217]\t Loss: 1.0278\n",
            "Epoch [3/15]\t Step [251/2217]\t Loss: 0.7095\n",
            "Epoch [3/15]\t Step [301/2217]\t Loss: 0.7828\n",
            "Epoch [3/15]\t Step [351/2217]\t Loss: 0.7180\n",
            "Epoch [3/15]\t Step [401/2217]\t Loss: 0.9594\n",
            "Epoch [3/15]\t Step [451/2217]\t Loss: 0.8781\n",
            "Epoch [3/15]\t Step [501/2217]\t Loss: 0.7237\n",
            "Epoch [3/15]\t Step [551/2217]\t Loss: 0.5128\n",
            "Epoch [3/15]\t Step [601/2217]\t Loss: 0.8130\n",
            "Epoch [3/15]\t Step [651/2217]\t Loss: 0.6374\n",
            "Epoch [3/15]\t Step [701/2217]\t Loss: 0.8591\n",
            "Epoch [3/15]\t Step [751/2217]\t Loss: 0.8450\n",
            "Epoch [3/15]\t Step [801/2217]\t Loss: 1.0381\n",
            "Epoch [3/15]\t Step [851/2217]\t Loss: 0.6198\n",
            "Epoch [3/15]\t Step [901/2217]\t Loss: 0.7497\n",
            "Epoch [3/15]\t Step [951/2217]\t Loss: 0.8378\n",
            "Epoch [3/15]\t Step [1001/2217]\t Loss: 0.7690\n",
            "Epoch [3/15]\t Step [1051/2217]\t Loss: 0.7838\n",
            "Epoch [3/15]\t Step [1101/2217]\t Loss: 0.7709\n",
            "Epoch [3/15]\t Step [1151/2217]\t Loss: 0.8456\n",
            "Epoch [3/15]\t Step [1201/2217]\t Loss: 0.9761\n",
            "Epoch [3/15]\t Step [1251/2217]\t Loss: 0.4615\n",
            "Epoch [3/15]\t Step [1301/2217]\t Loss: 0.6722\n",
            "Epoch [3/15]\t Step [1351/2217]\t Loss: 0.5931\n",
            "Epoch [3/15]\t Step [1401/2217]\t Loss: 0.6436\n",
            "Epoch [3/15]\t Step [1451/2217]\t Loss: 0.7681\n",
            "Epoch [3/15]\t Step [1501/2217]\t Loss: 0.4934\n",
            "Epoch [3/15]\t Step [1551/2217]\t Loss: 0.8454\n",
            "Epoch [3/15]\t Step [1601/2217]\t Loss: 0.8893\n",
            "Epoch [3/15]\t Step [1651/2217]\t Loss: 0.4827\n",
            "Epoch [3/15]\t Step [1701/2217]\t Loss: 0.6313\n",
            "Epoch [3/15]\t Step [1751/2217]\t Loss: 0.9168\n",
            "Epoch [3/15]\t Step [1801/2217]\t Loss: 0.6689\n",
            "Epoch [3/15]\t Step [1851/2217]\t Loss: 1.0049\n",
            "Epoch [3/15]\t Step [1901/2217]\t Loss: 0.6938\n",
            "Epoch [3/15]\t Step [1951/2217]\t Loss: 0.5755\n",
            "Epoch [3/15]\t Step [2001/2217]\t Loss: 0.6388\n",
            "Epoch [3/15]\t Step [2051/2217]\t Loss: 0.6958\n",
            "Epoch [3/15]\t Step [2101/2217]\t Loss: 0.5622\n",
            "Epoch [3/15]\t Step [2151/2217]\t Loss: 0.7393\n",
            "Epoch [3/15]\t Step [2201/2217]\t Loss: 0.7499\n",
            "Epoch: 3,\t Validation Loss: 0.7324,\t Accuracy: 0.6927\n",
            "Epoch [4/15]\t Step [1/2217]\t Loss: 0.6897\n",
            "Epoch [4/15]\t Step [51/2217]\t Loss: 0.7396\n",
            "Epoch [4/15]\t Step [101/2217]\t Loss: 0.9773\n",
            "Epoch [4/15]\t Step [151/2217]\t Loss: 0.6726\n",
            "Epoch [4/15]\t Step [201/2217]\t Loss: 1.0848\n",
            "Epoch [4/15]\t Step [251/2217]\t Loss: 0.8299\n",
            "Epoch [4/15]\t Step [301/2217]\t Loss: 0.7769\n",
            "Epoch [4/15]\t Step [351/2217]\t Loss: 0.7940\n",
            "Epoch [4/15]\t Step [401/2217]\t Loss: 0.7838\n",
            "Epoch [4/15]\t Step [451/2217]\t Loss: 0.9490\n",
            "Epoch [4/15]\t Step [501/2217]\t Loss: 0.5643\n",
            "Epoch [4/15]\t Step [551/2217]\t Loss: 1.0601\n",
            "Epoch [4/15]\t Step [601/2217]\t Loss: 0.4134\n",
            "Epoch [4/15]\t Step [651/2217]\t Loss: 0.6569\n",
            "Epoch [4/15]\t Step [701/2217]\t Loss: 0.7244\n",
            "Epoch [4/15]\t Step [751/2217]\t Loss: 0.6821\n",
            "Epoch [4/15]\t Step [801/2217]\t Loss: 0.9863\n",
            "Epoch [4/15]\t Step [851/2217]\t Loss: 0.6680\n",
            "Epoch [4/15]\t Step [901/2217]\t Loss: 0.7102\n",
            "Epoch [4/15]\t Step [951/2217]\t Loss: 0.6297\n",
            "Epoch [4/15]\t Step [1001/2217]\t Loss: 0.7849\n",
            "Epoch [4/15]\t Step [1051/2217]\t Loss: 0.7112\n",
            "Epoch [4/15]\t Step [1101/2217]\t Loss: 0.7817\n",
            "Epoch [4/15]\t Step [1151/2217]\t Loss: 0.4778\n",
            "Epoch [4/15]\t Step [1201/2217]\t Loss: 0.7530\n",
            "Epoch [4/15]\t Step [1251/2217]\t Loss: 0.7521\n",
            "Epoch [4/15]\t Step [1301/2217]\t Loss: 0.8192\n",
            "Epoch [4/15]\t Step [1351/2217]\t Loss: 0.6097\n",
            "Epoch [4/15]\t Step [1401/2217]\t Loss: 0.6833\n",
            "Epoch [4/15]\t Step [1451/2217]\t Loss: 0.8186\n",
            "Epoch [4/15]\t Step [1501/2217]\t Loss: 0.3581\n",
            "Epoch [4/15]\t Step [1551/2217]\t Loss: 0.7523\n",
            "Epoch [4/15]\t Step [1601/2217]\t Loss: 0.9082\n",
            "Epoch [4/15]\t Step [1651/2217]\t Loss: 0.8566\n",
            "Epoch [4/15]\t Step [1701/2217]\t Loss: 0.5802\n",
            "Epoch [4/15]\t Step [1751/2217]\t Loss: 0.7044\n",
            "Epoch [4/15]\t Step [1801/2217]\t Loss: 0.5757\n",
            "Epoch [4/15]\t Step [1851/2217]\t Loss: 0.8275\n",
            "Epoch [4/15]\t Step [1901/2217]\t Loss: 0.7443\n",
            "Epoch [4/15]\t Step [1951/2217]\t Loss: 0.5646\n",
            "Epoch [4/15]\t Step [2001/2217]\t Loss: 0.7462\n",
            "Epoch [4/15]\t Step [2051/2217]\t Loss: 0.6183\n",
            "Epoch [4/15]\t Step [2101/2217]\t Loss: 0.8786\n",
            "Epoch [4/15]\t Step [2151/2217]\t Loss: 0.8049\n",
            "Epoch [4/15]\t Step [2201/2217]\t Loss: 0.5268\n",
            "Epoch [5/15]\t Step [1/2217]\t Loss: 0.8746\n",
            "Epoch [5/15]\t Step [51/2217]\t Loss: 0.7418\n",
            "Epoch [5/15]\t Step [101/2217]\t Loss: 0.9306\n",
            "Epoch [5/15]\t Step [151/2217]\t Loss: 0.8906\n",
            "Epoch [5/15]\t Step [201/2217]\t Loss: 0.9482\n",
            "Epoch [5/15]\t Step [251/2217]\t Loss: 0.8120\n",
            "Epoch [5/15]\t Step [301/2217]\t Loss: 0.6339\n",
            "Epoch [5/15]\t Step [351/2217]\t Loss: 0.6952\n",
            "Epoch [5/15]\t Step [401/2217]\t Loss: 0.8167\n",
            "Epoch [5/15]\t Step [451/2217]\t Loss: 0.6642\n",
            "Epoch [5/15]\t Step [501/2217]\t Loss: 0.6957\n",
            "Epoch [5/15]\t Step [551/2217]\t Loss: 0.6857\n",
            "Epoch [5/15]\t Step [601/2217]\t Loss: 0.8791\n",
            "Epoch [5/15]\t Step [651/2217]\t Loss: 0.6313\n",
            "Epoch [5/15]\t Step [701/2217]\t Loss: 0.5235\n",
            "Epoch [5/15]\t Step [751/2217]\t Loss: 0.8891\n",
            "Epoch [5/15]\t Step [801/2217]\t Loss: 0.6584\n",
            "Epoch [5/15]\t Step [851/2217]\t Loss: 0.6590\n",
            "Epoch [5/15]\t Step [901/2217]\t Loss: 0.7170\n",
            "Epoch [5/15]\t Step [951/2217]\t Loss: 0.6463\n",
            "Epoch [5/15]\t Step [1001/2217]\t Loss: 0.8532\n",
            "Epoch [5/15]\t Step [1051/2217]\t Loss: 0.5637\n",
            "Epoch [5/15]\t Step [1101/2217]\t Loss: 0.6879\n",
            "Epoch [5/15]\t Step [1151/2217]\t Loss: 0.5432\n",
            "Epoch [5/15]\t Step [1201/2217]\t Loss: 0.5662\n",
            "Epoch [5/15]\t Step [1251/2217]\t Loss: 1.1772\n",
            "Epoch [5/15]\t Step [1301/2217]\t Loss: 0.7647\n",
            "Epoch [5/15]\t Step [1351/2217]\t Loss: 0.6629\n",
            "Epoch [5/15]\t Step [1401/2217]\t Loss: 0.8320\n",
            "Epoch [5/15]\t Step [1451/2217]\t Loss: 0.9108\n",
            "Epoch [5/15]\t Step [1501/2217]\t Loss: 0.7965\n",
            "Epoch [5/15]\t Step [1551/2217]\t Loss: 0.5465\n",
            "Epoch [5/15]\t Step [1601/2217]\t Loss: 0.7175\n",
            "Epoch [5/15]\t Step [1651/2217]\t Loss: 0.6138\n",
            "Epoch [5/15]\t Step [1701/2217]\t Loss: 0.6884\n",
            "Epoch [5/15]\t Step [1751/2217]\t Loss: 0.7093\n",
            "Epoch [5/15]\t Step [1801/2217]\t Loss: 0.7854\n",
            "Epoch [5/15]\t Step [1851/2217]\t Loss: 0.9265\n",
            "Epoch [5/15]\t Step [1901/2217]\t Loss: 0.9767\n",
            "Epoch [5/15]\t Step [1951/2217]\t Loss: 0.7783\n",
            "Epoch [5/15]\t Step [2001/2217]\t Loss: 0.9563\n",
            "Epoch [5/15]\t Step [2051/2217]\t Loss: 0.7988\n",
            "Epoch [5/15]\t Step [2101/2217]\t Loss: 0.4923\n",
            "Epoch [5/15]\t Step [2151/2217]\t Loss: 0.5793\n",
            "Epoch [5/15]\t Step [2201/2217]\t Loss: 0.6882\n",
            "Epoch: 5,\t Validation Loss: 0.6662,\t Accuracy: 0.7121\n",
            "Epoch [6/15]\t Step [1/2217]\t Loss: 0.8133\n",
            "Epoch [6/15]\t Step [51/2217]\t Loss: 0.4118\n",
            "Epoch [6/15]\t Step [101/2217]\t Loss: 0.8139\n",
            "Epoch [6/15]\t Step [151/2217]\t Loss: 0.6711\n",
            "Epoch [6/15]\t Step [201/2217]\t Loss: 0.6636\n",
            "Epoch [6/15]\t Step [251/2217]\t Loss: 0.5904\n",
            "Epoch [6/15]\t Step [301/2217]\t Loss: 0.7593\n",
            "Epoch [6/15]\t Step [351/2217]\t Loss: 0.6631\n",
            "Epoch [6/15]\t Step [401/2217]\t Loss: 0.6965\n",
            "Epoch [6/15]\t Step [451/2217]\t Loss: 0.6606\n",
            "Epoch [6/15]\t Step [501/2217]\t Loss: 0.5862\n",
            "Epoch [6/15]\t Step [551/2217]\t Loss: 0.6845\n",
            "Epoch [6/15]\t Step [601/2217]\t Loss: 0.5065\n",
            "Epoch [6/15]\t Step [651/2217]\t Loss: 0.9351\n",
            "Epoch [6/15]\t Step [701/2217]\t Loss: 0.6166\n",
            "Epoch [6/15]\t Step [751/2217]\t Loss: 0.8485\n",
            "Epoch [6/15]\t Step [801/2217]\t Loss: 0.5490\n",
            "Epoch [6/15]\t Step [851/2217]\t Loss: 0.7549\n",
            "Epoch [6/15]\t Step [901/2217]\t Loss: 0.3823\n",
            "Epoch [6/15]\t Step [951/2217]\t Loss: 0.5455\n",
            "Epoch [6/15]\t Step [1001/2217]\t Loss: 0.6738\n",
            "Epoch [6/15]\t Step [1051/2217]\t Loss: 0.6979\n",
            "Epoch [6/15]\t Step [1101/2217]\t Loss: 0.7786\n",
            "Epoch [6/15]\t Step [1151/2217]\t Loss: 0.8353\n",
            "Epoch [6/15]\t Step [1201/2217]\t Loss: 0.4727\n",
            "Epoch [6/15]\t Step [1251/2217]\t Loss: 0.6899\n",
            "Epoch [6/15]\t Step [1301/2217]\t Loss: 0.8153\n",
            "Epoch [6/15]\t Step [1351/2217]\t Loss: 1.2098\n",
            "Epoch [6/15]\t Step [1401/2217]\t Loss: 0.6351\n",
            "Epoch [6/15]\t Step [1451/2217]\t Loss: 0.4713\n",
            "Epoch [6/15]\t Step [1501/2217]\t Loss: 0.7628\n",
            "Epoch [6/15]\t Step [1551/2217]\t Loss: 0.5897\n",
            "Epoch [6/15]\t Step [1601/2217]\t Loss: 0.9253\n",
            "Epoch [6/15]\t Step [1651/2217]\t Loss: 0.6716\n",
            "Epoch [6/15]\t Step [1701/2217]\t Loss: 0.7565\n",
            "Epoch [6/15]\t Step [1751/2217]\t Loss: 0.6277\n",
            "Epoch [6/15]\t Step [1801/2217]\t Loss: 0.2972\n",
            "Epoch [6/15]\t Step [1851/2217]\t Loss: 0.7640\n",
            "Epoch [6/15]\t Step [1901/2217]\t Loss: 0.5810\n",
            "Epoch [6/15]\t Step [1951/2217]\t Loss: 0.3677\n",
            "Epoch [6/15]\t Step [2001/2217]\t Loss: 0.8528\n",
            "Epoch [6/15]\t Step [2051/2217]\t Loss: 1.1167\n",
            "Epoch [6/15]\t Step [2101/2217]\t Loss: 0.4895\n",
            "Epoch [6/15]\t Step [2151/2217]\t Loss: 0.7813\n",
            "Epoch [6/15]\t Step [2201/2217]\t Loss: 0.8943\n",
            "Epoch [7/15]\t Step [1/2217]\t Loss: 0.8442\n",
            "Epoch [7/15]\t Step [51/2217]\t Loss: 0.6339\n",
            "Epoch [7/15]\t Step [101/2217]\t Loss: 0.6847\n",
            "Epoch [7/15]\t Step [151/2217]\t Loss: 0.7445\n",
            "Epoch [7/15]\t Step [201/2217]\t Loss: 0.6386\n",
            "Epoch [7/15]\t Step [251/2217]\t Loss: 0.5245\n",
            "Epoch [7/15]\t Step [301/2217]\t Loss: 0.6669\n",
            "Epoch [7/15]\t Step [351/2217]\t Loss: 0.6152\n",
            "Epoch [7/15]\t Step [401/2217]\t Loss: 0.6970\n",
            "Epoch [7/15]\t Step [451/2217]\t Loss: 0.8582\n",
            "Epoch [7/15]\t Step [501/2217]\t Loss: 0.9020\n",
            "Epoch [7/15]\t Step [551/2217]\t Loss: 0.6731\n",
            "Epoch [7/15]\t Step [601/2217]\t Loss: 0.5306\n",
            "Epoch [7/15]\t Step [651/2217]\t Loss: 0.5260\n",
            "Epoch [7/15]\t Step [701/2217]\t Loss: 0.2401\n",
            "Epoch [7/15]\t Step [751/2217]\t Loss: 0.3597\n",
            "Epoch [7/15]\t Step [801/2217]\t Loss: 0.6639\n",
            "Epoch [7/15]\t Step [851/2217]\t Loss: 0.8585\n",
            "Epoch [7/15]\t Step [901/2217]\t Loss: 0.3385\n",
            "Epoch [7/15]\t Step [951/2217]\t Loss: 1.2067\n",
            "Epoch [7/15]\t Step [1001/2217]\t Loss: 0.8986\n",
            "Epoch [7/15]\t Step [1051/2217]\t Loss: 0.7975\n",
            "Epoch [7/15]\t Step [1101/2217]\t Loss: 0.4877\n",
            "Epoch [7/15]\t Step [1151/2217]\t Loss: 0.8858\n",
            "Epoch [7/15]\t Step [1201/2217]\t Loss: 0.7270\n",
            "Epoch [7/15]\t Step [1251/2217]\t Loss: 1.0119\n",
            "Epoch [7/15]\t Step [1301/2217]\t Loss: 0.3426\n",
            "Epoch [7/15]\t Step [1351/2217]\t Loss: 0.5031\n",
            "Epoch [7/15]\t Step [1401/2217]\t Loss: 0.4064\n",
            "Epoch [7/15]\t Step [1451/2217]\t Loss: 0.7892\n",
            "Epoch [7/15]\t Step [1501/2217]\t Loss: 0.8994\n",
            "Epoch [7/15]\t Step [1551/2217]\t Loss: 0.6670\n",
            "Epoch [7/15]\t Step [1601/2217]\t Loss: 0.7597\n",
            "Epoch [7/15]\t Step [1651/2217]\t Loss: 0.6641\n",
            "Epoch [7/15]\t Step [1701/2217]\t Loss: 0.7162\n",
            "Epoch [7/15]\t Step [1751/2217]\t Loss: 1.0202\n",
            "Epoch [7/15]\t Step [1801/2217]\t Loss: 0.7406\n",
            "Epoch [7/15]\t Step [1851/2217]\t Loss: 0.6125\n",
            "Epoch [7/15]\t Step [1901/2217]\t Loss: 0.8771\n",
            "Epoch [7/15]\t Step [1951/2217]\t Loss: 0.6708\n",
            "Epoch [7/15]\t Step [2001/2217]\t Loss: 0.6956\n",
            "Epoch [7/15]\t Step [2051/2217]\t Loss: 0.7792\n",
            "Epoch [7/15]\t Step [2101/2217]\t Loss: 0.5227\n",
            "Epoch [7/15]\t Step [2151/2217]\t Loss: 0.8687\n",
            "Epoch [7/15]\t Step [2201/2217]\t Loss: 0.8189\n",
            "Epoch: 7,\t Validation Loss: 0.6539,\t Accuracy: 0.7371\n",
            "Epoch [8/15]\t Step [1/2217]\t Loss: 0.6905\n",
            "Epoch [8/15]\t Step [51/2217]\t Loss: 0.6600\n",
            "Epoch [8/15]\t Step [101/2217]\t Loss: 0.7538\n",
            "Epoch [8/15]\t Step [151/2217]\t Loss: 0.5283\n",
            "Epoch [8/15]\t Step [201/2217]\t Loss: 0.7414\n",
            "Epoch [8/15]\t Step [251/2217]\t Loss: 0.9144\n",
            "Epoch [8/15]\t Step [301/2217]\t Loss: 0.7540\n",
            "Epoch [8/15]\t Step [351/2217]\t Loss: 0.8320\n",
            "Epoch [8/15]\t Step [401/2217]\t Loss: 0.5742\n",
            "Epoch [8/15]\t Step [451/2217]\t Loss: 0.3899\n",
            "Epoch [8/15]\t Step [501/2217]\t Loss: 0.7581\n",
            "Epoch [8/15]\t Step [551/2217]\t Loss: 0.5109\n",
            "Epoch [8/15]\t Step [601/2217]\t Loss: 0.7279\n",
            "Epoch [8/15]\t Step [651/2217]\t Loss: 0.6804\n",
            "Epoch [8/15]\t Step [701/2217]\t Loss: 0.8138\n",
            "Epoch [8/15]\t Step [751/2217]\t Loss: 0.9493\n",
            "Epoch [8/15]\t Step [801/2217]\t Loss: 0.5721\n",
            "Epoch [8/15]\t Step [851/2217]\t Loss: 0.5711\n",
            "Epoch [8/15]\t Step [901/2217]\t Loss: 0.7335\n",
            "Epoch [8/15]\t Step [951/2217]\t Loss: 0.6582\n",
            "Epoch [8/15]\t Step [1001/2217]\t Loss: 0.8313\n",
            "Epoch [8/15]\t Step [1051/2217]\t Loss: 0.8417\n",
            "Epoch [8/15]\t Step [1101/2217]\t Loss: 0.6660\n",
            "Epoch [8/15]\t Step [1151/2217]\t Loss: 0.8450\n",
            "Epoch [8/15]\t Step [1201/2217]\t Loss: 0.5226\n",
            "Epoch [8/15]\t Step [1251/2217]\t Loss: 0.9924\n",
            "Epoch [8/15]\t Step [1301/2217]\t Loss: 0.6251\n",
            "Epoch [8/15]\t Step [1351/2217]\t Loss: 0.4896\n",
            "Epoch [8/15]\t Step [1401/2217]\t Loss: 1.0541\n",
            "Epoch [8/15]\t Step [1451/2217]\t Loss: 0.7424\n",
            "Epoch [8/15]\t Step [1501/2217]\t Loss: 0.5508\n",
            "Epoch [8/15]\t Step [1551/2217]\t Loss: 0.3676\n",
            "Epoch [8/15]\t Step [1601/2217]\t Loss: 0.6138\n",
            "Epoch [8/15]\t Step [1651/2217]\t Loss: 0.5160\n",
            "Epoch [8/15]\t Step [1701/2217]\t Loss: 0.5332\n",
            "Epoch [8/15]\t Step [1751/2217]\t Loss: 0.6988\n",
            "Epoch [8/15]\t Step [1801/2217]\t Loss: 0.8043\n",
            "Epoch [8/15]\t Step [1851/2217]\t Loss: 0.6433\n",
            "Epoch [8/15]\t Step [1901/2217]\t Loss: 0.6642\n",
            "Epoch [8/15]\t Step [1951/2217]\t Loss: 0.7294\n",
            "Epoch [8/15]\t Step [2001/2217]\t Loss: 0.9380\n",
            "Epoch [8/15]\t Step [2051/2217]\t Loss: 0.9435\n",
            "Epoch [8/15]\t Step [2101/2217]\t Loss: 0.8641\n",
            "Epoch [8/15]\t Step [2151/2217]\t Loss: 0.7199\n",
            "Epoch [8/15]\t Step [2201/2217]\t Loss: 0.6403\n",
            "Epoch [9/15]\t Step [1/2217]\t Loss: 0.7813\n",
            "Epoch [9/15]\t Step [51/2217]\t Loss: 0.8473\n",
            "Epoch [9/15]\t Step [101/2217]\t Loss: 0.6309\n",
            "Epoch [9/15]\t Step [151/2217]\t Loss: 0.8057\n",
            "Epoch [9/15]\t Step [201/2217]\t Loss: 0.7626\n",
            "Epoch [9/15]\t Step [251/2217]\t Loss: 0.6069\n",
            "Epoch [9/15]\t Step [301/2217]\t Loss: 0.4725\n",
            "Epoch [9/15]\t Step [351/2217]\t Loss: 0.4740\n",
            "Epoch [9/15]\t Step [401/2217]\t Loss: 0.8824\n",
            "Epoch [9/15]\t Step [451/2217]\t Loss: 0.5951\n",
            "Epoch [9/15]\t Step [501/2217]\t Loss: 0.7506\n",
            "Epoch [9/15]\t Step [551/2217]\t Loss: 0.3373\n",
            "Epoch [9/15]\t Step [601/2217]\t Loss: 0.5681\n",
            "Epoch [9/15]\t Step [651/2217]\t Loss: 0.7040\n",
            "Epoch [9/15]\t Step [701/2217]\t Loss: 0.6850\n",
            "Epoch [9/15]\t Step [751/2217]\t Loss: 0.4526\n",
            "Epoch [9/15]\t Step [801/2217]\t Loss: 0.8348\n",
            "Epoch [9/15]\t Step [851/2217]\t Loss: 0.6939\n",
            "Epoch [9/15]\t Step [901/2217]\t Loss: 0.5245\n",
            "Epoch [9/15]\t Step [951/2217]\t Loss: 0.9607\n",
            "Epoch [9/15]\t Step [1001/2217]\t Loss: 0.8346\n",
            "Epoch [9/15]\t Step [1051/2217]\t Loss: 0.6714\n",
            "Epoch [9/15]\t Step [1101/2217]\t Loss: 0.6859\n",
            "Epoch [9/15]\t Step [1151/2217]\t Loss: 0.7280\n",
            "Epoch [9/15]\t Step [1201/2217]\t Loss: 0.4082\n",
            "Epoch [9/15]\t Step [1251/2217]\t Loss: 0.4911\n",
            "Epoch [9/15]\t Step [1301/2217]\t Loss: 0.9053\n",
            "Epoch [9/15]\t Step [1351/2217]\t Loss: 0.8814\n",
            "Epoch [9/15]\t Step [1401/2217]\t Loss: 0.7341\n",
            "Epoch [9/15]\t Step [1451/2217]\t Loss: 1.2603\n",
            "Epoch [9/15]\t Step [1501/2217]\t Loss: 0.7333\n",
            "Epoch [9/15]\t Step [1551/2217]\t Loss: 0.9265\n",
            "Epoch [9/15]\t Step [1601/2217]\t Loss: 0.7244\n",
            "Epoch [9/15]\t Step [1651/2217]\t Loss: 0.7060\n",
            "Epoch [9/15]\t Step [1701/2217]\t Loss: 0.8352\n",
            "Epoch [9/15]\t Step [1751/2217]\t Loss: 0.5353\n",
            "Epoch [9/15]\t Step [1801/2217]\t Loss: 0.6279\n",
            "Epoch [9/15]\t Step [1851/2217]\t Loss: 0.6359\n",
            "Epoch [9/15]\t Step [1901/2217]\t Loss: 0.5106\n",
            "Epoch [9/15]\t Step [1951/2217]\t Loss: 0.7881\n",
            "Epoch [9/15]\t Step [2001/2217]\t Loss: 0.5511\n",
            "Epoch [9/15]\t Step [2051/2217]\t Loss: 0.6467\n",
            "Epoch [9/15]\t Step [2101/2217]\t Loss: 0.6400\n",
            "Epoch [9/15]\t Step [2151/2217]\t Loss: 0.7069\n",
            "Epoch [9/15]\t Step [2201/2217]\t Loss: 0.8455\n",
            "Epoch: 9,\t Validation Loss: 0.6285,\t Accuracy: 0.7437\n",
            "Epoch [10/15]\t Step [1/2217]\t Loss: 0.4568\n",
            "Epoch [10/15]\t Step [51/2217]\t Loss: 1.0453\n",
            "Epoch [10/15]\t Step [101/2217]\t Loss: 0.6115\n",
            "Epoch [10/15]\t Step [151/2217]\t Loss: 0.5984\n",
            "Epoch [10/15]\t Step [201/2217]\t Loss: 0.6380\n",
            "Epoch [10/15]\t Step [251/2217]\t Loss: 0.5515\n",
            "Epoch [10/15]\t Step [301/2217]\t Loss: 0.7411\n",
            "Epoch [10/15]\t Step [351/2217]\t Loss: 1.2193\n",
            "Epoch [10/15]\t Step [401/2217]\t Loss: 0.4896\n",
            "Epoch [10/15]\t Step [451/2217]\t Loss: 0.7596\n",
            "Epoch [10/15]\t Step [501/2217]\t Loss: 0.3166\n",
            "Epoch [10/15]\t Step [551/2217]\t Loss: 0.5350\n",
            "Epoch [10/15]\t Step [601/2217]\t Loss: 0.5101\n",
            "Epoch [10/15]\t Step [651/2217]\t Loss: 0.6457\n",
            "Epoch [10/15]\t Step [701/2217]\t Loss: 0.6485\n",
            "Epoch [10/15]\t Step [751/2217]\t Loss: 0.7366\n",
            "Epoch [10/15]\t Step [801/2217]\t Loss: 0.8327\n",
            "Epoch [10/15]\t Step [851/2217]\t Loss: 0.6636\n",
            "Epoch [10/15]\t Step [901/2217]\t Loss: 0.6433\n",
            "Epoch [10/15]\t Step [951/2217]\t Loss: 0.9208\n",
            "Epoch [10/15]\t Step [1001/2217]\t Loss: 0.7822\n",
            "Epoch [10/15]\t Step [1051/2217]\t Loss: 0.6763\n",
            "Epoch [10/15]\t Step [1101/2217]\t Loss: 0.9286\n",
            "Epoch [10/15]\t Step [1151/2217]\t Loss: 0.7561\n",
            "Epoch [10/15]\t Step [1201/2217]\t Loss: 0.4817\n",
            "Epoch [10/15]\t Step [1251/2217]\t Loss: 0.5336\n",
            "Epoch [10/15]\t Step [1301/2217]\t Loss: 0.6751\n",
            "Epoch [10/15]\t Step [1351/2217]\t Loss: 0.4877\n",
            "Epoch [10/15]\t Step [1401/2217]\t Loss: 0.6856\n",
            "Epoch [10/15]\t Step [1451/2217]\t Loss: 0.6103\n",
            "Epoch [10/15]\t Step [1501/2217]\t Loss: 0.4145\n",
            "Epoch [10/15]\t Step [1551/2217]\t Loss: 0.8532\n",
            "Epoch [10/15]\t Step [1601/2217]\t Loss: 0.8521\n",
            "Epoch [10/15]\t Step [1651/2217]\t Loss: 0.7012\n",
            "Epoch [10/15]\t Step [1701/2217]\t Loss: 0.6278\n",
            "Epoch [10/15]\t Step [1751/2217]\t Loss: 0.7978\n",
            "Epoch [10/15]\t Step [1801/2217]\t Loss: 0.6674\n",
            "Epoch [10/15]\t Step [1851/2217]\t Loss: 0.4207\n",
            "Epoch [10/15]\t Step [1901/2217]\t Loss: 0.8784\n",
            "Epoch [10/15]\t Step [1951/2217]\t Loss: 0.8771\n",
            "Epoch [10/15]\t Step [2001/2217]\t Loss: 0.4562\n",
            "Epoch [10/15]\t Step [2051/2217]\t Loss: 0.6469\n",
            "Epoch [10/15]\t Step [2101/2217]\t Loss: 0.9045\n",
            "Epoch [10/15]\t Step [2151/2217]\t Loss: 0.6962\n",
            "Epoch [10/15]\t Step [2201/2217]\t Loss: 0.3703\n",
            "Epoch [11/15]\t Step [1/2217]\t Loss: 0.7030\n",
            "Epoch [11/15]\t Step [51/2217]\t Loss: 0.7954\n",
            "Epoch [11/15]\t Step [101/2217]\t Loss: 0.8172\n",
            "Epoch [11/15]\t Step [151/2217]\t Loss: 0.4867\n",
            "Epoch [11/15]\t Step [201/2217]\t Loss: 0.7566\n",
            "Epoch [11/15]\t Step [251/2217]\t Loss: 0.7290\n",
            "Epoch [11/15]\t Step [301/2217]\t Loss: 0.5345\n",
            "Epoch [11/15]\t Step [351/2217]\t Loss: 0.6858\n",
            "Epoch [11/15]\t Step [401/2217]\t Loss: 0.7423\n",
            "Epoch [11/15]\t Step [451/2217]\t Loss: 0.3652\n",
            "Epoch [11/15]\t Step [501/2217]\t Loss: 0.5517\n",
            "Epoch [11/15]\t Step [551/2217]\t Loss: 0.8283\n",
            "Epoch [11/15]\t Step [601/2217]\t Loss: 0.7804\n",
            "Epoch [11/15]\t Step [651/2217]\t Loss: 0.7198\n",
            "Epoch [11/15]\t Step [701/2217]\t Loss: 0.5573\n",
            "Epoch [11/15]\t Step [751/2217]\t Loss: 0.5257\n",
            "Epoch [11/15]\t Step [801/2217]\t Loss: 0.8959\n",
            "Epoch [11/15]\t Step [851/2217]\t Loss: 0.7784\n",
            "Epoch [11/15]\t Step [901/2217]\t Loss: 0.6547\n",
            "Epoch [11/15]\t Step [951/2217]\t Loss: 0.8839\n",
            "Epoch [11/15]\t Step [1001/2217]\t Loss: 0.7165\n",
            "Epoch [11/15]\t Step [1051/2217]\t Loss: 0.6307\n",
            "Epoch [11/15]\t Step [1101/2217]\t Loss: 0.8390\n",
            "Epoch [11/15]\t Step [1151/2217]\t Loss: 0.7412\n",
            "Epoch [11/15]\t Step [1201/2217]\t Loss: 0.6527\n",
            "Epoch [11/15]\t Step [1251/2217]\t Loss: 0.6662\n",
            "Epoch [11/15]\t Step [1301/2217]\t Loss: 0.9182\n",
            "Epoch [11/15]\t Step [1351/2217]\t Loss: 0.4924\n",
            "Epoch [11/15]\t Step [1401/2217]\t Loss: 0.5261\n",
            "Epoch [11/15]\t Step [1451/2217]\t Loss: 0.9169\n",
            "Epoch [11/15]\t Step [1501/2217]\t Loss: 0.4434\n",
            "Epoch [11/15]\t Step [1551/2217]\t Loss: 0.4654\n",
            "Epoch [11/15]\t Step [1601/2217]\t Loss: 0.4161\n",
            "Epoch [11/15]\t Step [1651/2217]\t Loss: 0.5287\n",
            "Epoch [11/15]\t Step [1701/2217]\t Loss: 0.5975\n",
            "Epoch [11/15]\t Step [1751/2217]\t Loss: 0.7605\n",
            "Epoch [11/15]\t Step [1801/2217]\t Loss: 1.0002\n",
            "Epoch [11/15]\t Step [1851/2217]\t Loss: 0.3277\n",
            "Epoch [11/15]\t Step [1901/2217]\t Loss: 0.8131\n",
            "Epoch [11/15]\t Step [1951/2217]\t Loss: 0.6708\n",
            "Epoch [11/15]\t Step [2001/2217]\t Loss: 0.5582\n",
            "Epoch [11/15]\t Step [2051/2217]\t Loss: 0.4825\n",
            "Epoch [11/15]\t Step [2101/2217]\t Loss: 0.5587\n",
            "Epoch [11/15]\t Step [2151/2217]\t Loss: 0.4178\n",
            "Epoch [11/15]\t Step [2201/2217]\t Loss: 0.6017\n",
            "Epoch: 11,\t Validation Loss: 0.6227,\t Accuracy: 0.7555\n",
            "Epoch [12/15]\t Step [1/2217]\t Loss: 0.6158\n",
            "Epoch [12/15]\t Step [51/2217]\t Loss: 0.5576\n",
            "Epoch [12/15]\t Step [101/2217]\t Loss: 0.6910\n",
            "Epoch [12/15]\t Step [151/2217]\t Loss: 0.4730\n",
            "Epoch [12/15]\t Step [201/2217]\t Loss: 0.5874\n",
            "Epoch [12/15]\t Step [251/2217]\t Loss: 0.5416\n",
            "Epoch [12/15]\t Step [301/2217]\t Loss: 0.7083\n",
            "Epoch [12/15]\t Step [351/2217]\t Loss: 0.6370\n",
            "Epoch [12/15]\t Step [401/2217]\t Loss: 1.1564\n",
            "Epoch [12/15]\t Step [451/2217]\t Loss: 0.6439\n",
            "Epoch [12/15]\t Step [501/2217]\t Loss: 0.5427\n",
            "Epoch [12/15]\t Step [551/2217]\t Loss: 0.5508\n",
            "Epoch [12/15]\t Step [601/2217]\t Loss: 0.5495\n",
            "Epoch [12/15]\t Step [651/2217]\t Loss: 0.7240\n",
            "Epoch [12/15]\t Step [701/2217]\t Loss: 0.6830\n",
            "Epoch [12/15]\t Step [751/2217]\t Loss: 0.5416\n",
            "Epoch [12/15]\t Step [801/2217]\t Loss: 0.7488\n",
            "Epoch [12/15]\t Step [851/2217]\t Loss: 0.5440\n",
            "Epoch [12/15]\t Step [901/2217]\t Loss: 0.7785\n",
            "Epoch [12/15]\t Step [951/2217]\t Loss: 0.5763\n",
            "Epoch [12/15]\t Step [1001/2217]\t Loss: 0.6221\n",
            "Epoch [12/15]\t Step [1051/2217]\t Loss: 0.5359\n",
            "Epoch [12/15]\t Step [1101/2217]\t Loss: 0.6583\n",
            "Epoch [12/15]\t Step [1151/2217]\t Loss: 0.7404\n",
            "Epoch [12/15]\t Step [1201/2217]\t Loss: 0.7411\n",
            "Epoch [12/15]\t Step [1251/2217]\t Loss: 0.8234\n",
            "Epoch [12/15]\t Step [1301/2217]\t Loss: 0.8255\n",
            "Epoch [12/15]\t Step [1351/2217]\t Loss: 0.4355\n",
            "Epoch [12/15]\t Step [1401/2217]\t Loss: 0.7824\n",
            "Epoch [12/15]\t Step [1451/2217]\t Loss: 0.6202\n",
            "Epoch [12/15]\t Step [1501/2217]\t Loss: 0.6439\n",
            "Epoch [12/15]\t Step [1551/2217]\t Loss: 0.5856\n",
            "Epoch [12/15]\t Step [1601/2217]\t Loss: 0.7501\n",
            "Epoch [12/15]\t Step [1651/2217]\t Loss: 0.4367\n",
            "Epoch [12/15]\t Step [1701/2217]\t Loss: 0.5985\n",
            "Epoch [12/15]\t Step [1751/2217]\t Loss: 0.4908\n",
            "Epoch [12/15]\t Step [1801/2217]\t Loss: 0.7668\n",
            "Epoch [12/15]\t Step [1851/2217]\t Loss: 0.6618\n",
            "Epoch [12/15]\t Step [1901/2217]\t Loss: 0.6019\n",
            "Epoch [12/15]\t Step [1951/2217]\t Loss: 0.6652\n",
            "Epoch [12/15]\t Step [2001/2217]\t Loss: 0.7149\n",
            "Epoch [12/15]\t Step [2051/2217]\t Loss: 0.6083\n",
            "Epoch [12/15]\t Step [2101/2217]\t Loss: 0.8627\n",
            "Epoch [12/15]\t Step [2151/2217]\t Loss: 0.6388\n",
            "Epoch [12/15]\t Step [2201/2217]\t Loss: 0.7898\n",
            "Epoch [13/15]\t Step [1/2217]\t Loss: 0.4636\n",
            "Epoch [13/15]\t Step [51/2217]\t Loss: 0.7012\n",
            "Epoch [13/15]\t Step [101/2217]\t Loss: 0.5095\n",
            "Epoch [13/15]\t Step [151/2217]\t Loss: 0.7901\n",
            "Epoch [13/15]\t Step [201/2217]\t Loss: 0.5670\n",
            "Epoch [13/15]\t Step [251/2217]\t Loss: 0.8933\n",
            "Epoch [13/15]\t Step [301/2217]\t Loss: 0.6937\n",
            "Epoch [13/15]\t Step [351/2217]\t Loss: 0.6625\n",
            "Epoch [13/15]\t Step [401/2217]\t Loss: 0.5590\n",
            "Epoch [13/15]\t Step [451/2217]\t Loss: 0.7659\n",
            "Epoch [13/15]\t Step [501/2217]\t Loss: 0.5699\n",
            "Epoch [13/15]\t Step [551/2217]\t Loss: 0.6478\n",
            "Epoch [13/15]\t Step [601/2217]\t Loss: 0.6184\n",
            "Epoch [13/15]\t Step [651/2217]\t Loss: 0.4827\n",
            "Epoch [13/15]\t Step [701/2217]\t Loss: 0.9374\n",
            "Epoch [13/15]\t Step [751/2217]\t Loss: 0.8185\n",
            "Epoch [13/15]\t Step [801/2217]\t Loss: 1.0399\n",
            "Epoch [13/15]\t Step [851/2217]\t Loss: 0.5390\n",
            "Epoch [13/15]\t Step [901/2217]\t Loss: 0.7781\n",
            "Epoch [13/15]\t Step [951/2217]\t Loss: 0.4991\n",
            "Epoch [13/15]\t Step [1001/2217]\t Loss: 1.1878\n",
            "Epoch [13/15]\t Step [1051/2217]\t Loss: 0.5976\n",
            "Epoch [13/15]\t Step [1101/2217]\t Loss: 0.6672\n",
            "Epoch [13/15]\t Step [1151/2217]\t Loss: 0.5638\n",
            "Epoch [13/15]\t Step [1201/2217]\t Loss: 0.4245\n",
            "Epoch [13/15]\t Step [1251/2217]\t Loss: 0.5899\n",
            "Epoch [13/15]\t Step [1301/2217]\t Loss: 0.6512\n",
            "Epoch [13/15]\t Step [1351/2217]\t Loss: 0.5860\n",
            "Epoch [13/15]\t Step [1401/2217]\t Loss: 0.9265\n",
            "Epoch [13/15]\t Step [1451/2217]\t Loss: 0.5361\n",
            "Epoch [13/15]\t Step [1501/2217]\t Loss: 0.5141\n",
            "Epoch [13/15]\t Step [1551/2217]\t Loss: 0.7436\n",
            "Epoch [13/15]\t Step [1601/2217]\t Loss: 0.4999\n",
            "Epoch [13/15]\t Step [1651/2217]\t Loss: 0.5755\n",
            "Epoch [13/15]\t Step [1701/2217]\t Loss: 0.6097\n",
            "Epoch [13/15]\t Step [1751/2217]\t Loss: 1.0668\n",
            "Epoch [13/15]\t Step [1801/2217]\t Loss: 0.8753\n",
            "Epoch [13/15]\t Step [1851/2217]\t Loss: 0.2579\n",
            "Epoch [13/15]\t Step [1901/2217]\t Loss: 0.3657\n",
            "Epoch [13/15]\t Step [1951/2217]\t Loss: 0.7230\n",
            "Epoch [13/15]\t Step [2001/2217]\t Loss: 0.3270\n",
            "Epoch [13/15]\t Step [2051/2217]\t Loss: 0.5428\n",
            "Epoch [13/15]\t Step [2101/2217]\t Loss: 0.4788\n",
            "Epoch [13/15]\t Step [2151/2217]\t Loss: 0.7457\n",
            "Epoch [13/15]\t Step [2201/2217]\t Loss: 0.7065\n",
            "Epoch: 13,\t Validation Loss: 0.6039,\t Accuracy: 0.7595\n",
            "Epoch [14/15]\t Step [1/2217]\t Loss: 0.4692\n",
            "Epoch [14/15]\t Step [51/2217]\t Loss: 0.3880\n",
            "Epoch [14/15]\t Step [101/2217]\t Loss: 0.7155\n",
            "Epoch [14/15]\t Step [151/2217]\t Loss: 0.9606\n",
            "Epoch [14/15]\t Step [201/2217]\t Loss: 0.6978\n",
            "Epoch [14/15]\t Step [251/2217]\t Loss: 0.6900\n",
            "Epoch [14/15]\t Step [301/2217]\t Loss: 0.7371\n",
            "Epoch [14/15]\t Step [351/2217]\t Loss: 0.7959\n",
            "Epoch [14/15]\t Step [401/2217]\t Loss: 0.6161\n",
            "Epoch [14/15]\t Step [451/2217]\t Loss: 0.6104\n",
            "Epoch [14/15]\t Step [501/2217]\t Loss: 0.7747\n",
            "Epoch [14/15]\t Step [551/2217]\t Loss: 0.7244\n",
            "Epoch [14/15]\t Step [601/2217]\t Loss: 0.7624\n",
            "Epoch [14/15]\t Step [651/2217]\t Loss: 0.5657\n",
            "Epoch [14/15]\t Step [701/2217]\t Loss: 0.7283\n",
            "Epoch [14/15]\t Step [751/2217]\t Loss: 0.9172\n",
            "Epoch [14/15]\t Step [801/2217]\t Loss: 0.5766\n",
            "Epoch [14/15]\t Step [851/2217]\t Loss: 0.5939\n",
            "Epoch [14/15]\t Step [901/2217]\t Loss: 0.4824\n",
            "Epoch [14/15]\t Step [951/2217]\t Loss: 0.6920\n",
            "Epoch [14/15]\t Step [1001/2217]\t Loss: 0.6282\n",
            "Epoch [14/15]\t Step [1051/2217]\t Loss: 0.6352\n",
            "Epoch [14/15]\t Step [1101/2217]\t Loss: 0.9741\n",
            "Epoch [14/15]\t Step [1151/2217]\t Loss: 0.9144\n",
            "Epoch [14/15]\t Step [1201/2217]\t Loss: 0.6774\n",
            "Epoch [14/15]\t Step [1251/2217]\t Loss: 0.7052\n",
            "Epoch [14/15]\t Step [1301/2217]\t Loss: 0.8347\n",
            "Epoch [14/15]\t Step [1351/2217]\t Loss: 0.5364\n",
            "Epoch [14/15]\t Step [1401/2217]\t Loss: 0.6565\n",
            "Epoch [14/15]\t Step [1451/2217]\t Loss: 0.9365\n",
            "Epoch [14/15]\t Step [1501/2217]\t Loss: 0.6324\n",
            "Epoch [14/15]\t Step [1551/2217]\t Loss: 0.5944\n",
            "Epoch [14/15]\t Step [1601/2217]\t Loss: 0.6277\n",
            "Epoch [14/15]\t Step [1651/2217]\t Loss: 0.8887\n",
            "Epoch [14/15]\t Step [1701/2217]\t Loss: 0.4401\n",
            "Epoch [14/15]\t Step [1751/2217]\t Loss: 0.6944\n",
            "Epoch [14/15]\t Step [1801/2217]\t Loss: 0.5764\n",
            "Epoch [14/15]\t Step [1851/2217]\t Loss: 0.6749\n",
            "Epoch [14/15]\t Step [1901/2217]\t Loss: 0.7490\n",
            "Epoch [14/15]\t Step [1951/2217]\t Loss: 0.6632\n",
            "Epoch [14/15]\t Step [2001/2217]\t Loss: 0.6588\n",
            "Epoch [14/15]\t Step [2051/2217]\t Loss: 0.7668\n",
            "Epoch [14/15]\t Step [2101/2217]\t Loss: 0.4558\n",
            "Epoch [14/15]\t Step [2151/2217]\t Loss: 0.7116\n",
            "Epoch [14/15]\t Step [2201/2217]\t Loss: 0.4521\n",
            "Epoch [15/15]\t Step [1/2217]\t Loss: 0.3504\n",
            "Epoch [15/15]\t Step [51/2217]\t Loss: 0.7288\n",
            "Epoch [15/15]\t Step [101/2217]\t Loss: 0.5271\n",
            "Epoch [15/15]\t Step [151/2217]\t Loss: 0.9235\n",
            "Epoch [15/15]\t Step [201/2217]\t Loss: 0.8397\n",
            "Epoch [15/15]\t Step [251/2217]\t Loss: 0.4253\n",
            "Epoch [15/15]\t Step [301/2217]\t Loss: 0.5414\n",
            "Epoch [15/15]\t Step [351/2217]\t Loss: 0.9317\n",
            "Epoch [15/15]\t Step [401/2217]\t Loss: 0.4308\n",
            "Epoch [15/15]\t Step [451/2217]\t Loss: 0.5247\n",
            "Epoch [15/15]\t Step [501/2217]\t Loss: 0.7362\n",
            "Epoch [15/15]\t Step [551/2217]\t Loss: 0.2038\n",
            "Epoch [15/15]\t Step [601/2217]\t Loss: 0.5807\n",
            "Epoch [15/15]\t Step [651/2217]\t Loss: 0.4735\n",
            "Epoch [15/15]\t Step [701/2217]\t Loss: 0.5039\n",
            "Epoch [15/15]\t Step [751/2217]\t Loss: 0.6251\n",
            "Epoch [15/15]\t Step [801/2217]\t Loss: 0.6389\n",
            "Epoch [15/15]\t Step [851/2217]\t Loss: 0.4688\n",
            "Epoch [15/15]\t Step [901/2217]\t Loss: 0.5354\n",
            "Epoch [15/15]\t Step [951/2217]\t Loss: 0.6831\n",
            "Epoch [15/15]\t Step [1001/2217]\t Loss: 0.6516\n",
            "Epoch [15/15]\t Step [1051/2217]\t Loss: 0.7398\n",
            "Epoch [15/15]\t Step [1101/2217]\t Loss: 0.7206\n",
            "Epoch [15/15]\t Step [1151/2217]\t Loss: 0.5186\n",
            "Epoch [15/15]\t Step [1201/2217]\t Loss: 0.8173\n",
            "Epoch [15/15]\t Step [1251/2217]\t Loss: 0.7518\n",
            "Epoch [15/15]\t Step [1301/2217]\t Loss: 0.5765\n",
            "Epoch [15/15]\t Step [1351/2217]\t Loss: 0.8150\n",
            "Epoch [15/15]\t Step [1401/2217]\t Loss: 0.5913\n",
            "Epoch [15/15]\t Step [1451/2217]\t Loss: 0.3831\n",
            "Epoch [15/15]\t Step [1501/2217]\t Loss: 0.6384\n",
            "Epoch [15/15]\t Step [1551/2217]\t Loss: 0.7529\n",
            "Epoch [15/15]\t Step [1601/2217]\t Loss: 0.5140\n",
            "Epoch [15/15]\t Step [1651/2217]\t Loss: 0.8535\n",
            "Epoch [15/15]\t Step [1701/2217]\t Loss: 0.8014\n",
            "Epoch [15/15]\t Step [1751/2217]\t Loss: 0.6430\n",
            "Epoch [15/15]\t Step [1801/2217]\t Loss: 0.7732\n",
            "Epoch [15/15]\t Step [1851/2217]\t Loss: 0.5559\n",
            "Epoch [15/15]\t Step [1901/2217]\t Loss: 0.8102\n",
            "Epoch [15/15]\t Step [1951/2217]\t Loss: 0.6324\n",
            "Epoch [15/15]\t Step [2001/2217]\t Loss: 0.4523\n",
            "Epoch [15/15]\t Step [2051/2217]\t Loss: 0.5240\n",
            "Epoch [15/15]\t Step [2101/2217]\t Loss: 0.9638\n",
            "Epoch [15/15]\t Step [2151/2217]\t Loss: 0.4941\n",
            "Epoch [15/15]\t Step [2201/2217]\t Loss: 0.5808\n",
            "Epoch: 15,\t Validation Loss: 0.5928,\t Accuracy: 0.7674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_model_drop.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_drop(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bmLqv5qp5rOJ",
        "outputId": "c1912d6a-593b-40b9-aeaf-12df2916eac3"
      },
      "id": "bmLqv5qp5rOJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6567,\t Accuracy: 0.6626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combination"
      ],
      "metadata": {
        "id": "vx78ON7Cks2-"
      },
      "id": "vx78ON7Cks2-"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "slNvrSeKqty9"
      },
      "id": "slNvrSeKqty9"
    },
    {
      "cell_type": "code",
      "source": [
        "trained_comb_mod= train_loop_L2(YourModel_drop)"
      ],
      "metadata": {
        "id": "4TQkBtumqsFv"
      },
      "id": "4TQkBtumqsFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And finally, test the model\n",
        "\n",
        "trained_comb_mod.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    hits = 0\n",
        "    losses = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_model_drop(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size()[0])\n",
        "        class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "        hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "    accuracy = hits / len(test_dataloader.dataset)\n",
        "    avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "    print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "qpVzVkBqmIxx"
      },
      "id": "qpVzVkBqmIxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0a23885a",
      "metadata": {
        "id": "0a23885a"
      },
      "source": [
        "#### Chapter 3.3 - Batch Normalization\n",
        "\n",
        "Batch Normalization is a technique to improve the training of deep neural networks by normalizing the inputs to each layer. It was introduced to address the problem of internal covariate shift, which refers to the change in the distribution of layer inputs during training as the parameters of the previous layers change.\n",
        "\n",
        "**Intuition:**\n",
        "The idea behind Batch Normalization is to normalize the inputs to each layer so that they have a mean of 0 and a standard deviation of 1. This ensures that the inputs to each layer are on a similar scale, which helps the network learn faster and more effectively. By normalizing the inputs, Batch Normalization reduces the sensitivity of the network to the initialization of weights and allows for the use of higher learning rates.\n",
        "\n",
        "**How it works:**\n",
        "1. For each mini-batch, Batch Normalization computes the mean and variance of the inputs.\n",
        "2. The inputs are then normalized using these statistics:\n",
        "    $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "    where $\\mu$ is the mean, $\\sigma^2$ is the variance, and $\\epsilon$ is a small constant added for numerical stability.\n",
        "3. To allow the network to learn the optimal scale and shift for the normalized inputs, two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), are introduced:\n",
        "    $y = \\gamma \\hat{x} + \\beta$\n",
        "\n",
        "**Problems it solves:**\n",
        "1. **Internal Covariate Shift:** By normalizing the inputs to each layer, Batch Normalization reduces the changes in the distribution of layer inputs during training, making the optimization process more stable.\n",
        "2. **Faster Training:** Normalized inputs allow for the use of higher learning rates, leading to faster convergence.\n",
        "3. **Regularization Effect:** Batch Normalization introduces some noise due to the mini-batch statistics, which acts as a form of regularization and reduces the need for other regularization techniques like Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ff313393",
      "metadata": {
        "id": "ff313393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "0abdcef7-1be6-4367-a16b-43011336fcd1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1135413189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here is how to add it into your model as a layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'num_features' is not defined"
          ]
        }
      ],
      "source": [
        "# Here is how to add it into your model as a layer: num_features muss vorher assigned werden\n",
        "\n",
        "bn = torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95af6dfc",
      "metadata": {
        "id": "95af6dfc"
      },
      "source": [
        "#### Chapter 3.4 - Modern Computer Vision Models\n",
        "\n",
        "AlexNet (original paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) is in many senses the grandfather of modern neural networks, being the first one to successfully combine multiple GPUs for training with a deep neural network. While it is no longer in use today, the lessons learned from AlexNet very much are, and multi-GPU setups and deep convolutional neural networks remain a staple of computer vision methods.\n",
        "\n",
        "Modern Computer Vision uses a number of different models, but perhaps none is as prolific as the original ResNet, in particular the ResNet-50. Even though it is far from the strongest model available today, its flexibility, modest size, and robust performance across tasks makes it a favorite, both in general computer vision and medical computer vision, where it is commonly used as the encoder in segmentation models (more on that later). The original paper (https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) has garnered almost 300'000 citations, and its descendants have dominated challenges and paper submissions in the field for a significant amount of time.\n",
        "\n",
        "**Task 2 (up to 6 points)**: Your task is to write one of these two modern models from scratch. The points are awarded for correctly implementing these models. You can choose your own difficulty here, and can earn fewer or more points, depending on which you feel more comfortable building. AlexNet requires only components that you have already seen last week - convolutions, pooling, and linear layers, while ResNet requires you to build skip connections and bottleneck blocks from scratch.\n",
        "\n",
        "Option 1 - AlexNet **(4 points)**:\n",
        "- Building the model **(2 points)**\n",
        "- You do not have to implement the parts where multiple GPUs are required\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "Option 2 - ResNet-50 **(7 points)**\n",
        "- Correctly implementing Skip Connections **(1 point)**\n",
        "- Correctly implementing Residual/Bottleneck Blocks **(3 points)**\n",
        "- Correctly building the ResNet from these Blocks **(1 point)**\n",
        "- For BatchNorm you are allowed to simply use the existing implementation\n",
        "- Add some type of data augmentation and regularization to the training script **(1 point)**\n",
        "- Do a proper evaluation of the test set, including confusion matrix and precision-recall curves **(1 point)**\n",
        "\n",
        "You must verify that your model actually trains and is capable of solving the classification task on LiTS 2017. You should be able to explain every piece of code to the tutors that grade your solution, so if you use any help in building the model (e.g. Chat-GPT, Cursor, etc.), be prepared to explain what code blocks do what, and why you implemented them in the specific way you did, and not any other. The points are awarded for programming *and* understanding!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c7122f",
      "metadata": {
        "id": "62c7122f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = tt.Compose([\n",
        "    tt.RandomHorizontalFlip(),\n",
        "    tt.RandomRotation(10),\n",
        "    tt.RandomAffine(degrees=15),\n",
        "])\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=3, in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "\n",
        "            nn.Conv2d(64, 192, 5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "\n",
        "            nn.Conv2d(192, 384, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "\n",
        "        # compute output size dynamically\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, 224, 224)  # your real image size here\n",
        "            feat = self.features(dummy)\n",
        "            dim = feat.numel()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(dim, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "FriwA4gh9WI7"
      },
      "id": "FriwA4gh9WI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_loop_alex(your_model): #with L2 and augmentation with gaussBlur and randomRotation\n",
        "    \"\"\"\n",
        "    This function runs your train loop and returns the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = your_model(in_channels = 1, out_classes = 3)\n",
        "    model = model.to(device = device)\n",
        "\n",
        "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for step, (data, targets) in enumerate(train_dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            #here the augmentation is being applied to our data\n",
        "            #stack because we have a batch and are applying it to every image in the batch\n",
        "            data= torch.stack([train_transforms(img) for img in data])\n",
        "\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # This uses the length of the current set - \"train\"\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Step [{step+1}/{len(train_dataloader.dataset)//batch_size}]\\t Loss: {loss.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "            # Validation mode on\n",
        "            model.eval()\n",
        "\n",
        "            # Don't track gradients for validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                hits = 0\n",
        "                losses = []\n",
        "                batch_sizes = []\n",
        "\n",
        "                for step, (data, targets) in enumerate(val_dataloader):\n",
        "\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    predictions = model(data)\n",
        "                    loss = loss_criterion(predictions, targets)\n",
        "                    losses.append(loss.item())\n",
        "                    batch_sizes.append(data.size()[0])\n",
        "\n",
        "                    class_predictions = torch.argmax(predictions, dim = 1).flatten()\n",
        "                    hits = hits + sum([1 if cp == t else 0 for cp, t in zip(class_predictions, targets)])\n",
        "\n",
        "                accuracy = hits / len(val_dataloader.dataset)\n",
        "                avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "                print(f\"Epoch: {epoch+1},\\t Validation Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # After we are done validating, let's not forget to go back to storing gradients.\n",
        "            model.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_alex = train_loop_alex(AlexNet)\n"
      ],
      "metadata": {
        "id": "twvLnRwAnZFU"
      },
      "id": "twvLnRwAnZFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, PrecisionRecallDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "trained_alex.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "hits = 0\n",
        "losses = []\n",
        "batch_sizes = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, (data, targets) in enumerate(test_dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        predictions = trained_alex(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "        losses.append(loss.item())\n",
        "        batch_sizes.append(data.size(0))\n",
        "\n",
        "        class_predictions = torch.argmax(predictions, dim=1).flatten()\n",
        "        hits += (class_predictions == targets).sum().item()\n",
        "\n",
        "        all_preds.extend(class_predictions.cpu().tolist())\n",
        "        all_targets.extend(targets.cpu().tolist())\n",
        "\n",
        "# Compute overall metrics\n",
        "accuracy = hits / len(test_dataloader.dataset)\n",
        "avg_loss = sum([l * bs for l, bs in zip(losses, batch_sizes)]) / sum(batch_sizes)\n",
        "\n",
        "print(f\"Test Loss: {avg_loss:.4f},\\t Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall curves (for multi-class)\n",
        "# Convert targets to one-hot\n",
        "num_classes = max(all_targets) + 1\n",
        "y_true = torch.nn.functional.one_hot(torch.tensor(all_targets), num_classes=num_classes).numpy()\n",
        "y_scores = torch.nn.functional.softmax(torch.tensor(predictions), dim=1).cpu().numpy()  # last batch for demonstration\n",
        "\n",
        "# Plot precision-recall for each class\n",
        "for i in range(num_classes):\n",
        "    precision, recall, _ = precision_recall_curve(y_true[:, i], y_scores[:, i])\n",
        "    disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "    disp.plot()\n",
        "    plt.title(f'Precision-Recall curve for class {i}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "9_YQ1X6IszxQ"
      },
      "id": "9_YQ1X6IszxQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
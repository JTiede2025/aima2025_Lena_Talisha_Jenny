{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9401b9c2",
      "metadata": {
        "id": "9401b9c2"
      },
      "source": [
        "### Chapter 2 - Machine Learning in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49feb29",
      "metadata": {
        "id": "e49feb29"
      },
      "source": [
        "**This week's exercise has 4 tasks for a total of 6 points (+2 bonus points). Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "\n",
        "In this chapter, we want you to become proficient at the following tasks:\n",
        "- Building a training, validation, and testing script using PyTorch\n",
        "- Identifying, and being able to build all the typical components of PyTorch model training\n",
        "- Avoiding, identifying, and fixing typical mistakes in PyTorch model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4a4347",
      "metadata": {
        "id": "fe4a4347"
      },
      "source": [
        "#### Chapter 2.5 - PyTorch Modules & Models\n",
        "\n",
        "Now comes the center piece of training a neural network - the actual neural network.\n",
        "\n",
        "All we have seen of the neural network so far, is that we stuff a tensor with shape $Batch size * Channels * Height * Width$ into the input, and get a tensor $Batch size * Something$ back out. In our previous example, this something was the number of flower classes. During the course, you have probably been shown what a typical network consists of (things like Convolutional Layers, Fully Connected Layers, Non-Linear Activation Functions, Batch Normalization, etc.), and what these things do, mathematically speaking.\n",
        "\n",
        "But how do programmers build them? As almost always, the answer is *classes*. Any neural network in PyTorch is a class, which does two things:\n",
        "- It inherits from a PyTorch class named *torch.nn.Module*.\n",
        "- It implements an \\_\\_init\\_\\_ and a forward method.\n",
        "\n",
        "The inheritance guarantees that any neural network (and often even its components), follow a common structure. They also guarantee that a lot of useful helper functions, e.g. *model.parameters()*, which lists all weights in a model, are always available to us when we build our own components.\n",
        "\n",
        "The forward function is implicitly called whenever we stuff something into our model and tell it to make a prediction for us. So, writing `model(data)` or `model.forward(data)` is the same (with very few exceptions). The corresponding backward path, which computes the gradients for each parameter and lets us learn, is mercifully something that PyTorch does *automatically*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7o9XKmsREltF",
      "metadata": {
        "id": "7o9XKmsREltF"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as ttf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5c164d8b",
      "metadata": {
        "id": "5c164d8b",
        "outputId": "088fb021-0a6c-4eb2-bb82-6733343a840e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# It's time we built a model ourselves.\n",
        "# Let's start with a model that does *nothing*.\n",
        "\n",
        "# We start with a class that inherits from torch.nn.Module.\n",
        "# We want this to have access to a lot of the useful\n",
        "# automation features which PyTorch offers.\n",
        "class Example_Model(torch.nn.Module):\n",
        "\n",
        "    # We want an init method, and at the very least we want\n",
        "    # to call the init function of our parent class aswell.\n",
        "    def __init__(self):\n",
        "        super(Example_Model, self).__init__()\n",
        "\n",
        "    # We also want a forward method.\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "# We make an instance of our model, ...\n",
        "my_first_model = Example_Model()\n",
        "\n",
        "# ... quickly grab an example \"image\" to feed in, ...\n",
        "random_noise = torch.rand((16, 1, 256, 256))\n",
        "\n",
        "# ... and tell our model to make a forward pass.\n",
        "# Of course, this prediction is currently bogus.\n",
        "prediction = my_first_model(random_noise)\n",
        "\n",
        "print(prediction.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6977a8e2",
      "metadata": {
        "id": "6977a8e2",
        "outputId": "5d4fa1e3-0cca-49b0-ffa1-6b1a5105087e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3])\n"
          ]
        }
      ],
      "source": [
        "# And that's already it!\n",
        "\n",
        "# ... or is it?\n",
        "# So far, our model has not done anything, of course, it just spat\n",
        "# our input tensor back out. Let's give it some learnable parameters.\n",
        "\n",
        "class Example_Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Example_Model, self).__init__()\n",
        "\n",
        "        # This time, we specify some components.\n",
        "        # You could write components yourself, but in practice,\n",
        "        # PyTorch has pretty much all you will ever need.\n",
        "\n",
        "        # Linear is the name of the fully connected layer in PyTorch,\n",
        "        # the one you know from MLPs already.\n",
        "        # The number of in-going signals will be as many as we have\n",
        "        # pixels in our toy model.\n",
        "        # The number of out-going features is up to our choosing.\n",
        "        self.fc1 = torch.nn.Linear(in_features = 1*(256**2), out_features = 3)\n",
        "\n",
        "        # We will learn more about these components in a little bit.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To turn the image from 2D into \"1D\" for the fully connected\n",
        "        # layer, we use the flatten() method.\n",
        "        # Remember that when we flatten our tensor, we only flatten the image,\n",
        "        # not the batch dimension, or else we start mixing images together!\n",
        "        x = x.flatten(start_dim = 1)\n",
        "        # This time, we actually use our predefined layer in our\n",
        "        # forward pass.\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "my_second_model = Example_Model()\n",
        "random_noise = torch.rand((16, 1, 256, 256))\n",
        "prediction = my_second_model(random_noise)\n",
        "print(prediction.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "980065fe",
      "metadata": {
        "id": "980065fe",
        "outputId": "3d77d2a5-a770-4297-d56f-7054b25d3259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example_Model(\n",
            "  (fc1): Linear(in_features=65536, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# You can also look at what building blocks your model is composed of:\n",
        "print(my_second_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "E6Ms7bhmIYTP",
      "metadata": {
        "id": "E6Ms7bhmIYTP",
        "outputId": "0ad95f26-f34b-4edb-a028-ed891893f363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1.weight: Parameter containing:\n",
            "tensor([[ 7.9811e-05, -8.8253e-04,  1.5795e-04,  ..., -1.3640e-03,\n",
            "          1.6280e-03,  2.1913e-05],\n",
            "        [-3.6933e-03,  1.8160e-04, -3.1656e-03,  ..., -2.4369e-03,\n",
            "         -6.0565e-04, -2.4907e-03],\n",
            "        [ 3.0679e-03, -2.3817e-03,  5.6737e-04,  ..., -4.5620e-04,\n",
            "         -3.8274e-03,  2.4772e-03]], requires_grad=True)\n",
            "fc1.bias: Parameter containing:\n",
            "tensor([-0.0023,  0.0007, -0.0033], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# and what your parameters currently are:\n",
        "for name, param in my_second_model.named_parameters():\n",
        "    print(f\"{name}: {param}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c6f44f",
      "metadata": {
        "id": "07c6f44f"
      },
      "source": [
        "For those who want to dive deeper:\n",
        "\n",
        "When we wrote `self.fc1 = nn.Linear(...)`, we added a module to our model (duh), and the class can henceforth refer to this module by its name (e.g. `x = self.fc1(x)` works). The model also knows to collect gradients for and to update the parameters of these modules. If you want to build sophisticated models, sometimes you will not handcraft everything, but rather make a couple of building blocks, and want to add a variable number of them to your model. For such cases, there is a function named `self.add_module(name, child_module)`. In essence, the following two things are equivalent:\n",
        "- `self.add_module(\"fc1\", nn.Linear(...))`\n",
        "- `self.fc1 = nn.Linear(...)`\n",
        "\n",
        "However, sometimes it is very convenient being able to fill in the left side of the equation dynamically."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c43eb12",
      "metadata": {
        "id": "4c43eb12"
      },
      "source": [
        "#### Chapter 2.6 - Model Lego\n",
        "\n",
        "In the preceding weeks of the seminar, you have probably learnt about some of the concepts behind neural networks. If not, or if you do not remember, here is a little crash course on what building blocks we have, how they work, and how we can easily conceptualize what they do.\n",
        "\n",
        "There are far more things that PyTorch can do, and it would be quite impossible to showcase all of them here. Others, like BatchNorm, we will learn about as we implement architectures from the more recent neural network milestone papers. If you come across something you haven't heard before in a paper, or have an idea for something you'd like to try, there is a good chance that you can find this operation already fully implemented in PyTorch - try googling it! You will find that the PyTorch documentation is quite good, and that you can discover some very useful features this way."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fc34b1",
      "metadata": {
        "id": "a3fc34b1"
      },
      "source": [
        "**Linear layers** - The simplest and oldest component of a neural network is the Linear layer, often called a Fully Connected layer. A fully connected layer is defined by a number of in-going and out-going connections. The number of in-going connections, naturally, is the same as the number of out-going connections from the layer before. The number of out-going connections can be is chosen so that every node in our layer is connected to every node in the following layer.\n",
        "\n",
        "The Linear layer is called linear because each out-going signal equals the sum of all products of a node's weight and in-going signal, plus a bias term per node:\n",
        "$y_i = \\sum_{j} w_{ij}* x_j + b_i$\n",
        "\n",
        "This makes it, mathematically speaking, a linear transformation plus a translation (also called an affine transformation):\n",
        "$y = x*A^T +b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0d3f8d2d",
      "metadata": {
        "id": "0d3f8d2d"
      },
      "outputs": [],
      "source": [
        "# In PyTorch, a Linear layer that takes 20 input signals and gives 10 output signals is made like this:\n",
        "\n",
        "fc1 = torch.nn.Linear(in_features = 20, out_features = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e7292ad",
      "metadata": {
        "id": "5e7292ad"
      },
      "source": [
        "**Activation functions** - Where neural networks are concerned, Linear layers are nice, but come with an inherent problem: Any combination of two affine transformations, is itself an affine transformation. Consequently, neural networks do not really gain much from multiple Linear layers that one Linear layer couldn't have done, and also generally have trouble capturing higher-order polynomial dependencies in data.\n",
        "\n",
        "However, there is a solution to this problem! A so-called activation function is commonly added after every Linear layer (and typically also after convolutions, which we will get to later), and its purpose is to introduce some form of non-linear transformation between our affine transformations. The combination `Affine + Affine` can be expressed as a single Affine, so it can encode the information of only one Affine Transformation. The combination `Affine + Non-linear function + Affine` cannot be expressed as a single Affine anymore. In this case, having two separate Affine Transformations is suddenly useful, and the issues capturing non-linear and particularly higher-order polynomial dependencies are lessened.\n",
        "\n",
        "There is a multitude of different activation functions, and historically, different ones have been used for different reasons. The most common among these activation functions are:\n",
        "- Tangens Hyperbolicus: $tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}$\n",
        "- Sigmoid: $\\sigma(x) = \\frac{1}{(1+e^{-x})}$\n",
        "- ReLU: $R(x) = \\{0 $ if $ x<0 $, else $ x\\}$\n",
        "- Swish: $S(x) = \\frac{x}{(1+e^{-ÃŸx})}$\n",
        "\n",
        "All of them have different advantages and disadvantages, although in most modern neural networks, you don't really feel either of them:\n",
        "- Sigmoid and Tanh are smooth, non-linear functions. In some cases, this affords you smoother outputs with fewer kinks. As a potential downside, the activations saturate for large absolute values (see plot). If our weights are large enough, this can create problems: Suddenly, all our activations look similar, even for very different inputs. This is typically very undesirable, and can cause models to stop learning.\n",
        "- ReLU has no continuous non-linearity, only a single point (zero) at which the function is not continuous. It is somewhat weaker in capturing non-linear dependencies as a result. However, it has really friendly and smooth gradients, which makes training with ReLU very stable and reliable. If your model ever doesn't learn, it's probably not ReLU's fault.\n",
        "- Swish is a modern activation function that was found by systematically testing random combinations or chains of functions and observing how they performed as activation functions. It looks very similar to ReLU and approaches ReLU for large absolute values, but has a smooth non-linearity around 0 instead of a kink. Generally, it has the same advantages as ReLU, and is said to have the same downsides except less pronounced. There are many other brands of \"ReLU but slightly different\" out there. If you're ever looking to squeeze out the final few drops of performance from your model, it can be worth checking them out.\n",
        "\n",
        "The most important things are this: Firstly, all activation functions have at least one non-linearity, sometimes even a big continuous one, and typically they are near zero. Secondly, most have a region where they function best. Take for example, the sigmoid, which neatly differentiates inputs near zero, but provides almost the same output for inputs that are very large or small. Typically this region is also near zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "32da6cb2",
      "metadata": {
        "id": "32da6cb2",
        "outputId": "5276771f-58fc-45ce-f423-2072b5ddbe43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Here are some plots for the various functions:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the functions\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def swish(x):\n",
        "    return x * sigmoid(x) # ÃŸ = 1\n",
        "\n",
        "# Create an array of x values\n",
        "x = np.linspace(-5, 5, 400)\n",
        "\n",
        "# Calculate y values for each function\n",
        "y_tanh = tanh(x)\n",
        "y_sigmoid = sigmoid(x)\n",
        "y_relu = relu(x)\n",
        "y_swish = swish(x)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y_tanh, label='Tanh', linewidth=2)\n",
        "plt.plot(x, y_sigmoid, label='Sigmoid', linewidth=2)\n",
        "plt.plot(x, y_relu, label='ReLU', linewidth=2)\n",
        "plt.plot(x, y_swish, label='Swish', linewidth=2)\n",
        "\n",
        "plt.title('Activation Functions')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7a1eea74",
      "metadata": {
        "id": "7a1eea74"
      },
      "outputs": [],
      "source": [
        "# In PyTorch, these functions can be invoked via:\n",
        "\n",
        "# This is a class instance of the ReLU class\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "# It does almost nothing except apply the ReLU function to\n",
        "# your tensor, and being almost everywhere differentiable.\n",
        "some_tensor = torch.randn((16,3,256,256))\n",
        "some_tensor = relu(some_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9edf033",
      "metadata": {
        "id": "b9edf033"
      },
      "source": [
        "**Convolution layers** - Conceptually, you can imagine a convolution operation as asking the question 'Where on my image do I have the feature **f**'? The (typical 2D) convolution operator (or kernel) is a tensor of shape M x N. The convolution operation assigns the value of a pixel in the result image by computing the element-wise product of our input image in an area around a so-called *anchor pixel* and the convolution kernel, and then summing. This process is repeated for all pixels in the original image:  \n",
        "  \n",
        "$I_{new}[x, y] = (I * K)[x, y] = \\sum_{a}\\sum_{b} K[a, b]*I[x - a, y - b]$  \n",
        "  \n",
        "with $I$ being our original image, $I_{new}$ being the result image, and $K$ being the convolution kernel.\n",
        "*In layman's terms, if you draw a little circle into the kernel, then the result of convolving the input image is a map of where in the input image you can find circles like the one in the kernel*. It is therefore often called a *feature map* or an *activation map*.\n",
        "\n",
        "In a modern neural network, convolutional layers are ubiquitous. The computer will, over the course of the training, adapt the contents of the convolution kernels in order to best represent and categorize the images it sees. Features in early convolutional layers correspond to the actual content of the image and make intuitive sense to a human observer - to classify cars, you can expect to be needing a couple of differently angled straight lines, some smooth curves, and circles for the wheels, for example. The features in later layers are less (or not at all) interpretable to human eyes. At that point, the image that the computer has, typically consists of activation maps of activation maps of activation maps of ... well, you get the gist.\n",
        "\n",
        "It is still possible to visualize what these represent, and some of the features are surprisingly human-interpretable (such as an entire car wheel, or the texture of a brick wall), while others look like LSD-induced hallucinations. If we have some time at the end of the course, we may try our hands at this visualization ourselves, and take a look at how your neural networks see the world.\n",
        "\n",
        "Convolution operations are defined by their number of in-going and out-going signal channels. At the beginning of a neural network, the in-going channel number is typically 3 (for RGB images) or 1 (for grayscale images). The out-going channel number can be freely chosen. Convolution operations can have any kernel size, but typically its height and width are chosen as odd numbers. This is done because the convolution kernel is applied to the area around the anchor pixel in our original image - a kernel with odd dimensions can be centered exactly on this anchor, while one with even dimensions can not. The latter case causes interesting systematic errors, which are fun to look at, but have a tendency to ruin the performance of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "86d31c64",
      "metadata": {
        "id": "86d31c64"
      },
      "outputs": [],
      "source": [
        "# Create a 2D convolutional layer that maps 3 input channels to 8 output channels\n",
        "# using 3x3 filters (learnable weights and biases)\n",
        "conv = torch.nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3,3))\n",
        "\n",
        "# Create a batch of 16 random RGB images (16 x 3 x 256 x 256)\n",
        "some_tensor = torch.randn((16,3,256,256))\n",
        "\n",
        "# Apply the convolutional layer to the input tensor\n",
        "# Output shape: (16, 8, 254, 254)\n",
        "some_tensor = conv(some_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d668a11d",
      "metadata": {
        "id": "d668a11d"
      },
      "source": [
        "**Pooling operations** - Conceptually, a pooling operation is quite simple. We compute a new image from a pooling operation by starting at a pixel, and applying the pooling operator to the pixel and its surroundings. The result is the value of the a new pixel. This process is repeated for every pixel in the image and you get a new image. Typically, the pooling operator either performs the max() or the mean() operation on the area it is applied to. These specific pooling ops are name MaxPool and AvgPool, respectively.\n",
        "\n",
        "Very often, something called a 'stride' is applied during the pooling operation. 'A stride of *s*' means that in order to get a new image, we do not iterate over every pixel and perform pooling on its respective surrounding, but that we instead skip *s* rows and columns on each step. Typically, stride and pooling operator shapes are chosen to be the same, so that no pixel is ignored during the calculations, and so that no pixel contributes to the new image more than once. Consequently, the new image after a pooling operation will be downscaled by a factor *s*. Since, in a sense, the new image still contains *all, or most of the relevant* information from the original image, strided pooling is often used for exactly this downscaling effect. This reduces the amount of computation needed for later steps, and helps extracting the most relevant information from an image.\n",
        "\n",
        "If you try to imagine applying a 2x2 pooling operator to, say, a 5x5 image, do you notice a problem? No matter how you define the 2x2 surrounding of your current pixel, some pixels you need to calculate the result of the pooling operator will not exist - they would be outside the bounds of our image! This is a very common occurence, and pooling operations perform something called *padding*. Padding extends the image by the necessary pixels, just for the calculations of pixels that actually exist. Typically, the image is extended using pixels which exclusively have the values zero, although other paddings exist. Convolution operations sometimes do the same thing, for the same reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "70dd8ca4",
      "metadata": {
        "id": "70dd8ca4"
      },
      "outputs": [],
      "source": [
        "# In PyTorch, these pooling operations can be invoked via:\n",
        "\n",
        "# This is a class instance of the MaxPool2d class\n",
        "max_pool = torch.nn.MaxPool2d(kernel_size = (2, 2), stride = 2)\n",
        "\n",
        "# It does almost nothing except apply the ReLU function to\n",
        "# your tensor, and being differentiable.\n",
        "some_tensor = torch.randn((16,3,256,256))\n",
        "some_tensor = max_pool(some_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2796287c",
      "metadata": {
        "id": "2796287c"
      },
      "source": [
        "And now it's your turn.\n",
        "\n",
        "**Task 1 (2 points)**: Try to make a model that performs mathematical operations that you've learned about in the course. For example, try adding in a convolutional layer or two, a fully connected layer or two, and nonlinearities such as ReLUs or tanhs.\n",
        "\n",
        "Let's prepare ourselves to make a model for the LiTS 2017 classification task that we've already previously talked about. All the model has to do is take the dummy batch below as input (batch size 16, one channel, 256 by 256 pixels), and produce an output of the shape B x 3, for the three classes Nothing, Liver, Liver+Tumor. It doesn't have to be good, or clever, or work particularly well for now. All we want is for you to get a feeling for the various layers we discussed, how to manipulate your input tensors into different shapes, and which input and output shapes a tensor will have after you add a specific layer.\n",
        "\n",
        "**Task 2 (1 point)**: Where in a convolutional neural network would you use a fully connected layer, if at all, and where shouldn't you? Why?\n",
        "\n",
        "**Task 3 (1 point)**: Which shape do convolution layer weights have if the input image has more than one input and output channel? To show that you understand the math properly, explain in simple terms why it must have this shape. A visual explanation probably works best. (You can look the shape up in your own code, if you don't know.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "60d7d08e",
      "metadata": {
        "id": "60d7d08e",
        "outputId": "4c5a9cb4-32a2-4227-bd77-f0a18d71c9df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# This dummy batch (batch size 16, 3 channels, 256x256 pixels)\n",
        "# will be your input. Try to see if the output of your model is\n",
        "# what you expected it to be.\n",
        "dummy_batch = torch.rand((16, 3, 256, 256))\n",
        "print(dummy_batch.size())\n",
        "\n",
        "# Remember that you can use some_tensor.size() to check the\n",
        "# current dimensions of your tensor, so that you keep track of\n",
        "# what the things you've just added do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "696517f3",
      "metadata": {
        "id": "696517f3",
        "outputId": "f5a7227f-3db7-48d0-b889-3f496dbc750c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Really_Good_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Ein kleines CNN, das am Ende 3 Ausgaben pro Batch liefert.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 3 Eingangs-KanÃ¤le, 3 Filter, 3x3 Kernel\n",
        "        self.conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Linear Layer: in_features = out_channels * H_out * W_out\n",
        "        # H_out = W_out = 256 - 3( kernel) + 1 (stride)= 254\n",
        "        # in_features = 3 * 254 * 254 = 193,548\n",
        "        self.fc1 = nn.Linear(in_features=3*254*254, out_features=3)\n",
        "\n",
        "    def forward(self, x): #reihenfolge der funktionen ist hier standard und sollte immer so gemacht werden!!!\n",
        "        x = self.conv(x)\n",
        "        # Shape nach Conv2d: (16, 3, 254, 254)\n",
        "        x = self.relu(x)\n",
        "        # Shape nach ReLU: (16, 3, 254, 254)\n",
        "        x = self.flatten(x)\n",
        "        # Shape nach Flatten: (16, 193548) (erster Wert, 3*254*254 alle hinteren Werte)\n",
        "        x = self.fc1(x)\n",
        "        # Shape nach Linear: (16, 3) (3 kommt raus, weil 3 output features)\n",
        "        return x\n",
        "\n",
        "# Beispiel-Batch: 16 Bilder, 3 KanÃ¤le, 256x256 Pixel\n",
        "dummy_batch = torch.rand((16, 3, 256, 256))\n",
        "\n",
        "a_really_good_model = Really_Good_Model()\n",
        "prediction = a_really_good_model(dummy_batch)\n",
        "print(prediction.size())  # sollte: torch.Size([16, 3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ohGx1hYOvveR"
      },
      "id": "ohGx1hYOvveR"
    },
    {
      "cell_type": "code",
      "source": [
        "#Wann Fully connected layer? Nur am Ende, weil FC = â€žEnd-Entscheidung aus allen Merkmalenâ€œ. im code ist es zu viel verbinden\n",
        "#von Parameteren und zerstÃ¶rt die rÃ¤rumlcihe AuflÃ¶sung von Pixeln.\n",
        "#Am Ende gut um features zusammen zu fasssen"
      ],
      "metadata": {
        "id": "2phXQHbyvuh3"
      },
      "id": "2phXQHbyvuh3",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e463efb9",
      "metadata": {
        "id": "e463efb9"
      },
      "source": [
        "#### Chapter 2.7 - Putting it all together\n",
        "\n",
        "We have all the pieces. We have a recipe, with which we can train a model using some data. We have made ourselves a dataset, using the LiTS data, last week. And we know how to make our own, small, CNN models.\n",
        "\n",
        "**Task 4 (2 points + 2 bonus points)**: Stitch together all the individual parts that you have built. You are allowed to just copy your previous solutions. Try using as many of the things you have learnt about as possible, and see if your neural network is capable of training to recognize liver cancer. You script has to do the following things:\n",
        "- Use your own dataset of LiTS 2017.\n",
        "- Use your own model from before.\n",
        "- Train your own model.\n",
        "- Use your own hyperparameters, if any. If you use hyperparameters or settings we have not discussed before, be ready to explain them to us.\n",
        "\n",
        "Whichever group gets the best results (you can build some different models and try out hyperparameters of your choosing), **gets 2 bonus points**. :v)\n",
        "\n",
        "If the loss goes down and your model's predictions are better than random guesses or all-zero guesses, the task is considered solved. All-zero guesses refers to the model always predicting \"No Liver, No Tumors\", which is typically class number 0 - this is a common failure mode for this task, and would not count. To check whether your solution works, take a look at your model's predictions.\n",
        "\n",
        "Below are some tips, things that students have had trouble with in the past, what they did and how you can spot whether you're accidentally doing them:\n",
        "- **Model loss never goes down** - Is your model always trainable, or did you put a model.eval() somewhere and never go back to model?train()? If you forget this, you never track gradients and consequently cannot learn. If the loss is always the same, very specific number, and your model is definitely trainable, it may also be something else. Maybe your model performs a softmax operation on its output, but the loss also does?\n",
        "- **Training loss immediately goes to zero** - You may be accidentally feeding your model the correct solutions as part of the input. Basically, the model \"cheats\" but never learns anything.\n",
        "- **Predictions are all the same class, loss is not going down or not a lot** - Your model is probably capable of learning, because this does not happen randomly, but it's probably not very good at learning. The issue is likely in the model class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8bda825d",
      "metadata": {
        "id": "8bda825d"
      },
      "outputs": [],
      "source": [
        "# Download our data again:\n",
        "#!gdown 1TItTaso19GFTPdDnynVnqJvHsCm_RGlI\n",
        "#!rm -rf ./sample_data/\n",
        "!rm -rf ./Clean_LiTS\n",
        "!unzip -qq ./drive/MyDrive/Clean_LiTS.zip -d .\n",
        "#!rm ./Clean_LiTS.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyZxdW3mbvko",
        "outputId": "c5ca02ab-f90e-499e-fc73-974bd4ed44e3"
      },
      "id": "lyZxdW3mbvko",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import PIL\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------------------\n",
        "# Datensatzdefinition\n",
        "# -------------------------------\n",
        "class LiTS_Dataset(Dataset):\n",
        "    def __init__(self, train_csv: str, val_csv: str, test_csv: str):\n",
        "        self.train_df = pd.read_csv(train_csv)\n",
        "        self.val_df = pd.read_csv(val_csv)\n",
        "        self.test_df = pd.read_csv(test_csv)\n",
        "        self.mode = \"train\"\n",
        "        self.label_map = {\"NoLiver\": 0, \"LiverWithoutTumor\": 1, \"LiverWithTumor\": 2}\n",
        "\n",
        "    def changemode(self, name: str):\n",
        "        self.mode = name\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.train_df)\n",
        "        elif self.mode == \"val\":\n",
        "            return len(self.val_df)\n",
        "        elif self.mode == \"test\":\n",
        "            return len(self.test_df)\n",
        "        else:\n",
        "            raise ValueError(\"Mode name problematic\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"train\":\n",
        "            row = self.train_df.iloc[idx]\n",
        "        elif self.mode == \"val\":\n",
        "            row = self.val_df.iloc[idx]\n",
        "        elif self.mode == \"test\":\n",
        "            row = self.test_df.iloc[idx]\n",
        "        else:\n",
        "            raise ValueError(\"Mode name problematic\")\n",
        "\n",
        "        img_name = row[\"filename\"]\n",
        "\n",
        "        # Labelzuweisung\n",
        "        if row[\"liver_visible\"] == True:\n",
        "            if row[\"lesion_visible\"] == True:\n",
        "                target_value = 2\n",
        "            else:\n",
        "                target_value = 1\n",
        "        else:\n",
        "            target_value = 0\n",
        "\n",
        "        target_tensor = torch.tensor(target_value, dtype=torch.long)\n",
        "\n",
        "        img_dir = f\"./Clean_LiTS/{self.mode}\"\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "        # Graustufen laden (1 Kanal)\n",
        "        img = PIL.Image.open(img_path).convert(\"L\")\n",
        "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
        "        img_tensor = torch.tensor(img_array).unsqueeze(0)  # (1, H, W)\n",
        "\n",
        "        return img_tensor, target_tensor\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset & Dataloader\n",
        "# -------------------------------\n",
        "lits_dataset = LiTS_Dataset(\n",
        "    \"./Clean_LiTS/train_classes.csv\",\n",
        "    \"./Clean_LiTS/val_classes.csv\",\n",
        "    \"./Clean_LiTS/test_classes.csv\"\n",
        ")\n",
        "\n",
        "lits_dataset.changemode(\"train\")\n",
        "\n",
        "print(\"Train DatensÃ¤tze:\", len(lits_dataset))\n",
        "\n",
        "# Beispielbild anschauen\n",
        "example_img, _ = lits_dataset.__getitem__(0)\n",
        "print(\"Einzelnes Bild Shape:\", example_img.shape)\n",
        "\n",
        "input_shape = example_img.shape  # (1, H, W)\n",
        "\n",
        "# Loader erstellen\n",
        "lits_dataloader = DataLoader(\n",
        "    dataset=lits_dataset,\n",
        "    batch_size=16,\n",
        "    num_workers=0,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Modell\n",
        "# -------------------------------\n",
        "class Really_Good_Model_forLIST(nn.Module):\n",
        "    \"\"\"\n",
        "    Ein kleines CNN, das am Ende 3 Klassen ausgibt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=3, kernel_size=3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dynamische Bestimmung der Flatten-Size\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, *input_shape)\n",
        "            out = self.conv(dummy)\n",
        "            out = self.relu(out)\n",
        "            out = self.conv2(out)\n",
        "            out = self.relu2(out)\n",
        "            flattened_size = out.view(1, -1).size(1)\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, out_features=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Setup Training\n",
        "# -------------------------------\n",
        "model = Really_Good_Model_forLIST(input_shape)\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Loop\n",
        "# -------------------------------\n",
        "num_epochs = 6\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    lits_dataset.changemode(\"train\")\n",
        "\n",
        "    for step, (data, targets) in enumerate(lits_dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(data)\n",
        "        loss = loss_criterion(predictions, targets)\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    lits_dataset.changemode(\"val\")\n",
        "    val_loader = DataLoader(dataset=lits_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    hits, losses, total = 0, [], 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            predictions = model(data)\n",
        "            loss = loss_criterion(predictions, targets)\n",
        "            losses.append(loss.item())\n",
        "            pred_classes = torch.argmax(predictions, dim=1)\n",
        "            hits += (pred_classes == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Validation Loss {np.mean(losses):.4f}, Accuracy {hits/total:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Testlauf\n",
        "# -------------------------------\n",
        "lits_dataset.changemode(\"test\")\n",
        "test_loader = DataLoader(dataset=lits_dataset, batch_size=16, shuffle=False)\n",
        "images, targets = next(iter(test_loader))\n",
        "preds = model(images.to(device))\n",
        "print(\"Prediction size:\", preds.size())\n"
      ],
      "metadata": {
        "id": "GvXjekfSgNOj",
        "outputId": "3a637f9d-f1c7-4644-e4b5-65994e4a907e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GvXjekfSgNOj",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DatensÃ¤tze: 35484\n",
            "Einzelnes Bild Shape: torch.Size([1, 256, 256])\n",
            "Device: cuda\n",
            "Epoch [1/6] Step [0] Loss: 1.1085\n",
            "Epoch [1/6] Step [10] Loss: 1.0575\n",
            "Epoch [1/6] Step [20] Loss: 0.8287\n",
            "Epoch [1/6] Step [30] Loss: 1.1593\n",
            "Epoch [1/6] Step [40] Loss: 0.7546\n",
            "Epoch [1/6] Step [50] Loss: 1.0167\n",
            "Epoch [1/6] Step [60] Loss: 0.9899\n",
            "Epoch [1/6] Step [70] Loss: 0.9728\n",
            "Epoch [1/6] Step [80] Loss: 0.7113\n",
            "Epoch [1/6] Step [90] Loss: 1.0458\n",
            "Epoch [1/6] Step [100] Loss: 0.8479\n",
            "Epoch [1/6] Step [110] Loss: 0.8025\n",
            "Epoch [1/6] Step [120] Loss: 0.8757\n",
            "Epoch [1/6] Step [130] Loss: 0.8472\n",
            "Epoch [1/6] Step [140] Loss: 0.7667\n",
            "Epoch [1/6] Step [150] Loss: 0.6741\n",
            "Epoch [1/6] Step [160] Loss: 0.7045\n",
            "Epoch [1/6] Step [170] Loss: 0.6948\n",
            "Epoch [1/6] Step [180] Loss: 0.9263\n",
            "Epoch [1/6] Step [190] Loss: 0.8661\n",
            "Epoch [1/6] Step [200] Loss: 0.5529\n",
            "Epoch [1/6] Step [210] Loss: 0.9739\n",
            "Epoch [1/6] Step [220] Loss: 0.5873\n",
            "Epoch [1/6] Step [230] Loss: 0.7168\n",
            "Epoch [1/6] Step [240] Loss: 0.7856\n",
            "Epoch [1/6] Step [250] Loss: 0.7154\n",
            "Epoch [1/6] Step [260] Loss: 0.5330\n",
            "Epoch [1/6] Step [270] Loss: 0.5974\n",
            "Epoch [1/6] Step [280] Loss: 0.5985\n",
            "Epoch [1/6] Step [290] Loss: 0.5540\n",
            "Epoch [1/6] Step [300] Loss: 0.5390\n",
            "Epoch [1/6] Step [310] Loss: 0.6416\n",
            "Epoch [1/6] Step [320] Loss: 0.7288\n",
            "Epoch [1/6] Step [330] Loss: 0.5310\n",
            "Epoch [1/6] Step [340] Loss: 0.8753\n",
            "Epoch [1/6] Step [350] Loss: 0.7199\n",
            "Epoch [1/6] Step [360] Loss: 0.5910\n",
            "Epoch [1/6] Step [370] Loss: 0.7282\n",
            "Epoch [1/6] Step [380] Loss: 0.3691\n",
            "Epoch [1/6] Step [390] Loss: 0.5607\n",
            "Epoch [1/6] Step [400] Loss: 0.7594\n",
            "Epoch [1/6] Step [410] Loss: 0.5899\n",
            "Epoch [1/6] Step [420] Loss: 0.7144\n",
            "Epoch [1/6] Step [430] Loss: 0.5864\n",
            "Epoch [1/6] Step [440] Loss: 0.8011\n",
            "Epoch [1/6] Step [450] Loss: 0.7550\n",
            "Epoch [1/6] Step [460] Loss: 0.6073\n",
            "Epoch [1/6] Step [470] Loss: 0.5627\n",
            "Epoch [1/6] Step [480] Loss: 0.6748\n",
            "Epoch [1/6] Step [490] Loss: 0.4163\n",
            "Epoch [1/6] Step [500] Loss: 0.6612\n",
            "Epoch [1/6] Step [510] Loss: 0.5177\n",
            "Epoch [1/6] Step [520] Loss: 0.6963\n",
            "Epoch [1/6] Step [530] Loss: 0.4432\n",
            "Epoch [1/6] Step [540] Loss: 0.6276\n",
            "Epoch [1/6] Step [550] Loss: 0.5893\n",
            "Epoch [1/6] Step [560] Loss: 0.6687\n",
            "Epoch [1/6] Step [570] Loss: 0.6632\n",
            "Epoch [1/6] Step [580] Loss: 0.7632\n",
            "Epoch [1/6] Step [590] Loss: 0.3683\n",
            "Epoch [1/6] Step [600] Loss: 0.4398\n",
            "Epoch [1/6] Step [610] Loss: 0.4175\n",
            "Epoch [1/6] Step [620] Loss: 0.4720\n",
            "Epoch [1/6] Step [630] Loss: 0.6468\n",
            "Epoch [1/6] Step [640] Loss: 0.6807\n",
            "Epoch [1/6] Step [650] Loss: 0.9475\n",
            "Epoch [1/6] Step [660] Loss: 0.5929\n",
            "Epoch [1/6] Step [670] Loss: 0.4605\n",
            "Epoch [1/6] Step [680] Loss: 0.5983\n",
            "Epoch [1/6] Step [690] Loss: 0.5089\n",
            "Epoch [1/6] Step [700] Loss: 0.7076\n",
            "Epoch [1/6] Step [710] Loss: 0.4438\n",
            "Epoch [1/6] Step [720] Loss: 0.5492\n",
            "Epoch [1/6] Step [730] Loss: 0.4139\n",
            "Epoch [1/6] Step [740] Loss: 0.7193\n",
            "Epoch [1/6] Step [750] Loss: 0.4258\n",
            "Epoch [1/6] Step [760] Loss: 0.4831\n",
            "Epoch [1/6] Step [770] Loss: 0.4132\n",
            "Epoch [1/6] Step [780] Loss: 0.5532\n",
            "Epoch [1/6] Step [790] Loss: 0.2999\n",
            "Epoch [1/6] Step [800] Loss: 0.6492\n",
            "Epoch [1/6] Step [810] Loss: 0.4587\n",
            "Epoch [1/6] Step [820] Loss: 0.3516\n",
            "Epoch [1/6] Step [830] Loss: 0.5226\n",
            "Epoch [1/6] Step [840] Loss: 0.4824\n",
            "Epoch [1/6] Step [850] Loss: 0.5598\n",
            "Epoch [1/6] Step [860] Loss: 0.5416\n",
            "Epoch [1/6] Step [870] Loss: 0.6871\n",
            "Epoch [1/6] Step [880] Loss: 0.8475\n",
            "Epoch [1/6] Step [890] Loss: 0.2532\n",
            "Epoch [1/6] Step [900] Loss: 0.5111\n",
            "Epoch [1/6] Step [910] Loss: 0.6810\n",
            "Epoch [1/6] Step [920] Loss: 0.4607\n",
            "Epoch [1/6] Step [930] Loss: 0.6110\n",
            "Epoch [1/6] Step [940] Loss: 0.6195\n",
            "Epoch [1/6] Step [950] Loss: 0.4462\n",
            "Epoch [1/6] Step [960] Loss: 0.4234\n",
            "Epoch [1/6] Step [970] Loss: 0.2683\n",
            "Epoch [1/6] Step [980] Loss: 0.3271\n",
            "Epoch [1/6] Step [990] Loss: 0.5615\n",
            "Epoch [1/6] Step [1000] Loss: 0.3494\n",
            "Epoch [1/6] Step [1010] Loss: 0.4997\n",
            "Epoch [1/6] Step [1020] Loss: 0.3739\n",
            "Epoch [1/6] Step [1030] Loss: 0.5131\n",
            "Epoch [1/6] Step [1040] Loss: 0.3862\n",
            "Epoch [1/6] Step [1050] Loss: 0.4286\n",
            "Epoch [1/6] Step [1060] Loss: 0.7205\n",
            "Epoch [1/6] Step [1070] Loss: 0.5011\n",
            "Epoch [1/6] Step [1080] Loss: 0.6578\n",
            "Epoch [1/6] Step [1090] Loss: 0.3539\n",
            "Epoch [1/6] Step [1100] Loss: 0.3390\n",
            "Epoch [1/6] Step [1110] Loss: 0.7524\n",
            "Epoch [1/6] Step [1120] Loss: 0.5732\n",
            "Epoch [1/6] Step [1130] Loss: 0.6192\n",
            "Epoch [1/6] Step [1140] Loss: 0.8035\n",
            "Epoch [1/6] Step [1150] Loss: 0.5539\n",
            "Epoch [1/6] Step [1160] Loss: 0.4249\n",
            "Epoch [1/6] Step [1170] Loss: 0.3298\n",
            "Epoch [1/6] Step [1180] Loss: 0.5300\n",
            "Epoch [1/6] Step [1190] Loss: 0.4846\n",
            "Epoch [1/6] Step [1200] Loss: 0.2058\n",
            "Epoch [1/6] Step [1210] Loss: 0.5239\n",
            "Epoch [1/6] Step [1220] Loss: 0.2173\n",
            "Epoch [1/6] Step [1230] Loss: 0.7234\n",
            "Epoch [1/6] Step [1240] Loss: 0.5701\n",
            "Epoch [1/6] Step [1250] Loss: 0.2093\n",
            "Epoch [1/6] Step [1260] Loss: 0.5220\n",
            "Epoch [1/6] Step [1270] Loss: 0.3255\n",
            "Epoch [1/6] Step [1280] Loss: 0.2897\n",
            "Epoch [1/6] Step [1290] Loss: 0.6408\n",
            "Epoch [1/6] Step [1300] Loss: 0.4604\n",
            "Epoch [1/6] Step [1310] Loss: 0.6695\n",
            "Epoch [1/6] Step [1320] Loss: 0.5001\n",
            "Epoch [1/6] Step [1330] Loss: 0.5256\n",
            "Epoch [1/6] Step [1340] Loss: 0.5380\n",
            "Epoch [1/6] Step [1350] Loss: 0.3274\n",
            "Epoch [1/6] Step [1360] Loss: 0.2482\n",
            "Epoch [1/6] Step [1370] Loss: 0.4720\n",
            "Epoch [1/6] Step [1380] Loss: 0.4121\n",
            "Epoch [1/6] Step [1390] Loss: 0.7715\n",
            "Epoch [1/6] Step [1400] Loss: 0.5161\n",
            "Epoch [1/6] Step [1410] Loss: 0.2901\n",
            "Epoch [1/6] Step [1420] Loss: 0.3665\n",
            "Epoch [1/6] Step [1430] Loss: 0.6044\n",
            "Epoch [1/6] Step [1440] Loss: 0.4386\n",
            "Epoch [1/6] Step [1450] Loss: 0.6546\n",
            "Epoch [1/6] Step [1460] Loss: 0.5302\n",
            "Epoch [1/6] Step [1470] Loss: 0.4101\n",
            "Epoch [1/6] Step [1480] Loss: 0.8092\n",
            "Epoch [1/6] Step [1490] Loss: 0.4053\n",
            "Epoch [1/6] Step [1500] Loss: 0.3256\n",
            "Epoch [1/6] Step [1510] Loss: 0.4494\n",
            "Epoch [1/6] Step [1520] Loss: 0.6767\n",
            "Epoch [1/6] Step [1530] Loss: 0.6684\n",
            "Epoch [1/6] Step [1540] Loss: 0.5733\n",
            "Epoch [1/6] Step [1550] Loss: 0.3591\n",
            "Epoch [1/6] Step [1560] Loss: 0.2801\n",
            "Epoch [1/6] Step [1570] Loss: 0.4876\n",
            "Epoch [1/6] Step [1580] Loss: 0.2947\n",
            "Epoch [1/6] Step [1590] Loss: 0.3260\n",
            "Epoch [1/6] Step [1600] Loss: 0.3377\n",
            "Epoch [1/6] Step [1610] Loss: 0.4870\n",
            "Epoch [1/6] Step [1620] Loss: 0.2398\n",
            "Epoch [1/6] Step [1630] Loss: 0.5919\n",
            "Epoch [1/6] Step [1640] Loss: 0.3859\n",
            "Epoch [1/6] Step [1650] Loss: 0.2886\n",
            "Epoch [1/6] Step [1660] Loss: 0.6089\n",
            "Epoch [1/6] Step [1670] Loss: 0.5882\n",
            "Epoch [1/6] Step [1680] Loss: 0.3561\n",
            "Epoch [1/6] Step [1690] Loss: 0.3943\n",
            "Epoch [1/6] Step [1700] Loss: 0.4007\n",
            "Epoch [1/6] Step [1710] Loss: 0.3364\n",
            "Epoch [1/6] Step [1720] Loss: 0.4398\n",
            "Epoch [1/6] Step [1730] Loss: 0.3275\n",
            "Epoch [1/6] Step [1740] Loss: 0.2306\n",
            "Epoch [1/6] Step [1750] Loss: 0.4126\n",
            "Epoch [1/6] Step [1760] Loss: 0.4810\n",
            "Epoch [1/6] Step [1770] Loss: 0.5088\n",
            "Epoch [1/6] Step [1780] Loss: 0.6305\n",
            "Epoch [1/6] Step [1790] Loss: 0.3719\n",
            "Epoch [1/6] Step [1800] Loss: 0.5401\n",
            "Epoch [1/6] Step [1810] Loss: 0.6017\n",
            "Epoch [1/6] Step [1820] Loss: 0.3518\n",
            "Epoch [1/6] Step [1830] Loss: 0.4797\n",
            "Epoch [1/6] Step [1840] Loss: 0.4879\n",
            "Epoch [1/6] Step [1850] Loss: 0.2451\n",
            "Epoch [1/6] Step [1860] Loss: 0.5237\n",
            "Epoch [1/6] Step [1870] Loss: 0.3857\n",
            "Epoch [1/6] Step [1880] Loss: 0.3838\n",
            "Epoch [1/6] Step [1890] Loss: 0.2906\n",
            "Epoch [1/6] Step [1900] Loss: 0.1998\n",
            "Epoch [1/6] Step [1910] Loss: 0.4765\n",
            "Epoch [1/6] Step [1920] Loss: 0.4498\n",
            "Epoch [1/6] Step [1930] Loss: 0.4887\n",
            "Epoch [1/6] Step [1940] Loss: 0.6196\n",
            "Epoch [1/6] Step [1950] Loss: 0.4061\n",
            "Epoch [1/6] Step [1960] Loss: 0.4063\n",
            "Epoch [1/6] Step [1970] Loss: 0.2467\n",
            "Epoch [1/6] Step [1980] Loss: 0.6322\n",
            "Epoch [1/6] Step [1990] Loss: 0.2831\n",
            "Epoch [1/6] Step [2000] Loss: 0.3593\n",
            "Epoch [1/6] Step [2010] Loss: 0.1538\n",
            "Epoch [1/6] Step [2020] Loss: 0.4962\n",
            "Epoch [1/6] Step [2030] Loss: 0.5059\n",
            "Epoch [1/6] Step [2040] Loss: 0.4853\n",
            "Epoch [1/6] Step [2050] Loss: 0.3587\n",
            "Epoch [1/6] Step [2060] Loss: 0.3459\n",
            "Epoch [1/6] Step [2070] Loss: 0.4677\n",
            "Epoch [1/6] Step [2080] Loss: 0.5127\n",
            "Epoch [1/6] Step [2090] Loss: 0.2803\n",
            "Epoch [1/6] Step [2100] Loss: 0.3547\n",
            "Epoch [1/6] Step [2110] Loss: 0.3420\n",
            "Epoch [1/6] Step [2120] Loss: 0.4030\n",
            "Epoch [1/6] Step [2130] Loss: 0.4934\n",
            "Epoch [1/6] Step [2140] Loss: 0.3746\n",
            "Epoch [1/6] Step [2150] Loss: 0.2040\n",
            "Epoch [1/6] Step [2160] Loss: 0.5458\n",
            "Epoch [1/6] Step [2170] Loss: 0.4171\n",
            "Epoch [1/6] Step [2180] Loss: 0.3363\n",
            "Epoch [1/6] Step [2190] Loss: 0.5452\n",
            "Epoch [1/6] Step [2200] Loss: 0.4224\n",
            "Epoch [1/6] Step [2210] Loss: 0.3089\n",
            "Epoch 1: Validation Loss 0.5432, Accuracy 0.7634\n",
            "Epoch [2/6] Step [0] Loss: 0.3456\n",
            "Epoch [2/6] Step [10] Loss: 0.3590\n",
            "Epoch [2/6] Step [20] Loss: 0.4511\n",
            "Epoch [2/6] Step [30] Loss: 0.2608\n",
            "Epoch [2/6] Step [40] Loss: 0.3178\n",
            "Epoch [2/6] Step [50] Loss: 0.5501\n",
            "Epoch [2/6] Step [60] Loss: 0.3583\n",
            "Epoch [2/6] Step [70] Loss: 0.6015\n",
            "Epoch [2/6] Step [80] Loss: 0.3998\n",
            "Epoch [2/6] Step [90] Loss: 0.2678\n",
            "Epoch [2/6] Step [100] Loss: 0.2413\n",
            "Epoch [2/6] Step [110] Loss: 0.6665\n",
            "Epoch [2/6] Step [120] Loss: 0.4904\n",
            "Epoch [2/6] Step [130] Loss: 0.2403\n",
            "Epoch [2/6] Step [140] Loss: 0.3569\n",
            "Epoch [2/6] Step [150] Loss: 0.5428\n",
            "Epoch [2/6] Step [160] Loss: 0.3796\n",
            "Epoch [2/6] Step [170] Loss: 0.2951\n",
            "Epoch [2/6] Step [180] Loss: 0.4221\n",
            "Epoch [2/6] Step [190] Loss: 0.4123\n",
            "Epoch [2/6] Step [200] Loss: 0.3228\n",
            "Epoch [2/6] Step [210] Loss: 0.4172\n",
            "Epoch [2/6] Step [220] Loss: 0.3450\n",
            "Epoch [2/6] Step [230] Loss: 0.4349\n",
            "Epoch [2/6] Step [240] Loss: 0.3204\n",
            "Epoch [2/6] Step [250] Loss: 0.3190\n",
            "Epoch [2/6] Step [260] Loss: 0.5856\n",
            "Epoch [2/6] Step [270] Loss: 0.1718\n",
            "Epoch [2/6] Step [280] Loss: 0.1656\n",
            "Epoch [2/6] Step [290] Loss: 0.2832\n",
            "Epoch [2/6] Step [300] Loss: 0.2592\n",
            "Epoch [2/6] Step [310] Loss: 0.4078\n",
            "Epoch [2/6] Step [320] Loss: 0.2318\n",
            "Epoch [2/6] Step [330] Loss: 0.8428\n",
            "Epoch [2/6] Step [340] Loss: 0.4770\n",
            "Epoch [2/6] Step [350] Loss: 0.5819\n",
            "Epoch [2/6] Step [360] Loss: 0.3950\n",
            "Epoch [2/6] Step [370] Loss: 0.4200\n",
            "Epoch [2/6] Step [380] Loss: 0.4088\n",
            "Epoch [2/6] Step [390] Loss: 0.4736\n",
            "Epoch [2/6] Step [400] Loss: 0.3716\n",
            "Epoch [2/6] Step [410] Loss: 0.2687\n",
            "Epoch [2/6] Step [420] Loss: 0.4773\n",
            "Epoch [2/6] Step [430] Loss: 0.2914\n",
            "Epoch [2/6] Step [440] Loss: 0.5368\n",
            "Epoch [2/6] Step [450] Loss: 0.3989\n",
            "Epoch [2/6] Step [460] Loss: 0.5226\n",
            "Epoch [2/6] Step [470] Loss: 0.2089\n",
            "Epoch [2/6] Step [480] Loss: 0.1590\n",
            "Epoch [2/6] Step [490] Loss: 0.2378\n",
            "Epoch [2/6] Step [500] Loss: 0.4710\n",
            "Epoch [2/6] Step [510] Loss: 0.2890\n",
            "Epoch [2/6] Step [520] Loss: 0.5333\n",
            "Epoch [2/6] Step [530] Loss: 0.3837\n",
            "Epoch [2/6] Step [540] Loss: 0.5128\n",
            "Epoch [2/6] Step [550] Loss: 0.2458\n",
            "Epoch [2/6] Step [560] Loss: 0.3133\n",
            "Epoch [2/6] Step [570] Loss: 0.5043\n",
            "Epoch [2/6] Step [580] Loss: 0.3583\n",
            "Epoch [2/6] Step [590] Loss: 0.3273\n",
            "Epoch [2/6] Step [600] Loss: 0.2909\n",
            "Epoch [2/6] Step [610] Loss: 0.3254\n",
            "Epoch [2/6] Step [620] Loss: 0.3326\n",
            "Epoch [2/6] Step [630] Loss: 0.3419\n",
            "Epoch [2/6] Step [640] Loss: 0.6262\n",
            "Epoch [2/6] Step [650] Loss: 0.2913\n",
            "Epoch [2/6] Step [660] Loss: 0.3141\n",
            "Epoch [2/6] Step [670] Loss: 0.3793\n",
            "Epoch [2/6] Step [680] Loss: 0.2210\n",
            "Epoch [2/6] Step [690] Loss: 0.3489\n",
            "Epoch [2/6] Step [700] Loss: 0.3639\n",
            "Epoch [2/6] Step [710] Loss: 0.1786\n",
            "Epoch [2/6] Step [720] Loss: 0.4629\n",
            "Epoch [2/6] Step [730] Loss: 0.5572\n",
            "Epoch [2/6] Step [740] Loss: 0.7315\n",
            "Epoch [2/6] Step [750] Loss: 0.2888\n",
            "Epoch [2/6] Step [760] Loss: 0.3343\n",
            "Epoch [2/6] Step [770] Loss: 0.2730\n",
            "Epoch [2/6] Step [780] Loss: 0.5136\n",
            "Epoch [2/6] Step [790] Loss: 0.3084\n",
            "Epoch [2/6] Step [800] Loss: 0.3618\n",
            "Epoch [2/6] Step [810] Loss: 0.1226\n",
            "Epoch [2/6] Step [820] Loss: 0.1953\n",
            "Epoch [2/6] Step [830] Loss: 0.1946\n",
            "Epoch [2/6] Step [840] Loss: 0.2045\n",
            "Epoch [2/6] Step [850] Loss: 0.6724\n",
            "Epoch [2/6] Step [860] Loss: 0.3600\n",
            "Epoch [2/6] Step [870] Loss: 0.1099\n",
            "Epoch [2/6] Step [880] Loss: 0.2874\n",
            "Epoch [2/6] Step [890] Loss: 0.2371\n",
            "Epoch [2/6] Step [900] Loss: 0.3614\n",
            "Epoch [2/6] Step [910] Loss: 0.2185\n",
            "Epoch [2/6] Step [920] Loss: 0.4053\n",
            "Epoch [2/6] Step [930] Loss: 0.4342\n",
            "Epoch [2/6] Step [940] Loss: 0.4150\n",
            "Epoch [2/6] Step [950] Loss: 0.3588\n",
            "Epoch [2/6] Step [960] Loss: 0.4183\n",
            "Epoch [2/6] Step [970] Loss: 0.2117\n",
            "Epoch [2/6] Step [980] Loss: 0.2518\n",
            "Epoch [2/6] Step [990] Loss: 0.3648\n",
            "Epoch [2/6] Step [1000] Loss: 0.2014\n",
            "Epoch [2/6] Step [1010] Loss: 0.2759\n",
            "Epoch [2/6] Step [1020] Loss: 0.2238\n",
            "Epoch [2/6] Step [1030] Loss: 0.3917\n",
            "Epoch [2/6] Step [1040] Loss: 0.3402\n",
            "Epoch [2/6] Step [1050] Loss: 0.2474\n",
            "Epoch [2/6] Step [1060] Loss: 0.1830\n",
            "Epoch [2/6] Step [1070] Loss: 0.4992\n",
            "Epoch [2/6] Step [1080] Loss: 0.1470\n",
            "Epoch [2/6] Step [1090] Loss: 0.6639\n",
            "Epoch [2/6] Step [1100] Loss: 0.1647\n",
            "Epoch [2/6] Step [1110] Loss: 0.4594\n",
            "Epoch [2/6] Step [1120] Loss: 0.3030\n",
            "Epoch [2/6] Step [1130] Loss: 0.4931\n",
            "Epoch [2/6] Step [1140] Loss: 0.1935\n",
            "Epoch [2/6] Step [1150] Loss: 0.4951\n",
            "Epoch [2/6] Step [1160] Loss: 0.3043\n",
            "Epoch [2/6] Step [1170] Loss: 0.4567\n",
            "Epoch [2/6] Step [1180] Loss: 0.3928\n",
            "Epoch [2/6] Step [1190] Loss: 0.3195\n",
            "Epoch [2/6] Step [1200] Loss: 0.1741\n",
            "Epoch [2/6] Step [1210] Loss: 0.3747\n",
            "Epoch [2/6] Step [1220] Loss: 0.2303\n",
            "Epoch [2/6] Step [1230] Loss: 0.6880\n",
            "Epoch [2/6] Step [1240] Loss: 0.3910\n",
            "Epoch [2/6] Step [1250] Loss: 0.4615\n",
            "Epoch [2/6] Step [1260] Loss: 0.1742\n",
            "Epoch [2/6] Step [1270] Loss: 0.3496\n",
            "Epoch [2/6] Step [1280] Loss: 0.3762\n",
            "Epoch [2/6] Step [1290] Loss: 0.3715\n",
            "Epoch [2/6] Step [1300] Loss: 0.7777\n",
            "Epoch [2/6] Step [1310] Loss: 0.4184\n",
            "Epoch [2/6] Step [1320] Loss: 0.5722\n",
            "Epoch [2/6] Step [1330] Loss: 0.2446\n",
            "Epoch [2/6] Step [1340] Loss: 0.2658\n",
            "Epoch [2/6] Step [1350] Loss: 0.7263\n",
            "Epoch [2/6] Step [1360] Loss: 0.2586\n",
            "Epoch [2/6] Step [1370] Loss: 0.3695\n",
            "Epoch [2/6] Step [1380] Loss: 0.1284\n",
            "Epoch [2/6] Step [1390] Loss: 0.3419\n",
            "Epoch [2/6] Step [1400] Loss: 0.4962\n",
            "Epoch [2/6] Step [1410] Loss: 0.2615\n",
            "Epoch [2/6] Step [1420] Loss: 0.1370\n",
            "Epoch [2/6] Step [1430] Loss: 0.3295\n",
            "Epoch [2/6] Step [1440] Loss: 0.2834\n",
            "Epoch [2/6] Step [1450] Loss: 0.2278\n",
            "Epoch [2/6] Step [1460] Loss: 0.5415\n",
            "Epoch [2/6] Step [1470] Loss: 0.3799\n",
            "Epoch [2/6] Step [1480] Loss: 0.3413\n",
            "Epoch [2/6] Step [1490] Loss: 0.2276\n",
            "Epoch [2/6] Step [1500] Loss: 0.2917\n",
            "Epoch [2/6] Step [1510] Loss: 0.2056\n",
            "Epoch [2/6] Step [1520] Loss: 0.2978\n",
            "Epoch [2/6] Step [1530] Loss: 0.1088\n",
            "Epoch [2/6] Step [1540] Loss: 0.3014\n",
            "Epoch [2/6] Step [1550] Loss: 0.3382\n",
            "Epoch [2/6] Step [1560] Loss: 0.2332\n",
            "Epoch [2/6] Step [1570] Loss: 0.2734\n",
            "Epoch [2/6] Step [1580] Loss: 0.3263\n",
            "Epoch [2/6] Step [1590] Loss: 0.4776\n",
            "Epoch [2/6] Step [1600] Loss: 0.2172\n",
            "Epoch [2/6] Step [1610] Loss: 0.2578\n",
            "Epoch [2/6] Step [1620] Loss: 0.2924\n",
            "Epoch [2/6] Step [1630] Loss: 0.3493\n",
            "Epoch [2/6] Step [1640] Loss: 0.1673\n",
            "Epoch [2/6] Step [1650] Loss: 0.3262\n",
            "Epoch [2/6] Step [1660] Loss: 0.2322\n",
            "Epoch [2/6] Step [1670] Loss: 0.3994\n",
            "Epoch [2/6] Step [1680] Loss: 0.7140\n",
            "Epoch [2/6] Step [1690] Loss: 0.5113\n",
            "Epoch [2/6] Step [1700] Loss: 0.4071\n",
            "Epoch [2/6] Step [1710] Loss: 0.3053\n",
            "Epoch [2/6] Step [1720] Loss: 0.2835\n",
            "Epoch [2/6] Step [1730] Loss: 0.4606\n",
            "Epoch [2/6] Step [1740] Loss: 0.1463\n",
            "Epoch [2/6] Step [1750] Loss: 0.2006\n",
            "Epoch [2/6] Step [1760] Loss: 0.1698\n",
            "Epoch [2/6] Step [1770] Loss: 0.2048\n",
            "Epoch [2/6] Step [1780] Loss: 0.2731\n",
            "Epoch [2/6] Step [1790] Loss: 0.3146\n",
            "Epoch [2/6] Step [1800] Loss: 0.1196\n",
            "Epoch [2/6] Step [1810] Loss: 0.4521\n",
            "Epoch [2/6] Step [1820] Loss: 0.2376\n",
            "Epoch [2/6] Step [1830] Loss: 0.3323\n",
            "Epoch [2/6] Step [1840] Loss: 0.3533\n",
            "Epoch [2/6] Step [1850] Loss: 0.3901\n",
            "Epoch [2/6] Step [1860] Loss: 0.3938\n",
            "Epoch [2/6] Step [1870] Loss: 0.3616\n",
            "Epoch [2/6] Step [1880] Loss: 0.3124\n",
            "Epoch [2/6] Step [1890] Loss: 0.1864\n",
            "Epoch [2/6] Step [1900] Loss: 0.4545\n",
            "Epoch [2/6] Step [1910] Loss: 0.2512\n",
            "Epoch [2/6] Step [1920] Loss: 0.6305\n",
            "Epoch [2/6] Step [1930] Loss: 0.3326\n",
            "Epoch [2/6] Step [1940] Loss: 0.1145\n",
            "Epoch [2/6] Step [1950] Loss: 0.3455\n",
            "Epoch [2/6] Step [1960] Loss: 0.2383\n",
            "Epoch [2/6] Step [1970] Loss: 0.1916\n",
            "Epoch [2/6] Step [1980] Loss: 0.5138\n",
            "Epoch [2/6] Step [1990] Loss: 0.2517\n",
            "Epoch [2/6] Step [2000] Loss: 0.4263\n",
            "Epoch [2/6] Step [2010] Loss: 0.4369\n",
            "Epoch [2/6] Step [2020] Loss: 0.2644\n",
            "Epoch [2/6] Step [2030] Loss: 0.3571\n",
            "Epoch [2/6] Step [2040] Loss: 0.4292\n",
            "Epoch [2/6] Step [2050] Loss: 0.2158\n",
            "Epoch [2/6] Step [2060] Loss: 0.5672\n",
            "Epoch [2/6] Step [2070] Loss: 0.1898\n",
            "Epoch [2/6] Step [2080] Loss: 0.2865\n",
            "Epoch [2/6] Step [2090] Loss: 0.1738\n",
            "Epoch [2/6] Step [2100] Loss: 0.1267\n",
            "Epoch [2/6] Step [2110] Loss: 0.5646\n",
            "Epoch [2/6] Step [2120] Loss: 0.2726\n",
            "Epoch [2/6] Step [2130] Loss: 0.2678\n",
            "Epoch [2/6] Step [2140] Loss: 0.3338\n",
            "Epoch [2/6] Step [2150] Loss: 0.1062\n",
            "Epoch [2/6] Step [2160] Loss: 0.2914\n",
            "Epoch [2/6] Step [2170] Loss: 0.1003\n",
            "Epoch [2/6] Step [2180] Loss: 0.3386\n",
            "Epoch [2/6] Step [2190] Loss: 0.3536\n",
            "Epoch [2/6] Step [2200] Loss: 0.2920\n",
            "Epoch [2/6] Step [2210] Loss: 0.3999\n",
            "Epoch 2: Validation Loss 0.4430, Accuracy 0.7966\n",
            "Epoch [3/6] Step [0] Loss: 0.2551\n",
            "Epoch [3/6] Step [10] Loss: 0.2841\n",
            "Epoch [3/6] Step [20] Loss: 0.3383\n",
            "Epoch [3/6] Step [30] Loss: 0.4185\n",
            "Epoch [3/6] Step [40] Loss: 0.3827\n",
            "Epoch [3/6] Step [50] Loss: 0.2902\n",
            "Epoch [3/6] Step [60] Loss: 0.2287\n",
            "Epoch [3/6] Step [70] Loss: 0.2497\n",
            "Epoch [3/6] Step [80] Loss: 0.5491\n",
            "Epoch [3/6] Step [90] Loss: 0.3734\n",
            "Epoch [3/6] Step [100] Loss: 0.2736\n",
            "Epoch [3/6] Step [110] Loss: 0.3544\n",
            "Epoch [3/6] Step [120] Loss: 0.3044\n",
            "Epoch [3/6] Step [130] Loss: 0.3823\n",
            "Epoch [3/6] Step [140] Loss: 0.2701\n",
            "Epoch [3/6] Step [150] Loss: 0.4389\n",
            "Epoch [3/6] Step [160] Loss: 0.2251\n",
            "Epoch [3/6] Step [170] Loss: 0.5666\n",
            "Epoch [3/6] Step [180] Loss: 0.2830\n",
            "Epoch [3/6] Step [190] Loss: 0.3961\n",
            "Epoch [3/6] Step [200] Loss: 0.4833\n",
            "Epoch [3/6] Step [210] Loss: 0.3869\n",
            "Epoch [3/6] Step [220] Loss: 0.2210\n",
            "Epoch [3/6] Step [230] Loss: 0.5355\n",
            "Epoch [3/6] Step [240] Loss: 0.5418\n",
            "Epoch [3/6] Step [250] Loss: 0.3478\n",
            "Epoch [3/6] Step [260] Loss: 0.3370\n",
            "Epoch [3/6] Step [270] Loss: 0.3330\n",
            "Epoch [3/6] Step [280] Loss: 0.2373\n",
            "Epoch [3/6] Step [290] Loss: 0.3750\n",
            "Epoch [3/6] Step [300] Loss: 0.2330\n",
            "Epoch [3/6] Step [310] Loss: 0.2306\n",
            "Epoch [3/6] Step [320] Loss: 0.2953\n",
            "Epoch [3/6] Step [330] Loss: 0.1933\n",
            "Epoch [3/6] Step [340] Loss: 0.0783\n",
            "Epoch [3/6] Step [350] Loss: 0.2574\n",
            "Epoch [3/6] Step [360] Loss: 0.3402\n",
            "Epoch [3/6] Step [370] Loss: 0.3579\n",
            "Epoch [3/6] Step [380] Loss: 0.6724\n",
            "Epoch [3/6] Step [390] Loss: 0.1442\n",
            "Epoch [3/6] Step [400] Loss: 0.2383\n",
            "Epoch [3/6] Step [410] Loss: 0.2443\n",
            "Epoch [3/6] Step [420] Loss: 0.4981\n",
            "Epoch [3/6] Step [430] Loss: 0.3660\n",
            "Epoch [3/6] Step [440] Loss: 0.1452\n",
            "Epoch [3/6] Step [450] Loss: 0.2819\n",
            "Epoch [3/6] Step [460] Loss: 0.7546\n",
            "Epoch [3/6] Step [470] Loss: 0.2653\n",
            "Epoch [3/6] Step [480] Loss: 0.6188\n",
            "Epoch [3/6] Step [490] Loss: 0.2325\n",
            "Epoch [3/6] Step [500] Loss: 0.2406\n",
            "Epoch [3/6] Step [510] Loss: 0.5214\n",
            "Epoch [3/6] Step [520] Loss: 0.2830\n",
            "Epoch [3/6] Step [530] Loss: 0.2961\n",
            "Epoch [3/6] Step [540] Loss: 0.4407\n",
            "Epoch [3/6] Step [550] Loss: 0.2160\n",
            "Epoch [3/6] Step [560] Loss: 0.5437\n",
            "Epoch [3/6] Step [570] Loss: 0.2956\n",
            "Epoch [3/6] Step [580] Loss: 0.1902\n",
            "Epoch [3/6] Step [590] Loss: 0.0928\n",
            "Epoch [3/6] Step [600] Loss: 0.2067\n",
            "Epoch [3/6] Step [610] Loss: 0.0940\n",
            "Epoch [3/6] Step [620] Loss: 0.0726\n",
            "Epoch [3/6] Step [630] Loss: 0.2250\n",
            "Epoch [3/6] Step [640] Loss: 0.6164\n",
            "Epoch [3/6] Step [650] Loss: 0.1503\n",
            "Epoch [3/6] Step [660] Loss: 0.4337\n",
            "Epoch [3/6] Step [670] Loss: 0.1308\n",
            "Epoch [3/6] Step [680] Loss: 0.2131\n",
            "Epoch [3/6] Step [690] Loss: 0.0971\n",
            "Epoch [3/6] Step [700] Loss: 0.2803\n",
            "Epoch [3/6] Step [710] Loss: 0.2051\n",
            "Epoch [3/6] Step [720] Loss: 0.1642\n",
            "Epoch [3/6] Step [730] Loss: 0.5788\n",
            "Epoch [3/6] Step [740] Loss: 0.1205\n",
            "Epoch [3/6] Step [750] Loss: 0.1378\n",
            "Epoch [3/6] Step [760] Loss: 0.3900\n",
            "Epoch [3/6] Step [770] Loss: 0.1635\n",
            "Epoch [3/6] Step [780] Loss: 0.1577\n",
            "Epoch [3/6] Step [790] Loss: 0.1515\n",
            "Epoch [3/6] Step [800] Loss: 0.3122\n",
            "Epoch [3/6] Step [810] Loss: 0.2903\n",
            "Epoch [3/6] Step [820] Loss: 0.3650\n",
            "Epoch [3/6] Step [830] Loss: 0.1253\n",
            "Epoch [3/6] Step [840] Loss: 0.1643\n",
            "Epoch [3/6] Step [850] Loss: 0.2451\n",
            "Epoch [3/6] Step [860] Loss: 0.2988\n",
            "Epoch [3/6] Step [870] Loss: 0.4442\n",
            "Epoch [3/6] Step [880] Loss: 0.0788\n",
            "Epoch [3/6] Step [890] Loss: 0.4092\n",
            "Epoch [3/6] Step [900] Loss: 0.2900\n",
            "Epoch [3/6] Step [910] Loss: 0.4203\n",
            "Epoch [3/6] Step [920] Loss: 0.3451\n",
            "Epoch [3/6] Step [930] Loss: 0.3802\n",
            "Epoch [3/6] Step [940] Loss: 0.3811\n",
            "Epoch [3/6] Step [950] Loss: 0.1604\n",
            "Epoch [3/6] Step [960] Loss: 0.2395\n",
            "Epoch [3/6] Step [970] Loss: 0.3617\n",
            "Epoch [3/6] Step [980] Loss: 0.1961\n",
            "Epoch [3/6] Step [990] Loss: 0.3090\n",
            "Epoch [3/6] Step [1000] Loss: 0.6752\n",
            "Epoch [3/6] Step [1010] Loss: 0.2795\n",
            "Epoch [3/6] Step [1020] Loss: 0.1621\n",
            "Epoch [3/6] Step [1030] Loss: 0.3633\n",
            "Epoch [3/6] Step [1040] Loss: 0.2947\n",
            "Epoch [3/6] Step [1050] Loss: 0.8890\n",
            "Epoch [3/6] Step [1060] Loss: 0.2776\n",
            "Epoch [3/6] Step [1070] Loss: 0.1729\n",
            "Epoch [3/6] Step [1080] Loss: 0.1895\n",
            "Epoch [3/6] Step [1090] Loss: 0.4228\n",
            "Epoch [3/6] Step [1100] Loss: 0.4878\n",
            "Epoch [3/6] Step [1110] Loss: 0.2580\n",
            "Epoch [3/6] Step [1120] Loss: 0.3850\n",
            "Epoch [3/6] Step [1130] Loss: 0.2581\n",
            "Epoch [3/6] Step [1140] Loss: 0.2107\n",
            "Epoch [3/6] Step [1150] Loss: 0.6615\n",
            "Epoch [3/6] Step [1160] Loss: 0.4322\n",
            "Epoch [3/6] Step [1170] Loss: 0.2042\n",
            "Epoch [3/6] Step [1180] Loss: 0.3708\n",
            "Epoch [3/6] Step [1190] Loss: 0.2785\n",
            "Epoch [3/6] Step [1200] Loss: 0.2018\n",
            "Epoch [3/6] Step [1210] Loss: 0.1613\n",
            "Epoch [3/6] Step [1220] Loss: 0.2411\n",
            "Epoch [3/6] Step [1230] Loss: 0.3662\n",
            "Epoch [3/6] Step [1240] Loss: 0.2273\n",
            "Epoch [3/6] Step [1250] Loss: 0.2337\n",
            "Epoch [3/6] Step [1260] Loss: 0.2009\n",
            "Epoch [3/6] Step [1270] Loss: 0.4397\n",
            "Epoch [3/6] Step [1280] Loss: 0.1066\n",
            "Epoch [3/6] Step [1290] Loss: 0.3595\n",
            "Epoch [3/6] Step [1300] Loss: 0.1561\n",
            "Epoch [3/6] Step [1310] Loss: 0.4843\n",
            "Epoch [3/6] Step [1320] Loss: 0.2747\n",
            "Epoch [3/6] Step [1330] Loss: 0.2562\n",
            "Epoch [3/6] Step [1340] Loss: 0.2057\n",
            "Epoch [3/6] Step [1350] Loss: 0.1577\n",
            "Epoch [3/6] Step [1360] Loss: 0.2244\n",
            "Epoch [3/6] Step [1370] Loss: 0.2124\n",
            "Epoch [3/6] Step [1380] Loss: 0.2393\n",
            "Epoch [3/6] Step [1390] Loss: 0.3272\n",
            "Epoch [3/6] Step [1400] Loss: 0.1758\n",
            "Epoch [3/6] Step [1410] Loss: 0.1916\n",
            "Epoch [3/6] Step [1420] Loss: 0.2481\n",
            "Epoch [3/6] Step [1430] Loss: 0.6751\n",
            "Epoch [3/6] Step [1440] Loss: 0.1880\n",
            "Epoch [3/6] Step [1450] Loss: 0.2697\n",
            "Epoch [3/6] Step [1460] Loss: 0.5206\n",
            "Epoch [3/6] Step [1470] Loss: 0.2204\n",
            "Epoch [3/6] Step [1480] Loss: 0.1112\n",
            "Epoch [3/6] Step [1490] Loss: 0.3592\n",
            "Epoch [3/6] Step [1500] Loss: 0.6091\n",
            "Epoch [3/6] Step [1510] Loss: 0.2944\n",
            "Epoch [3/6] Step [1520] Loss: 0.1937\n",
            "Epoch [3/6] Step [1530] Loss: 0.2769\n",
            "Epoch [3/6] Step [1540] Loss: 0.2912\n",
            "Epoch [3/6] Step [1550] Loss: 0.1566\n",
            "Epoch [3/6] Step [1560] Loss: 0.1424\n",
            "Epoch [3/6] Step [1570] Loss: 0.2039\n",
            "Epoch [3/6] Step [1580] Loss: 0.1955\n",
            "Epoch [3/6] Step [1590] Loss: 0.1659\n",
            "Epoch [3/6] Step [1600] Loss: 0.2793\n",
            "Epoch [3/6] Step [1610] Loss: 0.2314\n",
            "Epoch [3/6] Step [1620] Loss: 0.1827\n",
            "Epoch [3/6] Step [1630] Loss: 0.2556\n",
            "Epoch [3/6] Step [1640] Loss: 0.2943\n",
            "Epoch [3/6] Step [1650] Loss: 0.4432\n",
            "Epoch [3/6] Step [1660] Loss: 0.3056\n",
            "Epoch [3/6] Step [1670] Loss: 0.2139\n",
            "Epoch [3/6] Step [1680] Loss: 0.1895\n",
            "Epoch [3/6] Step [1690] Loss: 0.2082\n",
            "Epoch [3/6] Step [1700] Loss: 0.1885\n",
            "Epoch [3/6] Step [1710] Loss: 0.3206\n",
            "Epoch [3/6] Step [1720] Loss: 0.1262\n",
            "Epoch [3/6] Step [1730] Loss: 0.3126\n",
            "Epoch [3/6] Step [1740] Loss: 0.2332\n",
            "Epoch [3/6] Step [1750] Loss: 0.3778\n",
            "Epoch [3/6] Step [1760] Loss: 0.1303\n",
            "Epoch [3/6] Step [1770] Loss: 0.4008\n",
            "Epoch [3/6] Step [1780] Loss: 0.4889\n",
            "Epoch [3/6] Step [1790] Loss: 0.2671\n",
            "Epoch [3/6] Step [1800] Loss: 0.2521\n",
            "Epoch [3/6] Step [1810] Loss: 0.2373\n",
            "Epoch [3/6] Step [1820] Loss: 0.0609\n",
            "Epoch [3/6] Step [1830] Loss: 0.2698\n",
            "Epoch [3/6] Step [1840] Loss: 0.3823\n",
            "Epoch [3/6] Step [1850] Loss: 0.2073\n",
            "Epoch [3/6] Step [1860] Loss: 0.7166\n",
            "Epoch [3/6] Step [1870] Loss: 0.2342\n",
            "Epoch [3/6] Step [1880] Loss: 0.3617\n",
            "Epoch [3/6] Step [1890] Loss: 0.4555\n",
            "Epoch [3/6] Step [1900] Loss: 0.2967\n",
            "Epoch [3/6] Step [1910] Loss: 0.3458\n",
            "Epoch [3/6] Step [1920] Loss: 0.1718\n",
            "Epoch [3/6] Step [1930] Loss: 0.1146\n",
            "Epoch [3/6] Step [1940] Loss: 0.2263\n",
            "Epoch [3/6] Step [1950] Loss: 0.1372\n",
            "Epoch [3/6] Step [1960] Loss: 0.3457\n",
            "Epoch [3/6] Step [1970] Loss: 0.3717\n",
            "Epoch [3/6] Step [1980] Loss: 0.1688\n",
            "Epoch [3/6] Step [1990] Loss: 0.1003\n",
            "Epoch [3/6] Step [2000] Loss: 0.2529\n",
            "Epoch [3/6] Step [2010] Loss: 0.4225\n",
            "Epoch [3/6] Step [2020] Loss: 0.3961\n",
            "Epoch [3/6] Step [2030] Loss: 0.2503\n",
            "Epoch [3/6] Step [2040] Loss: 0.2043\n",
            "Epoch [3/6] Step [2050] Loss: 0.2018\n",
            "Epoch [3/6] Step [2060] Loss: 0.3527\n",
            "Epoch [3/6] Step [2070] Loss: 0.2731\n",
            "Epoch [3/6] Step [2080] Loss: 0.2382\n",
            "Epoch [3/6] Step [2090] Loss: 0.3822\n",
            "Epoch [3/6] Step [2100] Loss: 0.1179\n",
            "Epoch [3/6] Step [2110] Loss: 0.3747\n",
            "Epoch [3/6] Step [2120] Loss: 0.1982\n",
            "Epoch [3/6] Step [2130] Loss: 0.2136\n",
            "Epoch [3/6] Step [2140] Loss: 0.1850\n",
            "Epoch [3/6] Step [2150] Loss: 0.2735\n",
            "Epoch [3/6] Step [2160] Loss: 0.3108\n",
            "Epoch [3/6] Step [2170] Loss: 0.1593\n",
            "Epoch [3/6] Step [2180] Loss: 0.3422\n",
            "Epoch [3/6] Step [2190] Loss: 0.2203\n",
            "Epoch [3/6] Step [2200] Loss: 0.2190\n",
            "Epoch [3/6] Step [2210] Loss: 0.4003\n",
            "Epoch 3: Validation Loss 0.4232, Accuracy 0.8496\n",
            "Epoch [4/6] Step [0] Loss: 0.1937\n",
            "Epoch [4/6] Step [10] Loss: 0.0720\n",
            "Epoch [4/6] Step [20] Loss: 0.4198\n",
            "Epoch [4/6] Step [30] Loss: 0.1376\n",
            "Epoch [4/6] Step [40] Loss: 0.2354\n",
            "Epoch [4/6] Step [50] Loss: 0.2327\n",
            "Epoch [4/6] Step [60] Loss: 0.3543\n",
            "Epoch [4/6] Step [70] Loss: 0.3661\n",
            "Epoch [4/6] Step [80] Loss: 0.2557\n",
            "Epoch [4/6] Step [90] Loss: 0.1522\n",
            "Epoch [4/6] Step [100] Loss: 0.3797\n",
            "Epoch [4/6] Step [110] Loss: 0.3777\n",
            "Epoch [4/6] Step [120] Loss: 0.3404\n",
            "Epoch [4/6] Step [130] Loss: 0.1753\n",
            "Epoch [4/6] Step [140] Loss: 0.2267\n",
            "Epoch [4/6] Step [150] Loss: 0.1703\n",
            "Epoch [4/6] Step [160] Loss: 0.4013\n",
            "Epoch [4/6] Step [170] Loss: 0.5642\n",
            "Epoch [4/6] Step [180] Loss: 0.2178\n",
            "Epoch [4/6] Step [190] Loss: 0.4361\n",
            "Epoch [4/6] Step [200] Loss: 0.2038\n",
            "Epoch [4/6] Step [210] Loss: 0.3991\n",
            "Epoch [4/6] Step [220] Loss: 0.2040\n",
            "Epoch [4/6] Step [230] Loss: 0.2988\n",
            "Epoch [4/6] Step [240] Loss: 0.0858\n",
            "Epoch [4/6] Step [250] Loss: 0.1742\n",
            "Epoch [4/6] Step [260] Loss: 0.1733\n",
            "Epoch [4/6] Step [270] Loss: 0.4277\n",
            "Epoch [4/6] Step [280] Loss: 0.3934\n",
            "Epoch [4/6] Step [290] Loss: 0.3858\n",
            "Epoch [4/6] Step [300] Loss: 0.3188\n",
            "Epoch [4/6] Step [310] Loss: 0.4188\n",
            "Epoch [4/6] Step [320] Loss: 0.1657\n",
            "Epoch [4/6] Step [330] Loss: 0.3834\n",
            "Epoch [4/6] Step [340] Loss: 0.2517\n",
            "Epoch [4/6] Step [350] Loss: 0.2647\n",
            "Epoch [4/6] Step [360] Loss: 0.2126\n",
            "Epoch [4/6] Step [370] Loss: 0.3708\n",
            "Epoch [4/6] Step [380] Loss: 0.3772\n",
            "Epoch [4/6] Step [390] Loss: 0.3320\n",
            "Epoch [4/6] Step [400] Loss: 0.2800\n",
            "Epoch [4/6] Step [410] Loss: 0.0831\n",
            "Epoch [4/6] Step [420] Loss: 0.1882\n",
            "Epoch [4/6] Step [430] Loss: 0.2644\n",
            "Epoch [4/6] Step [440] Loss: 0.0805\n",
            "Epoch [4/6] Step [450] Loss: 0.3726\n",
            "Epoch [4/6] Step [460] Loss: 0.2710\n",
            "Epoch [4/6] Step [470] Loss: 0.2852\n",
            "Epoch [4/6] Step [480] Loss: 0.4994\n",
            "Epoch [4/6] Step [490] Loss: 0.3934\n",
            "Epoch [4/6] Step [500] Loss: 0.3089\n",
            "Epoch [4/6] Step [510] Loss: 0.1176\n",
            "Epoch [4/6] Step [520] Loss: 0.2031\n",
            "Epoch [4/6] Step [530] Loss: 0.4515\n",
            "Epoch [4/6] Step [540] Loss: 0.2163\n",
            "Epoch [4/6] Step [550] Loss: 0.5740\n",
            "Epoch [4/6] Step [560] Loss: 0.3129\n",
            "Epoch [4/6] Step [570] Loss: 0.2424\n",
            "Epoch [4/6] Step [580] Loss: 0.3777\n",
            "Epoch [4/6] Step [590] Loss: 0.2226\n",
            "Epoch [4/6] Step [600] Loss: 0.1870\n",
            "Epoch [4/6] Step [610] Loss: 0.3809\n",
            "Epoch [4/6] Step [620] Loss: 0.2195\n",
            "Epoch [4/6] Step [630] Loss: 0.3085\n",
            "Epoch [4/6] Step [640] Loss: 0.2840\n",
            "Epoch [4/6] Step [650] Loss: 0.6339\n",
            "Epoch [4/6] Step [660] Loss: 0.1189\n",
            "Epoch [4/6] Step [670] Loss: 0.2957\n",
            "Epoch [4/6] Step [680] Loss: 0.4563\n",
            "Epoch [4/6] Step [690] Loss: 0.4040\n",
            "Epoch [4/6] Step [700] Loss: 0.3949\n",
            "Epoch [4/6] Step [710] Loss: 0.0960\n",
            "Epoch [4/6] Step [720] Loss: 0.3010\n",
            "Epoch [4/6] Step [730] Loss: 0.0810\n",
            "Epoch [4/6] Step [740] Loss: 0.1860\n",
            "Epoch [4/6] Step [750] Loss: 0.2928\n",
            "Epoch [4/6] Step [760] Loss: 0.2589\n",
            "Epoch [4/6] Step [770] Loss: 0.0335\n",
            "Epoch [4/6] Step [780] Loss: 0.1534\n",
            "Epoch [4/6] Step [790] Loss: 0.3267\n",
            "Epoch [4/6] Step [800] Loss: 0.3815\n",
            "Epoch [4/6] Step [810] Loss: 0.4156\n",
            "Epoch [4/6] Step [820] Loss: 0.1558\n",
            "Epoch [4/6] Step [830] Loss: 0.1959\n",
            "Epoch [4/6] Step [840] Loss: 0.4369\n",
            "Epoch [4/6] Step [850] Loss: 0.1353\n",
            "Epoch [4/6] Step [860] Loss: 0.2624\n",
            "Epoch [4/6] Step [870] Loss: 0.0573\n",
            "Epoch [4/6] Step [880] Loss: 0.2979\n",
            "Epoch [4/6] Step [890] Loss: 0.2946\n",
            "Epoch [4/6] Step [900] Loss: 0.1763\n",
            "Epoch [4/6] Step [910] Loss: 0.4455\n",
            "Epoch [4/6] Step [920] Loss: 0.2372\n",
            "Epoch [4/6] Step [930] Loss: 0.0579\n",
            "Epoch [4/6] Step [940] Loss: 0.2771\n",
            "Epoch [4/6] Step [950] Loss: 0.1761\n",
            "Epoch [4/6] Step [960] Loss: 0.2308\n",
            "Epoch [4/6] Step [970] Loss: 0.5619\n",
            "Epoch [4/6] Step [980] Loss: 0.4926\n",
            "Epoch [4/6] Step [990] Loss: 0.4729\n",
            "Epoch [4/6] Step [1000] Loss: 0.0880\n",
            "Epoch [4/6] Step [1010] Loss: 0.1626\n",
            "Epoch [4/6] Step [1020] Loss: 0.1417\n",
            "Epoch [4/6] Step [1030] Loss: 0.3162\n",
            "Epoch [4/6] Step [1040] Loss: 0.1728\n",
            "Epoch [4/6] Step [1050] Loss: 0.2883\n",
            "Epoch [4/6] Step [1060] Loss: 0.1847\n",
            "Epoch [4/6] Step [1070] Loss: 0.4507\n",
            "Epoch [4/6] Step [1080] Loss: 0.2082\n",
            "Epoch [4/6] Step [1090] Loss: 0.2200\n",
            "Epoch [4/6] Step [1100] Loss: 0.2715\n",
            "Epoch [4/6] Step [1110] Loss: 0.2781\n",
            "Epoch [4/6] Step [1120] Loss: 0.6392\n",
            "Epoch [4/6] Step [1130] Loss: 0.0818\n",
            "Epoch [4/6] Step [1140] Loss: 0.1914\n",
            "Epoch [4/6] Step [1150] Loss: 0.3667\n",
            "Epoch [4/6] Step [1160] Loss: 0.2052\n",
            "Epoch [4/6] Step [1170] Loss: 0.1379\n",
            "Epoch [4/6] Step [1180] Loss: 0.1516\n",
            "Epoch [4/6] Step [1190] Loss: 0.1708\n",
            "Epoch [4/6] Step [1200] Loss: 0.3494\n",
            "Epoch [4/6] Step [1210] Loss: 0.1508\n",
            "Epoch [4/6] Step [1220] Loss: 0.8259\n",
            "Epoch [4/6] Step [1230] Loss: 0.4642\n",
            "Epoch [4/6] Step [1240] Loss: 0.5366\n",
            "Epoch [4/6] Step [1250] Loss: 0.2892\n",
            "Epoch [4/6] Step [1260] Loss: 0.4113\n",
            "Epoch [4/6] Step [1270] Loss: 0.1533\n",
            "Epoch [4/6] Step [1280] Loss: 0.4271\n",
            "Epoch [4/6] Step [1290] Loss: 0.2792\n",
            "Epoch [4/6] Step [1300] Loss: 0.3808\n",
            "Epoch [4/6] Step [1310] Loss: 0.2460\n",
            "Epoch [4/6] Step [1320] Loss: 0.1050\n",
            "Epoch [4/6] Step [1330] Loss: 0.0582\n",
            "Epoch [4/6] Step [1340] Loss: 0.1393\n",
            "Epoch [4/6] Step [1350] Loss: 0.3421\n",
            "Epoch [4/6] Step [1360] Loss: 0.5258\n",
            "Epoch [4/6] Step [1370] Loss: 0.1801\n",
            "Epoch [4/6] Step [1380] Loss: 0.1802\n",
            "Epoch [4/6] Step [1390] Loss: 0.1673\n",
            "Epoch [4/6] Step [1400] Loss: 0.2882\n",
            "Epoch [4/6] Step [1410] Loss: 0.2094\n",
            "Epoch [4/6] Step [1420] Loss: 0.1851\n",
            "Epoch [4/6] Step [1430] Loss: 0.2155\n",
            "Epoch [4/6] Step [1440] Loss: 0.3733\n",
            "Epoch [4/6] Step [1450] Loss: 0.2001\n",
            "Epoch [4/6] Step [1460] Loss: 0.6466\n",
            "Epoch [4/6] Step [1470] Loss: 0.2797\n",
            "Epoch [4/6] Step [1480] Loss: 0.4485\n",
            "Epoch [4/6] Step [1490] Loss: 0.2647\n",
            "Epoch [4/6] Step [1500] Loss: 0.0984\n",
            "Epoch [4/6] Step [1510] Loss: 0.2493\n",
            "Epoch [4/6] Step [1520] Loss: 0.2579\n",
            "Epoch [4/6] Step [1530] Loss: 0.1080\n",
            "Epoch [4/6] Step [1540] Loss: 0.2843\n",
            "Epoch [4/6] Step [1550] Loss: 0.7206\n",
            "Epoch [4/6] Step [1560] Loss: 0.2978\n",
            "Epoch [4/6] Step [1570] Loss: 0.1013\n",
            "Epoch [4/6] Step [1580] Loss: 0.2542\n",
            "Epoch [4/6] Step [1590] Loss: 0.1267\n",
            "Epoch [4/6] Step [1600] Loss: 0.4108\n",
            "Epoch [4/6] Step [1610] Loss: 0.4620\n",
            "Epoch [4/6] Step [1620] Loss: 0.2811\n",
            "Epoch [4/6] Step [1630] Loss: 0.1739\n",
            "Epoch [4/6] Step [1640] Loss: 0.0970\n",
            "Epoch [4/6] Step [1650] Loss: 0.2057\n",
            "Epoch [4/6] Step [1660] Loss: 0.3817\n",
            "Epoch [4/6] Step [1670] Loss: 0.1441\n",
            "Epoch [4/6] Step [1680] Loss: 0.5302\n",
            "Epoch [4/6] Step [1690] Loss: 0.1545\n",
            "Epoch [4/6] Step [1700] Loss: 0.2219\n",
            "Epoch [4/6] Step [1710] Loss: 0.2592\n",
            "Epoch [4/6] Step [1720] Loss: 0.2838\n",
            "Epoch [4/6] Step [1730] Loss: 0.4219\n",
            "Epoch [4/6] Step [1740] Loss: 0.4691\n",
            "Epoch [4/6] Step [1750] Loss: 0.2787\n",
            "Epoch [4/6] Step [1760] Loss: 0.6338\n",
            "Epoch [4/6] Step [1770] Loss: 0.6347\n",
            "Epoch [4/6] Step [1780] Loss: 0.1096\n",
            "Epoch [4/6] Step [1790] Loss: 0.1512\n",
            "Epoch [4/6] Step [1800] Loss: 0.1694\n",
            "Epoch [4/6] Step [1810] Loss: 0.3188\n",
            "Epoch [4/6] Step [1820] Loss: 0.1826\n",
            "Epoch [4/6] Step [1830] Loss: 0.5371\n",
            "Epoch [4/6] Step [1840] Loss: 0.3718\n",
            "Epoch [4/6] Step [1850] Loss: 0.3611\n",
            "Epoch [4/6] Step [1860] Loss: 0.1782\n",
            "Epoch [4/6] Step [1870] Loss: 0.2349\n",
            "Epoch [4/6] Step [1880] Loss: 0.1023\n",
            "Epoch [4/6] Step [1890] Loss: 0.1414\n",
            "Epoch [4/6] Step [1900] Loss: 0.3073\n",
            "Epoch [4/6] Step [1910] Loss: 0.2702\n",
            "Epoch [4/6] Step [1920] Loss: 0.2211\n",
            "Epoch [4/6] Step [1930] Loss: 0.4912\n",
            "Epoch [4/6] Step [1940] Loss: 0.1953\n",
            "Epoch [4/6] Step [1950] Loss: 0.4410\n",
            "Epoch [4/6] Step [1960] Loss: 0.1926\n",
            "Epoch [4/6] Step [1970] Loss: 0.4342\n",
            "Epoch [4/6] Step [1980] Loss: 0.4233\n",
            "Epoch [4/6] Step [1990] Loss: 0.2385\n",
            "Epoch [4/6] Step [2000] Loss: 0.1487\n",
            "Epoch [4/6] Step [2010] Loss: 0.1955\n",
            "Epoch [4/6] Step [2020] Loss: 0.2287\n",
            "Epoch [4/6] Step [2030] Loss: 0.1685\n",
            "Epoch [4/6] Step [2040] Loss: 0.1539\n",
            "Epoch [4/6] Step [2050] Loss: 0.3908\n",
            "Epoch [4/6] Step [2060] Loss: 0.2534\n",
            "Epoch [4/6] Step [2070] Loss: 0.1159\n",
            "Epoch [4/6] Step [2080] Loss: 0.3700\n",
            "Epoch [4/6] Step [2090] Loss: 0.1336\n",
            "Epoch [4/6] Step [2100] Loss: 0.1236\n",
            "Epoch [4/6] Step [2110] Loss: 0.3644\n",
            "Epoch [4/6] Step [2120] Loss: 0.1884\n",
            "Epoch [4/6] Step [2130] Loss: 0.1980\n",
            "Epoch [4/6] Step [2140] Loss: 0.2820\n",
            "Epoch [4/6] Step [2150] Loss: 0.2498\n",
            "Epoch [4/6] Step [2160] Loss: 0.2206\n",
            "Epoch [4/6] Step [2170] Loss: 0.1172\n",
            "Epoch [4/6] Step [2180] Loss: 0.2061\n",
            "Epoch [4/6] Step [2190] Loss: 0.3326\n",
            "Epoch [4/6] Step [2200] Loss: 0.1984\n",
            "Epoch [4/6] Step [2210] Loss: 0.1923\n",
            "Epoch 4: Validation Loss 0.4204, Accuracy 0.8407\n",
            "Epoch [5/6] Step [0] Loss: 0.3854\n",
            "Epoch [5/6] Step [10] Loss: 0.1751\n",
            "Epoch [5/6] Step [20] Loss: 0.3621\n",
            "Epoch [5/6] Step [30] Loss: 0.1313\n",
            "Epoch [5/6] Step [40] Loss: 0.1960\n",
            "Epoch [5/6] Step [50] Loss: 0.3179\n",
            "Epoch [5/6] Step [60] Loss: 0.1974\n",
            "Epoch [5/6] Step [70] Loss: 0.1963\n",
            "Epoch [5/6] Step [80] Loss: 0.1872\n",
            "Epoch [5/6] Step [90] Loss: 0.3306\n",
            "Epoch [5/6] Step [100] Loss: 0.1141\n",
            "Epoch [5/6] Step [110] Loss: 0.1401\n",
            "Epoch [5/6] Step [120] Loss: 0.1688\n",
            "Epoch [5/6] Step [130] Loss: 0.1715\n",
            "Epoch [5/6] Step [140] Loss: 0.1847\n",
            "Epoch [5/6] Step [150] Loss: 0.1388\n",
            "Epoch [5/6] Step [160] Loss: 0.1358\n",
            "Epoch [5/6] Step [170] Loss: 0.0683\n",
            "Epoch [5/6] Step [180] Loss: 0.1592\n",
            "Epoch [5/6] Step [190] Loss: 0.2799\n",
            "Epoch [5/6] Step [200] Loss: 0.3044\n",
            "Epoch [5/6] Step [210] Loss: 0.3388\n",
            "Epoch [5/6] Step [220] Loss: 0.1253\n",
            "Epoch [5/6] Step [230] Loss: 0.2386\n",
            "Epoch [5/6] Step [240] Loss: 0.1099\n",
            "Epoch [5/6] Step [250] Loss: 0.4901\n",
            "Epoch [5/6] Step [260] Loss: 0.1839\n",
            "Epoch [5/6] Step [270] Loss: 0.1698\n",
            "Epoch [5/6] Step [280] Loss: 0.2922\n",
            "Epoch [5/6] Step [290] Loss: 0.2656\n",
            "Epoch [5/6] Step [300] Loss: 0.1944\n",
            "Epoch [5/6] Step [310] Loss: 0.2802\n",
            "Epoch [5/6] Step [320] Loss: 0.3650\n",
            "Epoch [5/6] Step [330] Loss: 0.1077\n",
            "Epoch [5/6] Step [340] Loss: 0.2001\n",
            "Epoch [5/6] Step [350] Loss: 0.0648\n",
            "Epoch [5/6] Step [360] Loss: 0.1087\n",
            "Epoch [5/6] Step [370] Loss: 0.1235\n",
            "Epoch [5/6] Step [380] Loss: 0.1204\n",
            "Epoch [5/6] Step [390] Loss: 0.1334\n",
            "Epoch [5/6] Step [400] Loss: 0.1306\n",
            "Epoch [5/6] Step [410] Loss: 0.4835\n",
            "Epoch [5/6] Step [420] Loss: 0.0341\n",
            "Epoch [5/6] Step [430] Loss: 0.1293\n",
            "Epoch [5/6] Step [440] Loss: 0.2319\n",
            "Epoch [5/6] Step [450] Loss: 0.4482\n",
            "Epoch [5/6] Step [460] Loss: 0.2312\n",
            "Epoch [5/6] Step [470] Loss: 0.1377\n",
            "Epoch [5/6] Step [480] Loss: 0.2682\n",
            "Epoch [5/6] Step [490] Loss: 0.4317\n",
            "Epoch [5/6] Step [500] Loss: 0.0704\n",
            "Epoch [5/6] Step [510] Loss: 0.2413\n",
            "Epoch [5/6] Step [520] Loss: 0.3170\n",
            "Epoch [5/6] Step [530] Loss: 0.1152\n",
            "Epoch [5/6] Step [540] Loss: 0.2488\n",
            "Epoch [5/6] Step [550] Loss: 0.3102\n",
            "Epoch [5/6] Step [560] Loss: 0.2981\n",
            "Epoch [5/6] Step [570] Loss: 0.1912\n",
            "Epoch [5/6] Step [580] Loss: 0.1252\n",
            "Epoch [5/6] Step [590] Loss: 0.1272\n",
            "Epoch [5/6] Step [600] Loss: 0.2870\n",
            "Epoch [5/6] Step [610] Loss: 0.2393\n",
            "Epoch [5/6] Step [620] Loss: 0.0935\n",
            "Epoch [5/6] Step [630] Loss: 0.1485\n",
            "Epoch [5/6] Step [640] Loss: 0.2159\n",
            "Epoch [5/6] Step [650] Loss: 0.4080\n",
            "Epoch [5/6] Step [660] Loss: 0.3036\n",
            "Epoch [5/6] Step [670] Loss: 0.3009\n",
            "Epoch [5/6] Step [680] Loss: 0.1623\n",
            "Epoch [5/6] Step [690] Loss: 0.1395\n",
            "Epoch [5/6] Step [700] Loss: 0.2338\n",
            "Epoch [5/6] Step [710] Loss: 0.5544\n",
            "Epoch [5/6] Step [720] Loss: 0.1492\n",
            "Epoch [5/6] Step [730] Loss: 0.2970\n",
            "Epoch [5/6] Step [740] Loss: 0.3454\n",
            "Epoch [5/6] Step [750] Loss: 0.2903\n",
            "Epoch [5/6] Step [760] Loss: 0.2117\n",
            "Epoch [5/6] Step [770] Loss: 0.1608\n",
            "Epoch [5/6] Step [780] Loss: 0.1889\n",
            "Epoch [5/6] Step [790] Loss: 0.2012\n",
            "Epoch [5/6] Step [800] Loss: 0.1602\n",
            "Epoch [5/6] Step [810] Loss: 0.1820\n",
            "Epoch [5/6] Step [820] Loss: 0.1916\n",
            "Epoch [5/6] Step [830] Loss: 0.1429\n",
            "Epoch [5/6] Step [840] Loss: 0.1248\n",
            "Epoch [5/6] Step [850] Loss: 0.1099\n",
            "Epoch [5/6] Step [860] Loss: 0.3050\n",
            "Epoch [5/6] Step [870] Loss: 0.0780\n",
            "Epoch [5/6] Step [880] Loss: 0.5021\n",
            "Epoch [5/6] Step [890] Loss: 0.3368\n",
            "Epoch [5/6] Step [900] Loss: 0.3779\n",
            "Epoch [5/6] Step [910] Loss: 0.2839\n",
            "Epoch [5/6] Step [920] Loss: 0.3007\n",
            "Epoch [5/6] Step [930] Loss: 0.2630\n",
            "Epoch [5/6] Step [940] Loss: 0.1755\n",
            "Epoch [5/6] Step [950] Loss: 0.0925\n",
            "Epoch [5/6] Step [960] Loss: 0.1890\n",
            "Epoch [5/6] Step [970] Loss: 0.1307\n",
            "Epoch [5/6] Step [980] Loss: 0.2075\n",
            "Epoch [5/6] Step [990] Loss: 0.1218\n",
            "Epoch [5/6] Step [1000] Loss: 0.2819\n",
            "Epoch [5/6] Step [1010] Loss: 0.2323\n",
            "Epoch [5/6] Step [1020] Loss: 0.2815\n",
            "Epoch [5/6] Step [1030] Loss: 0.1198\n",
            "Epoch [5/6] Step [1040] Loss: 0.2282\n",
            "Epoch [5/6] Step [1050] Loss: 0.3456\n",
            "Epoch [5/6] Step [1060] Loss: 0.2135\n",
            "Epoch [5/6] Step [1070] Loss: 0.4535\n",
            "Epoch [5/6] Step [1080] Loss: 0.0878\n",
            "Epoch [5/6] Step [1090] Loss: 0.4205\n",
            "Epoch [5/6] Step [1100] Loss: 0.2080\n",
            "Epoch [5/6] Step [1110] Loss: 0.1928\n",
            "Epoch [5/6] Step [1120] Loss: 0.3417\n",
            "Epoch [5/6] Step [1130] Loss: 0.1985\n",
            "Epoch [5/6] Step [1140] Loss: 0.1829\n",
            "Epoch [5/6] Step [1150] Loss: 0.2439\n",
            "Epoch [5/6] Step [1160] Loss: 0.2615\n",
            "Epoch [5/6] Step [1170] Loss: 0.1737\n",
            "Epoch [5/6] Step [1180] Loss: 0.3296\n",
            "Epoch [5/6] Step [1190] Loss: 0.2679\n",
            "Epoch [5/6] Step [1200] Loss: 0.1599\n",
            "Epoch [5/6] Step [1210] Loss: 0.3692\n",
            "Epoch [5/6] Step [1220] Loss: 0.2116\n",
            "Epoch [5/6] Step [1230] Loss: 0.1479\n",
            "Epoch [5/6] Step [1240] Loss: 0.1427\n",
            "Epoch [5/6] Step [1250] Loss: 0.2168\n",
            "Epoch [5/6] Step [1260] Loss: 0.2403\n",
            "Epoch [5/6] Step [1270] Loss: 0.2543\n",
            "Epoch [5/6] Step [1280] Loss: 0.1285\n",
            "Epoch [5/6] Step [1290] Loss: 0.0549\n",
            "Epoch [5/6] Step [1300] Loss: 0.1960\n",
            "Epoch [5/6] Step [1310] Loss: 0.1195\n",
            "Epoch [5/6] Step [1320] Loss: 0.3028\n",
            "Epoch [5/6] Step [1330] Loss: 0.3845\n",
            "Epoch [5/6] Step [1340] Loss: 0.0814\n",
            "Epoch [5/6] Step [1350] Loss: 0.2951\n",
            "Epoch [5/6] Step [1360] Loss: 0.3243\n",
            "Epoch [5/6] Step [1370] Loss: 0.1734\n",
            "Epoch [5/6] Step [1380] Loss: 0.1888\n",
            "Epoch [5/6] Step [1390] Loss: 0.5973\n",
            "Epoch [5/6] Step [1400] Loss: 0.1098\n",
            "Epoch [5/6] Step [1410] Loss: 0.1441\n",
            "Epoch [5/6] Step [1420] Loss: 0.1109\n",
            "Epoch [5/6] Step [1430] Loss: 0.5163\n",
            "Epoch [5/6] Step [1440] Loss: 0.3436\n",
            "Epoch [5/6] Step [1450] Loss: 0.0360\n",
            "Epoch [5/6] Step [1460] Loss: 0.1255\n",
            "Epoch [5/6] Step [1470] Loss: 0.2566\n",
            "Epoch [5/6] Step [1480] Loss: 0.1568\n",
            "Epoch [5/6] Step [1490] Loss: 0.3405\n",
            "Epoch [5/6] Step [1500] Loss: 0.2321\n",
            "Epoch [5/6] Step [1510] Loss: 0.5978\n",
            "Epoch [5/6] Step [1520] Loss: 0.2627\n",
            "Epoch [5/6] Step [1530] Loss: 0.1800\n",
            "Epoch [5/6] Step [1540] Loss: 0.0776\n",
            "Epoch [5/6] Step [1550] Loss: 0.3401\n",
            "Epoch [5/6] Step [1560] Loss: 0.1884\n",
            "Epoch [5/6] Step [1570] Loss: 0.2518\n",
            "Epoch [5/6] Step [1580] Loss: 0.3431\n",
            "Epoch [5/6] Step [1590] Loss: 0.2150\n",
            "Epoch [5/6] Step [1600] Loss: 0.0717\n",
            "Epoch [5/6] Step [1610] Loss: 0.1991\n",
            "Epoch [5/6] Step [1620] Loss: 0.1546\n",
            "Epoch [5/6] Step [1630] Loss: 0.1370\n",
            "Epoch [5/6] Step [1640] Loss: 0.2396\n",
            "Epoch [5/6] Step [1650] Loss: 0.2361\n",
            "Epoch [5/6] Step [1660] Loss: 0.1034\n",
            "Epoch [5/6] Step [1670] Loss: 0.1520\n",
            "Epoch [5/6] Step [1680] Loss: 0.2641\n",
            "Epoch [5/6] Step [1690] Loss: 0.1884\n",
            "Epoch [5/6] Step [1700] Loss: 0.3708\n",
            "Epoch [5/6] Step [1710] Loss: 0.5440\n",
            "Epoch [5/6] Step [1720] Loss: 0.2307\n",
            "Epoch [5/6] Step [1730] Loss: 0.1102\n",
            "Epoch [5/6] Step [1740] Loss: 0.1951\n",
            "Epoch [5/6] Step [1750] Loss: 0.3120\n",
            "Epoch [5/6] Step [1760] Loss: 0.1255\n",
            "Epoch [5/6] Step [1770] Loss: 0.3084\n",
            "Epoch [5/6] Step [1780] Loss: 0.0888\n",
            "Epoch [5/6] Step [1790] Loss: 0.0774\n",
            "Epoch [5/6] Step [1800] Loss: 0.2623\n",
            "Epoch [5/6] Step [1810] Loss: 0.1541\n",
            "Epoch [5/6] Step [1820] Loss: 0.1974\n",
            "Epoch [5/6] Step [1830] Loss: 0.2016\n",
            "Epoch [5/6] Step [1840] Loss: 0.0682\n",
            "Epoch [5/6] Step [1850] Loss: 0.1373\n",
            "Epoch [5/6] Step [1860] Loss: 0.3137\n",
            "Epoch [5/6] Step [1870] Loss: 0.2013\n",
            "Epoch [5/6] Step [1880] Loss: 0.1718\n",
            "Epoch [5/6] Step [1890] Loss: 0.1152\n",
            "Epoch [5/6] Step [1900] Loss: 0.1037\n",
            "Epoch [5/6] Step [1910] Loss: 0.2948\n",
            "Epoch [5/6] Step [1920] Loss: 0.1014\n",
            "Epoch [5/6] Step [1930] Loss: 0.0786\n",
            "Epoch [5/6] Step [1940] Loss: 0.3058\n",
            "Epoch [5/6] Step [1950] Loss: 0.3461\n",
            "Epoch [5/6] Step [1960] Loss: 0.3285\n",
            "Epoch [5/6] Step [1970] Loss: 0.1039\n",
            "Epoch [5/6] Step [1980] Loss: 0.2920\n",
            "Epoch [5/6] Step [1990] Loss: 0.2573\n",
            "Epoch [5/6] Step [2000] Loss: 0.2492\n",
            "Epoch [5/6] Step [2010] Loss: 0.1863\n",
            "Epoch [5/6] Step [2020] Loss: 0.1011\n",
            "Epoch [5/6] Step [2030] Loss: 0.2815\n",
            "Epoch [5/6] Step [2040] Loss: 0.4956\n",
            "Epoch [5/6] Step [2050] Loss: 0.1559\n",
            "Epoch [5/6] Step [2060] Loss: 0.1403\n",
            "Epoch [5/6] Step [2070] Loss: 0.1821\n",
            "Epoch [5/6] Step [2080] Loss: 0.2799\n",
            "Epoch [5/6] Step [2090] Loss: 0.4038\n",
            "Epoch [5/6] Step [2100] Loss: 0.2026\n",
            "Epoch [5/6] Step [2110] Loss: 0.3607\n",
            "Epoch [5/6] Step [2120] Loss: 0.1794\n",
            "Epoch [5/6] Step [2130] Loss: 0.1946\n",
            "Epoch [5/6] Step [2140] Loss: 0.3435\n",
            "Epoch [5/6] Step [2150] Loss: 0.1431\n",
            "Epoch [5/6] Step [2160] Loss: 0.1707\n",
            "Epoch [5/6] Step [2170] Loss: 0.1452\n",
            "Epoch [5/6] Step [2180] Loss: 0.0483\n",
            "Epoch [5/6] Step [2190] Loss: 0.3110\n",
            "Epoch [5/6] Step [2200] Loss: 0.1652\n",
            "Epoch [5/6] Step [2210] Loss: 0.0854\n",
            "Epoch 5: Validation Loss 0.4786, Accuracy 0.7973\n",
            "Epoch [6/6] Step [0] Loss: 0.0702\n",
            "Epoch [6/6] Step [10] Loss: 0.5091\n",
            "Epoch [6/6] Step [20] Loss: 0.1075\n",
            "Epoch [6/6] Step [30] Loss: 0.1042\n",
            "Epoch [6/6] Step [40] Loss: 0.2282\n",
            "Epoch [6/6] Step [50] Loss: 0.1643\n",
            "Epoch [6/6] Step [60] Loss: 0.1363\n",
            "Epoch [6/6] Step [70] Loss: 0.1882\n",
            "Epoch [6/6] Step [80] Loss: 0.4256\n",
            "Epoch [6/6] Step [90] Loss: 0.3075\n",
            "Epoch [6/6] Step [100] Loss: 0.0621\n",
            "Epoch [6/6] Step [110] Loss: 0.1315\n",
            "Epoch [6/6] Step [120] Loss: 0.3970\n",
            "Epoch [6/6] Step [130] Loss: 0.2536\n",
            "Epoch [6/6] Step [140] Loss: 0.0237\n",
            "Epoch [6/6] Step [150] Loss: 0.1496\n",
            "Epoch [6/6] Step [160] Loss: 0.0389\n",
            "Epoch [6/6] Step [170] Loss: 0.2151\n",
            "Epoch [6/6] Step [180] Loss: 0.1390\n",
            "Epoch [6/6] Step [190] Loss: 0.3707\n",
            "Epoch [6/6] Step [200] Loss: 0.2942\n",
            "Epoch [6/6] Step [210] Loss: 0.2661\n",
            "Epoch [6/6] Step [220] Loss: 0.4461\n",
            "Epoch [6/6] Step [230] Loss: 0.3277\n",
            "Epoch [6/6] Step [240] Loss: 0.3669\n",
            "Epoch [6/6] Step [250] Loss: 0.2008\n",
            "Epoch [6/6] Step [260] Loss: 0.2845\n",
            "Epoch [6/6] Step [270] Loss: 0.4331\n",
            "Epoch [6/6] Step [280] Loss: 0.0752\n",
            "Epoch [6/6] Step [290] Loss: 0.3219\n",
            "Epoch [6/6] Step [300] Loss: 0.3226\n",
            "Epoch [6/6] Step [310] Loss: 0.2666\n",
            "Epoch [6/6] Step [320] Loss: 0.3793\n",
            "Epoch [6/6] Step [330] Loss: 0.3162\n",
            "Epoch [6/6] Step [340] Loss: 0.0927\n",
            "Epoch [6/6] Step [350] Loss: 0.2039\n",
            "Epoch [6/6] Step [360] Loss: 0.0552\n",
            "Epoch [6/6] Step [370] Loss: 0.3214\n",
            "Epoch [6/6] Step [380] Loss: 0.3194\n",
            "Epoch [6/6] Step [390] Loss: 0.3562\n",
            "Epoch [6/6] Step [400] Loss: 0.1506\n",
            "Epoch [6/6] Step [410] Loss: 0.1233\n",
            "Epoch [6/6] Step [420] Loss: 0.3712\n",
            "Epoch [6/6] Step [430] Loss: 0.2124\n",
            "Epoch [6/6] Step [440] Loss: 0.1576\n",
            "Epoch [6/6] Step [450] Loss: 0.2469\n",
            "Epoch [6/6] Step [460] Loss: 0.1279\n",
            "Epoch [6/6] Step [470] Loss: 0.0592\n",
            "Epoch [6/6] Step [480] Loss: 0.1160\n",
            "Epoch [6/6] Step [490] Loss: 0.2086\n",
            "Epoch [6/6] Step [500] Loss: 0.2585\n",
            "Epoch [6/6] Step [510] Loss: 0.1814\n",
            "Epoch [6/6] Step [520] Loss: 0.2137\n",
            "Epoch [6/6] Step [530] Loss: 0.2962\n",
            "Epoch [6/6] Step [540] Loss: 0.1452\n",
            "Epoch [6/6] Step [550] Loss: 0.2342\n",
            "Epoch [6/6] Step [560] Loss: 0.1577\n",
            "Epoch [6/6] Step [570] Loss: 0.2039\n",
            "Epoch [6/6] Step [580] Loss: 0.1569\n",
            "Epoch [6/6] Step [590] Loss: 0.3312\n",
            "Epoch [6/6] Step [600] Loss: 0.2251\n",
            "Epoch [6/6] Step [610] Loss: 0.4121\n",
            "Epoch [6/6] Step [620] Loss: 0.1661\n",
            "Epoch [6/6] Step [630] Loss: 0.1541\n",
            "Epoch [6/6] Step [640] Loss: 0.1199\n",
            "Epoch [6/6] Step [650] Loss: 0.5156\n",
            "Epoch [6/6] Step [660] Loss: 0.1546\n",
            "Epoch [6/6] Step [670] Loss: 0.0767\n",
            "Epoch [6/6] Step [680] Loss: 0.3205\n",
            "Epoch [6/6] Step [690] Loss: 0.1167\n",
            "Epoch [6/6] Step [700] Loss: 0.1856\n",
            "Epoch [6/6] Step [710] Loss: 0.2485\n",
            "Epoch [6/6] Step [720] Loss: 0.1883\n",
            "Epoch [6/6] Step [730] Loss: 0.0739\n",
            "Epoch [6/6] Step [740] Loss: 0.0494\n",
            "Epoch [6/6] Step [750] Loss: 0.1212\n",
            "Epoch [6/6] Step [760] Loss: 0.1061\n",
            "Epoch [6/6] Step [770] Loss: 0.1446\n",
            "Epoch [6/6] Step [780] Loss: 0.3688\n",
            "Epoch [6/6] Step [790] Loss: 0.1693\n",
            "Epoch [6/6] Step [800] Loss: 0.0932\n",
            "Epoch [6/6] Step [810] Loss: 0.0817\n",
            "Epoch [6/6] Step [820] Loss: 0.3554\n",
            "Epoch [6/6] Step [830] Loss: 0.3008\n",
            "Epoch [6/6] Step [840] Loss: 0.0918\n",
            "Epoch [6/6] Step [850] Loss: 0.0801\n",
            "Epoch [6/6] Step [860] Loss: 0.1122\n",
            "Epoch [6/6] Step [870] Loss: 0.3495\n",
            "Epoch [6/6] Step [880] Loss: 0.1195\n",
            "Epoch [6/6] Step [890] Loss: 0.1745\n",
            "Epoch [6/6] Step [900] Loss: 0.2471\n",
            "Epoch [6/6] Step [910] Loss: 0.0112\n",
            "Epoch [6/6] Step [920] Loss: 0.2122\n",
            "Epoch [6/6] Step [930] Loss: 0.1579\n",
            "Epoch [6/6] Step [940] Loss: 0.3276\n",
            "Epoch [6/6] Step [950] Loss: 0.2650\n",
            "Epoch [6/6] Step [960] Loss: 0.1563\n",
            "Epoch [6/6] Step [970] Loss: 0.4501\n",
            "Epoch [6/6] Step [980] Loss: 0.1419\n",
            "Epoch [6/6] Step [990] Loss: 0.3197\n",
            "Epoch [6/6] Step [1000] Loss: 0.3851\n",
            "Epoch [6/6] Step [1010] Loss: 0.2905\n",
            "Epoch [6/6] Step [1020] Loss: 0.3290\n",
            "Epoch [6/6] Step [1030] Loss: 0.4257\n",
            "Epoch [6/6] Step [1040] Loss: 0.1837\n",
            "Epoch [6/6] Step [1050] Loss: 0.1902\n",
            "Epoch [6/6] Step [1060] Loss: 0.2011\n",
            "Epoch [6/6] Step [1070] Loss: 0.1380\n",
            "Epoch [6/6] Step [1080] Loss: 0.2797\n",
            "Epoch [6/6] Step [1090] Loss: 0.1904\n",
            "Epoch [6/6] Step [1100] Loss: 0.0868\n",
            "Epoch [6/6] Step [1110] Loss: 0.2350\n",
            "Epoch [6/6] Step [1120] Loss: 0.1203\n",
            "Epoch [6/6] Step [1130] Loss: 0.2241\n",
            "Epoch [6/6] Step [1140] Loss: 0.2156\n",
            "Epoch [6/6] Step [1150] Loss: 0.1012\n",
            "Epoch [6/6] Step [1160] Loss: 0.1767\n",
            "Epoch [6/6] Step [1170] Loss: 0.2200\n",
            "Epoch [6/6] Step [1180] Loss: 0.1604\n",
            "Epoch [6/6] Step [1190] Loss: 0.0610\n",
            "Epoch [6/6] Step [1200] Loss: 0.3981\n",
            "Epoch [6/6] Step [1210] Loss: 0.2162\n",
            "Epoch [6/6] Step [1220] Loss: 0.2716\n",
            "Epoch [6/6] Step [1230] Loss: 0.4661\n",
            "Epoch [6/6] Step [1240] Loss: 0.0766\n",
            "Epoch [6/6] Step [1250] Loss: 0.0980\n",
            "Epoch [6/6] Step [1260] Loss: 0.1231\n",
            "Epoch [6/6] Step [1270] Loss: 0.0499\n",
            "Epoch [6/6] Step [1280] Loss: 0.0850\n",
            "Epoch [6/6] Step [1290] Loss: 0.0380\n",
            "Epoch [6/6] Step [1300] Loss: 0.2539\n",
            "Epoch [6/6] Step [1310] Loss: 0.3268\n",
            "Epoch [6/6] Step [1320] Loss: 0.2587\n",
            "Epoch [6/6] Step [1330] Loss: 0.0996\n",
            "Epoch [6/6] Step [1340] Loss: 0.2995\n",
            "Epoch [6/6] Step [1350] Loss: 0.1894\n",
            "Epoch [6/6] Step [1360] Loss: 0.5006\n",
            "Epoch [6/6] Step [1370] Loss: 0.2265\n",
            "Epoch [6/6] Step [1380] Loss: 0.2121\n",
            "Epoch [6/6] Step [1390] Loss: 0.3709\n",
            "Epoch [6/6] Step [1400] Loss: 0.1871\n",
            "Epoch [6/6] Step [1410] Loss: 0.1511\n",
            "Epoch [6/6] Step [1420] Loss: 0.2898\n",
            "Epoch [6/6] Step [1430] Loss: 0.2254\n",
            "Epoch [6/6] Step [1440] Loss: 0.2860\n",
            "Epoch [6/6] Step [1450] Loss: 0.0403\n",
            "Epoch [6/6] Step [1460] Loss: 0.1815\n",
            "Epoch [6/6] Step [1470] Loss: 0.1575\n",
            "Epoch [6/6] Step [1480] Loss: 0.0990\n",
            "Epoch [6/6] Step [1490] Loss: 0.3099\n",
            "Epoch [6/6] Step [1500] Loss: 0.2205\n",
            "Epoch [6/6] Step [1510] Loss: 0.1405\n",
            "Epoch [6/6] Step [1520] Loss: 0.0740\n",
            "Epoch [6/6] Step [1530] Loss: 0.1885\n",
            "Epoch [6/6] Step [1540] Loss: 0.5868\n",
            "Epoch [6/6] Step [1550] Loss: 0.3209\n",
            "Epoch [6/6] Step [1560] Loss: 0.3508\n",
            "Epoch [6/6] Step [1570] Loss: 0.1456\n",
            "Epoch [6/6] Step [1580] Loss: 0.0110\n",
            "Epoch [6/6] Step [1590] Loss: 0.2475\n",
            "Epoch [6/6] Step [1600] Loss: 0.2777\n",
            "Epoch [6/6] Step [1610] Loss: 0.3855\n",
            "Epoch [6/6] Step [1620] Loss: 0.2349\n",
            "Epoch [6/6] Step [1630] Loss: 0.0505\n",
            "Epoch [6/6] Step [1640] Loss: 0.0421\n",
            "Epoch [6/6] Step [1650] Loss: 0.2001\n",
            "Epoch [6/6] Step [1660] Loss: 0.1805\n",
            "Epoch [6/6] Step [1670] Loss: 0.0608\n",
            "Epoch [6/6] Step [1680] Loss: 0.2667\n",
            "Epoch [6/6] Step [1690] Loss: 0.1091\n",
            "Epoch [6/6] Step [1700] Loss: 0.1497\n",
            "Epoch [6/6] Step [1710] Loss: 0.1181\n",
            "Epoch [6/6] Step [1720] Loss: 0.1892\n",
            "Epoch [6/6] Step [1730] Loss: 0.1796\n",
            "Epoch [6/6] Step [1740] Loss: 0.0746\n",
            "Epoch [6/6] Step [1750] Loss: 0.1215\n",
            "Epoch [6/6] Step [1760] Loss: 0.1895\n",
            "Epoch [6/6] Step [1770] Loss: 0.0775\n",
            "Epoch [6/6] Step [1780] Loss: 0.0730\n",
            "Epoch [6/6] Step [1790] Loss: 0.1085\n",
            "Epoch [6/6] Step [1800] Loss: 0.5073\n",
            "Epoch [6/6] Step [1810] Loss: 0.1636\n",
            "Epoch [6/6] Step [1820] Loss: 0.3530\n",
            "Epoch [6/6] Step [1830] Loss: 0.0353\n",
            "Epoch [6/6] Step [1840] Loss: 0.1698\n",
            "Epoch [6/6] Step [1850] Loss: 0.0630\n",
            "Epoch [6/6] Step [1860] Loss: 0.3741\n",
            "Epoch [6/6] Step [1870] Loss: 0.4063\n",
            "Epoch [6/6] Step [1880] Loss: 0.3079\n",
            "Epoch [6/6] Step [1890] Loss: 0.0961\n",
            "Epoch [6/6] Step [1900] Loss: 0.0669\n",
            "Epoch [6/6] Step [1910] Loss: 0.1040\n",
            "Epoch [6/6] Step [1920] Loss: 0.0776\n",
            "Epoch [6/6] Step [1930] Loss: 0.2681\n",
            "Epoch [6/6] Step [1940] Loss: 0.1213\n",
            "Epoch [6/6] Step [1950] Loss: 0.2019\n",
            "Epoch [6/6] Step [1960] Loss: 0.2752\n",
            "Epoch [6/6] Step [1970] Loss: 0.1834\n",
            "Epoch [6/6] Step [1980] Loss: 0.1569\n",
            "Epoch [6/6] Step [1990] Loss: 0.0831\n",
            "Epoch [6/6] Step [2000] Loss: 0.2290\n",
            "Epoch [6/6] Step [2010] Loss: 0.2825\n",
            "Epoch [6/6] Step [2020] Loss: 0.2260\n",
            "Epoch [6/6] Step [2030] Loss: 0.1550\n",
            "Epoch [6/6] Step [2040] Loss: 0.2368\n",
            "Epoch [6/6] Step [2050] Loss: 0.3634\n",
            "Epoch [6/6] Step [2060] Loss: 0.1468\n",
            "Epoch [6/6] Step [2070] Loss: 0.1395\n",
            "Epoch [6/6] Step [2080] Loss: 0.3049\n",
            "Epoch [6/6] Step [2090] Loss: 0.4749\n",
            "Epoch [6/6] Step [2100] Loss: 0.2151\n",
            "Epoch [6/6] Step [2110] Loss: 0.4060\n",
            "Epoch [6/6] Step [2120] Loss: 0.1405\n",
            "Epoch [6/6] Step [2130] Loss: 0.1628\n",
            "Epoch [6/6] Step [2140] Loss: 0.1639\n",
            "Epoch [6/6] Step [2150] Loss: 0.3363\n",
            "Epoch [6/6] Step [2160] Loss: 0.1369\n",
            "Epoch [6/6] Step [2170] Loss: 0.1087\n",
            "Epoch [6/6] Step [2180] Loss: 0.1549\n",
            "Epoch [6/6] Step [2190] Loss: 0.0683\n",
            "Epoch [6/6] Step [2200] Loss: 0.1701\n",
            "Epoch [6/6] Step [2210] Loss: 0.4057\n",
            "Epoch 6: Validation Loss 0.4813, Accuracy 0.8519\n",
            "Prediction size: torch.Size([16, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mode auf train setzen\n",
        "lits_dataset.changemode(\"train\")\n",
        "\n",
        "# ZufÃ¤lliges Bild aus dem Dataset\n",
        "import random\n",
        "idx = random.randint(0, len(lits_dataset)-1)\n",
        "img, target = lits_dataset[idx]\n",
        "\n",
        "# Batch-Dimension hinzufÃ¼gen\n",
        "img = img.unsqueeze(0).to(device)  # shape [1, 1, H, W]\n",
        "\n",
        "model.eval()  # eval mode, keine Gradienten\n",
        "with torch.no_grad():\n",
        "    pred = model(img)\n",
        "    pred_class = torch.argmax(pred, dim=1).item()\n",
        "\n",
        "print(f\"Echter Label: {target.item()}, Vorhersage: {pred_class}\")\n"
      ],
      "metadata": {
        "id": "BTe9Er-rkZvz",
        "outputId": "a17c4459-dbbd-42de-b0fe-61e4c7c42190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BTe9Er-rkZvz",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Echter Label: 0, Vorhersage: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zitYaPI6oAKw"
      },
      "id": "zitYaPI6oAKw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2t3KXbpQoMul"
      },
      "id": "2t3KXbpQoMul",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}